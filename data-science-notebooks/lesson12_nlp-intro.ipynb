{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_fXdSNXtaF1"
      },
      "source": [
        "# Lesson 12 - Introduction to NLP\n",
        "\n",
        "> Introduction to Natural Language Processing (NLP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u-e5R0CtaF7"
      },
      "source": [
        "## Learning objectives\n",
        "In this lecture we cover the basics of NLP to build a sentiment classifier in scikit-learn. The learning goals are:\n",
        "* Know the basics of string processing in python\n",
        "* Preprocessing steps in NLP\n",
        "* Count and TF-IDF encodings\n",
        "* Sentiment classification workflow\n",
        "\n",
        "## References\n",
        "* Chapter 10: Representing and Mining Text in _Data Science for Business_ by F. Provost and P. Fawcett\n",
        "\n",
        "## Homework\n",
        "As homework read the references, work carefully through the notebook, solve the exercises, and read this explanation on the [Naive Bayes classifier](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c). \n",
        "\n",
        "## Introduction to NLP\n",
        "Natural language processing (NLP) concerns the part of Machine Learning about the analysis of digital, human written texts. The topic of NLP is as old as machine learning itself and dates back to Alan Turing himself. Since text is a widely used medium there are plenty of applications of machine learning:\n",
        "\n",
        "- Text classification\n",
        "- Question/answering systems\n",
        "- Dialogue systems\n",
        "- Summarization\n",
        "- Text generation\n",
        "etc.\n",
        "\n",
        "Especially in the past few years there has been exciting and rapid progress in the field. If you want to try your own examples you can do so at [talktotransformer.com](https://talktotransformer.com/) or read the original article on [OpenAI's webpage](https://openai.com/blog/better-language-models/).Natural text is different to other data sources such as numerical tables or images. One way to look at text is to consider each word to be a feature. Since most languages have of the order of 100k words in their vocabulary plus many variations this leads to an enormous feature space. At the same time most words in the vocabulary do not appear in a small text. This leads to extreme sparsity. These properties call for a different approach to NLP than the methods we encountered and used for tabular data.\n",
        "\n",
        "## Notebook overview\n",
        "The goal of this notebook is to classify movie reviews in terms of positive or negative feedback. This task is called sentiment analysis and is a common NLP application. As a company you might use a sentiment classifier to analyse customer feedback or detect toxic comments on your website.\n",
        "\n",
        "Text data can be messy and require some clean up. The specific steps for the clean-up can depend on how the data was generated or where it was found. Text from the web might have some html artifacts that need cleaning or product reviews could include meta information on the review. Python offers powerful tools to manipulate strings.\n",
        "\n",
        "Once the text is cleaned we have to encode it in a way that machine learning methods can handle. Directly using text as input is not possible. Most machine learning methods can only handle numerical data such as vectors and matrices. So we have to encode the input texts as vectors or matrices. These text representations are called vector encodings. Furthermore, we look at n-grams to keep some of the sequential structure of text.\n",
        "\n",
        "Finally, we can train a model to classify the movie review texts. However, the Random Forest models we already know well do not work well for the high-dimensional data. We introduce a new methods that is common for text data called the Na√Øve Bayes classifier that utilises Bayes theorem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtKgOuGDtaF8"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS35dqDstaF9",
        "outputId": "2694738a-6782-429c-ab1a-a4a8df31745b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\joelt\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\joelt\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJX57tuUtaF-"
      },
      "source": [
        "## Part 1: Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zin-AtbtaF-"
      },
      "source": [
        "First, we load the IMDB dataset as a dataframe. Note that this is not the original dataset from [here](https://ai.stanford.edu/~amaas/data/sentiment/), but a version that I pre-processed for the ease of use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x3ESQizCtaF_"
      },
      "outputs": [],
      "source": [
        "df_imdb = pd.read_csv('imdb.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 5)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_imdb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-aQYwBGttaGA",
        "outputId": "ebb9fab0-4fec-4d0f-f066-d7044877ec8e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>train_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4715_9</td>\n",
              "      <td>For a movie that gets no respect there sure ar...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12390_8</td>\n",
              "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8329_7</td>\n",
              "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9063_8</td>\n",
              "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3092_10</td>\n",
              "      <td>You probably all already know this by now, but...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  filename                                               text sentiment  \\\n",
              "0   4715_9  For a movie that gets no respect there sure ar...       pos   \n",
              "1  12390_8  Bizarre horror movie filled with famous faces ...       pos   \n",
              "2   8329_7  A solid, if unremarkable film. Matthau, as Ein...       pos   \n",
              "3   9063_8  It's a strange feeling to sit alone in a theat...       pos   \n",
              "4  3092_10  You probably all already know this by now, but...       pos   \n",
              "\n",
              "  train_label  \n",
              "0       train  \n",
              "1       train  \n",
              "2       train  \n",
              "3       train  \n",
              "4       train  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_imdb.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBDKFcQOtaGA"
      },
      "source": [
        "The dataset consists of a `filename`, `text`, `sentiment` and a `train_label`. The latter splits the data into a train and test set which is used as the official benchmark. We will follow that same split. \n",
        "\n",
        "But first we want to make the `sentiment` column categorical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wKxG-UxdtaGA"
      },
      "outputs": [],
      "source": [
        "df_imdb['sentiment'] = df_imdb['sentiment'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q_GK3nYtaGA"
      },
      "source": [
        "Now, let's have a look at a few text examples. For that purpose we wrote a helper function to print examples from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kZQRHBbVtaGB"
      },
      "outputs": [],
      "source": [
        "def print_n_samples(df, n):\n",
        "    \"\"\"\n",
        "    Helper function to print data samples from IMDB dataset.\n",
        "    \"\"\"\n",
        "    for i in range(n):\n",
        "        print('SAMPLE', i+1, '\\n')\n",
        "        df_sample = df.sample(1)\n",
        "        print(df_sample['text'].values[0])\n",
        "        print('\\nSentiment:', df_sample['sentiment'].values[0],'\\n')\n",
        "        print(\"\".join(100*['=']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_adwxKStaGB"
      },
      "source": [
        "We can show a few examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_lxlaCPitaGB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAMPLE 1 \n",
            "\n",
            "I was hoping this would be of the calibre of Das Boot and echo the stark realism created by acclaimed German Director Leni RiefenStahl in her documentaries, sadly I was monumentally disappointed. The story line is implausible and defies credulity. An RAF airman is shot down and somehow finds his way to a hospital in Dresden. Anna a nurse whose father runs the hospital and is about to become engaged to a doctor she works with falls in love with the airman and they make love. The next evening at a lavish engagement party the airman turns up disguised as a German officer and dances with Anna. Although well directed and acted, to me it is soap opera of the lowest order.\n",
            "\n",
            "Sentiment: neg \n",
            "\n",
            "====================================================================================================\n",
            "SAMPLE 2 \n",
            "\n",
            "This B&W film reached the spartan movie house of my Frisian village about 18 months after its release. In those days much of our full-length comedy fare hailed from Denmark (Nils Poppe anyone?) so this movie struck like a thunderbolt -- it had me weeping with helpless mirth, ROTFL as we'd now put it. OK, so some of the sight gags were in fact recycled vaudeville 'schtick', but how was this 'barefoot boy with cheeks of brass' to know that at the time? In any case, my favorite scenes had Jerry's unique brand of frantic clowning, like that Hawaii boxing match.<br /><br />Seeing \"Sailor Beware\" again fifty years later I still guffawed loudly at the goings-on. Granted, without the nostalgia component it would probably be just another fair-to-middling comedy. But then, another movie that once had me in stitches even more helplessly, the Spike Jones outing \"Fireman Save My Child\", now seems dated and stilted apart from some too-short orchestra bits and Doodles Weaver scenes. Must be some special ingredient that makes Martin & Lewis product stay fresher longer. To me this one at least rates eight out of ten.\n",
            "\n",
            "Sentiment: pos \n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print_n_samples(df_imdb, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I0Ck3IktaGC"
      },
      "source": [
        "We can see that the reviews are medium sized texts with positive and negative labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHgrsGrmtaGD"
      },
      "source": [
        "### Exercise 1\n",
        "A few exploratory and processing tasks:\n",
        "* Describe the distribution of positive and negative comments in the train and test dataset.\n",
        "* Study the distribution of the text lengths. You can perform string operations on a `pandas.DataFrame` by accessing the `str` object of a column: `df['YOUR_TEXT_COLUMN'].str.len()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='Type', ylabel='Amount'>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "number_pos = df_imdb.loc[df_imdb[\"sentiment\"] == \"pos\"].shape[0]\n",
        "number_neg = df_imdb.loc[df_imdb[\"sentiment\"] == \"neg\"].shape[0]\n",
        "\n",
        "plot_dict :dict = {\n",
        "    \"Type\" :        ['pos', 'neg'],\n",
        "    \"Amount\" :      [number_pos, number_neg]\n",
        "}\n",
        "\n",
        "df_plot = pd.DataFrame(plot_dict)\n",
        "\n",
        "sns.barplot(data = df_plot, x = \"Type\", y = \"Amount\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pos</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neg</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Type  Amount\n",
              "0  pos   25000\n",
              "1  neg   25000"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>train_label</th>\n",
              "      <th>count</th>\n",
              "      <th>text_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4715_9</td>\n",
              "      <td>For a movie that gets no respect there sure ar...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "      <td>1</td>\n",
              "      <td>284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12390_8</td>\n",
              "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "      <td>1</td>\n",
              "      <td>1033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8329_7</td>\n",
              "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "      <td>1</td>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9063_8</td>\n",
              "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "      <td>1</td>\n",
              "      <td>2596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3092_10</td>\n",
              "      <td>You probably all already know this by now, but...</td>\n",
              "      <td>pos</td>\n",
              "      <td>train</td>\n",
              "      <td>1</td>\n",
              "      <td>783</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  filename                                               text sentiment  \\\n",
              "0   4715_9  For a movie that gets no respect there sure ar...       pos   \n",
              "1  12390_8  Bizarre horror movie filled with famous faces ...       pos   \n",
              "2   8329_7  A solid, if unremarkable film. Matthau, as Ein...       pos   \n",
              "3   9063_8  It's a strange feeling to sit alone in a theat...       pos   \n",
              "4  3092_10  You probably all already know this by now, but...       pos   \n",
              "\n",
              "  train_label  count  text_len  \n",
              "0       train      1       284  \n",
              "1       train      1      1033  \n",
              "2       train      1       318  \n",
              "3       train      1      2596  \n",
              "4       train      1       783  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_imdb[\"text_len\"] = df_imdb[\"text\"].str.len()\n",
        "df_imdb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.distplot(df_imdb[\"text_len\"])\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean:\t 1309.43102\n",
            "Median:\t 970.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"Mean:\\t\", df_imdb[\"text_len\"].mean())\n",
        "print(\"Median:\\t\", df_imdb[\"text_len\"].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm-_X2RltaGE"
      },
      "source": [
        "## Part 2: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MLXYsrrtaGE"
      },
      "source": [
        "In this section we have a look at the basics of string processing. Being able to filter/combine/manipulate strings is a crucial skill to do natural language processing.\n",
        "\n",
        "Cleaning up text for NLP tasks usually involves the following steps.\n",
        "* Normalization\n",
        "* Tokenization\n",
        "* Remove stop-words\n",
        "* Remove non-alphabetical tokens\n",
        "* Stemming\n",
        "\n",
        "Some of the steps might not be necessary or you need to add steps depending on the text, task and method. For our task these steps are fine. We apply these steps on one text as an example and then build a function to apply it to all texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rslEcqwhtaGE"
      },
      "source": [
        "\n",
        "### String processing in Python\n",
        "Python offers powerful properties and functions to manipulate strings. Let's preprocess the texts in the dataset to bring them to a cleaner form. Fortunately we don't need to implement everything from scratch. One of the richest Python libraries to process texts is the Natural Language Toolkit (NLTK) which offers some powerful functions we will use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pdu7h2ptaGF"
      },
      "source": [
        "### Input\n",
        "We see that the raw text has several features such as capitalisation, special characters, numericals and puncuations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "pgv38xgLtaGF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.\n"
          ]
        }
      ],
      "source": [
        "text = df_imdb.loc[0, 'text']\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhAbwKsctaGG"
      },
      "source": [
        "### Normalize\n",
        "This is the process of transforming the text to lower-case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "GpTj_GZptaGG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan \"the skipper\" hale jr. as a police sgt.\n"
          ]
        }
      ],
      "source": [
        "text = text.lower()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Xn6DuxtaGG"
      },
      "source": [
        "### Tokenize\n",
        "Now we split the text in words/tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzRzMz_ctaGG",
        "outputId": "b41423c7-5187-4bc3-b8d2-136b57ca926d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', '``', 'the', 'skipper', \"''\", 'hale', 'jr.', 'as', 'a', 'police', 'sgt', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgAR02oHtaGH"
      },
      "source": [
        "### Stop Words\n",
        "Next, we remove words that are too common and don't add to the content of sentences. These words are commonly called 'stop words'. NLTK provides a list of stop words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbh9VuqjtaGH",
        "outputId": "ef0959c6-0d6f-4b7a-f6d7-10c4bfa2e439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'needn', 'when', 'has', \"couldn't\", 'so', 'and', 'aren', 'himself', 'not', 'mightn', 'a', 'yourselves', 'above', 'wasn', 'who', 'how', 'for', 'him', 'won', 'again', \"aren't\", 'didn', \"you've\", 'they', 'now', \"wouldn't\", 'doesn', 'shan', 'will', 'by', 'own', 'myself', 'further', 'd', \"you'll\", 'me', 'did', 've', 'other', 'themselves', 'have', 'very', 'between', 'during', 're', 'no', \"shan't\", 'or', 'hasn', 'don', 'weren', \"mightn't\", 'herself', 'we', 'he', 'wouldn', 'her', 'having', 'up', 'yours', 'most', 's', 'shouldn', \"should've\", 'does', 'those', 'hadn', \"hadn't\", 'if', 'such', 'through', 'after', 'being', 'too', 'about', 'because', 'at', 'an', 'its', \"hasn't\", 'once', 'some', 'below', 'why', 'which', 'y', 'ain', 'mustn', 'my', \"you're\", 'am', 'there', 'doing', 'any', 't', \"haven't\", 'be', 'were', 'under', 'of', 'out', 'more', 'before', 'o', 'this', 'into', \"she's\", 'had', 'same', 'while', 'just', 'each', 'these', 'hers', 'only', \"it's\", 'is', 'do', 'his', 'them', \"wasn't\", 'against', 'theirs', 'can', 'll', 'whom', 'on', \"won't\", 'but', 'here', 'where', 'that', 'couldn', 'nor', 'it', 'both', 'm', 'are', 'itself', 'the', \"doesn't\", 'their', 'off', 'over', 'ours', \"needn't\", 'in', 'haven', 'was', 'she', 'you', 'been', 'your', 'our', \"shouldn't\", 'ourselves', 'from', 'than', 'should', 'i', 'down', \"you'd\", 'until', \"isn't\", 'what', 'isn', 'ma', 'yourself', 'with', 'few', 'all', 'to', 'then', \"weren't\", \"mustn't\", \"that'll\", 'as', \"didn't\", \"don't\"}\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHYofYaetaGH"
      },
      "source": [
        "We keep only the words that are **not** in the list of stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "kAz1F1QptaGH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['movie', 'gets', 'respect', 'sure', 'lot', 'memorable', 'quotes', 'listed', 'gem', '.', 'imagine', 'movie', 'joe', 'piscopo', 'actually', 'funny', '!', 'maureen', 'stapleton', 'scene', 'stealer', '.', 'moroni', 'character', 'absolute', 'scream', '.', 'watch', 'alan', '``', 'skipper', \"''\", 'hale', 'jr.', 'police', 'sgt', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = [i for i in tokens if not i in stop_words]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KhdrK4PtaGH"
      },
      "source": [
        "### Punctuation\n",
        "We also want to remove all tokens that are not composed of letters (e.g. punctuation and numbers). We can check if a token is only composed of alphabetic letters with the `isalpha()` and filter with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DRZnkoktaGI",
        "outputId": "ef7335a0-c33f-4d7d-c157-77037113fc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['movie', 'gets', 'respect', 'sure', 'lot', 'memorable', 'quotes', 'listed', 'gem', 'imagine', 'movie', 'joe', 'piscopo', 'actually', 'funny', 'maureen', 'stapleton', 'scene', 'stealer', 'moroni', 'character', 'absolute', 'scream', 'watch', 'alan', 'skipper', 'hale', 'police', 'sgt']\n"
          ]
        }
      ],
      "source": [
        "tokens = [i for i in tokens if i.isalpha()]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-92lD1GLtaGI"
      },
      "source": [
        "### Stemming\n",
        "As a final step we want to trim the words to the stem. This helps drastically decrease the vocabulary size and maps similar/same words onto the same word. E.g. plural/singular words or different forms of verbs:\n",
        "* pen, pens --> pen\n",
        "* happy, happier --> happi\n",
        "* go, goes --> go\n",
        "\n",
        "There are several languages available in nltk since this is a **language dependant process**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "QYtAjLzhtaGI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ],
      "source": [
        "print(SnowballStemmer.languages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmYRoZLVtaGI"
      },
      "source": [
        "Applied to the text sample, this yields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYWycVXutaGI",
        "outputId": "ec491950-938c-4ca9-8fe9-ba62ac8e3da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['movi', 'get', 'respect', 'sure', 'lot', 'memor', 'quot', 'list', 'gem', 'imagin', 'movi', 'joe', 'piscopo', 'actual', 'funni', 'maureen', 'stapleton', 'scene', 'stealer', 'moroni', 'charact', 'absolut', 'scream', 'watch', 'alan', 'skipper', 'hale', 'polic', 'sgt']\n"
          ]
        }
      ],
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "tokens = [stemmer.stem(i) for i in tokens]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-dpMsFAtaGJ"
      },
      "source": [
        "### Preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "g57piLUBtaGJ"
      },
      "outputs": [],
      "source": [
        "def preprocessing(text, language='english', stemming=True):\n",
        "    \"\"\"\n",
        "    preprocess a string and return processed tokens.\n",
        "    args:\n",
        "        text: text string\n",
        "    return:\n",
        "        tokens: list of processed and cleaned words\n",
        "    \"\"\"\n",
        "    \n",
        "    stop_words = set(stopwords.words(language))\n",
        "    stemmer = SnowballStemmer(language)    \n",
        "    \n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [i for i in tokens if not i in stop_words]\n",
        "    tokens = [i for i in tokens if i.isalpha()]\n",
        "    if stemming:\n",
        "        tokens = [stemmer.stem(i) for i in tokens]\n",
        "    \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQNjON6etaGJ"
      },
      "source": [
        "Finally, we can apply these steps to all texts. We use the `apply` function of pandas which applies a function to every entry in a DataFrame column. Since we registered `tqdm` we can use the `progress_apply` function which uses `apply` and adds a progress bar to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "CeGO2IbFtaGJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [02:28<00:00, 335.66it/s]\n"
          ]
        }
      ],
      "source": [
        "df_imdb['text_processed_stemmed'] = df_imdb['text'].progress_apply(preprocessing) # add new column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nkZYvC8taGK"
      },
      "source": [
        "## Part 3: Vector encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqfuKgrEtaGK"
      },
      "source": [
        "### Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVPgeWSotaGK"
      },
      "source": [
        "Now that we cleaned up and tokenized the text corpus we are now ready to encode the texts in vectors. In class we had a look at simple **one-hot encodings** that can be extended to count encodings and **TF-IDF encodings**.\n",
        "\n",
        "TF-IDF stands is for term frequency‚Äìinverse document frequency and is a metric that indicates how important a token is in a document.\n",
        "\n",
        "Scikit-learn comes with functions to do both count and TF-IDF encodings on text. The interface is very similar to the classifier just the `predict` step is replaced with `transform`:\n",
        "\n",
        "```python\n",
        "count_vectorizer = CountVectorizer(your_settings)\n",
        "count_vectorizer.fit(your_dataset)\n",
        "vec = count_vectorizer.transform('your_text')\n",
        "```\n",
        "\n",
        "This creates a vectorizer that can transform texts to vectors. We can also limit the number of words take into account when building the vector. This limits the vector size and cuts off words that occur rarely. If you set `max_features=10000` only the 10000 most occurring words are used to build the vector and all rare words are excluded. This means that the encoding vector then has a dimension of 10000. For now we take all words (`max_features=None`). Since we used our own tokenizer and preprocessing step we overwrite the standard steps in the vectorizer library with the `vec_default_settings`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "HpQrjTrptaGK"
      },
      "outputs": [],
      "source": [
        "vec_default_settings = {'analyzer':'word', 'tokenizer':lambda x: x, 'preprocessor':lambda x: x, 'token_pattern':None,}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "tE5ymfgwtaGK"
      },
      "outputs": [],
      "source": [
        "# TF-IDF vectorizer\n",
        "tfidf_vec = TfidfVectorizer(max_features=None, **vec_default_settings)\n",
        "\n",
        "# Count vectorizer\n",
        "count_vec = CountVectorizer(max_features=None, **vec_default_settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LskFzksztaGK"
      },
      "source": [
        "Let's test both vectorizers on a small, dummy dataset with **4 documents**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yIk_W-yVtaGK"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    ['this','is','the','first','document','in','the','corpus'],\n",
        "    ['this','document','is','the','second','document','in','the','corpus'],\n",
        "    ['and','this','is','the','third','one','in','this','corpus'],\n",
        "    ['is','this','the','first','document','in','this','corpus'],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4fksiHktaGL"
      },
      "source": [
        "Now we fit a count vectorizer to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk8uiMV1taGL",
        "outputId": "5d03e92e-9c59-4747-a8dc-5407d520783d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CountVectorizer(preprocessor=<function <lambda> at 0x0000014A99972790>,\n",
              "                token_pattern=None,\n",
              "                tokenizer=<function <lambda> at 0x0000014A99972160>)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vec.fit(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9E6FtiktaGL"
      },
      "source": [
        "Once a vectorizer is fitted, we can investigate the vocabulary. It is a dictionary that points each word to the index in the vector it corresponds to. For example the word `'this'` corresponds to the 10+1-nth (+1 because we start counting at zero) entry in the vector and the word `'and'` corresponds to the first entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4hUyWfEtaGL",
        "outputId": "0e673099-2cec-4f26-a901-ff99dfb83acb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(count_vec.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q5fAaEptaGM",
        "outputId": "cecad770-2da7-4435-aec2-6338aba90bea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'this': 10,\n",
              " 'is': 5,\n",
              " 'the': 8,\n",
              " 'first': 3,\n",
              " 'document': 2,\n",
              " 'in': 4,\n",
              " 'corpus': 1,\n",
              " 'second': 7,\n",
              " 'and': 0,\n",
              " 'third': 9,\n",
              " 'one': 6}"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vec.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDZV6dvztaGM"
      },
      "source": [
        "Now we can transform the corpus and get a list of vectors in the form of a matrix (each row corresponds to a document vector). For instance, the first row contains how frequently each vocabulary element occurs in the first row of the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrlLf4ektaGN",
        "outputId": "a1354a83-c561-4e24-eab6-13fe260dc1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 1 1 0 0 2 0 1]\n",
            " [0 1 2 0 1 1 0 1 2 0 1]\n",
            " [1 1 0 0 1 1 1 0 1 1 2]\n",
            " [0 1 1 1 1 1 0 0 1 0 2]]\n"
          ]
        }
      ],
      "source": [
        "X = count_vec.transform(corpus)\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmLvWbcztaGN"
      },
      "source": [
        "If we now do the same thing with the TF-IDF vectorizer we see that the output looks different:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OlBchfQtaGO",
        "outputId": "b17ea3e0-c4ac-40bc-b8b7-08a9620fb00e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.29137467 0.35639305 0.44021632 0.29137467 0.29137467\n",
            "  0.         0.         0.58274934 0.         0.29137467]\n",
            " [0.         0.23798402 0.58217725 0.         0.23798402 0.23798402\n",
            "  0.         0.45604658 0.47596805 0.         0.23798402]\n",
            " [0.43943636 0.22931612 0.         0.         0.22931612 0.22931612\n",
            "  0.43943636 0.         0.22931612 0.43943636 0.45863224]\n",
            " [0.         0.29137467 0.35639305 0.44021632 0.29137467 0.29137467\n",
            "  0.         0.         0.29137467 0.         0.58274934]]\n"
          ]
        }
      ],
      "source": [
        "tfidf_vec.fit(corpus)\n",
        "X = tfidf_vec.transform(corpus)\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHEB2Xz6taGO"
      },
      "source": [
        "* The shape of the matrix is the same.\n",
        "* Instead of integers (corresponding to counts) we have continous values.\n",
        "* Elements that occur in multilple documents have lower scores than those appearing in fewer.\n",
        "\n",
        "This should just illustrate that different vectorizers are available but we do not go into detail on them. Now let's apply this to our dataset and create encodings with `100000` words:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGmY3RrKtaGO"
      },
      "source": [
        "### n-grams\n",
        "When we use a count or TF-IDF vectorizer we throw away all sequential information in the texts. From the vector encodings above we could not reconstruct the original sentences. For this reason these encodings are called Bag-of-Words encodings (all words go in a bag and are shuffeled). However, sequential information can be important for the meaning of a sentence. As an example imagine the sentence:\n",
        "\n",
        "```Python\n",
        "text = 'The movie was good and not bad.'\n",
        "```\n",
        "\n",
        "It is important to know if the word `'not'` is in front of `'good'` or `'bad'` for determining the sentiment of the sentence. We can preserve some of that information by using n-grams. Instead of just encoding single words we can also encode tuple, triplets etc. called n-grams. The n encodes how many words we bundle together. \n",
        "\n",
        "The vectorizers can do this for us if we provide them a range of n's we want to include. In the following example we encode the text in 1- and 2-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "_h8g8znOtaGO"
      },
      "outputs": [],
      "source": [
        "count_vec = CountVectorizer(max_features=None, ngram_range=(1,2), **vec_default_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrdTKs-BtaGP",
        "outputId": "681d4a3e-df3e-4bd9-bc03-4e2c0b5e7057"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CountVectorizer(ngram_range=(1, 2),\n",
              "                preprocessor=<function <lambda> at 0x0000014A99972790>,\n",
              "                token_pattern=None,\n",
              "                tokenizer=<function <lambda> at 0x0000014A99972160>)"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vec.fit(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEPMYjf-taGP"
      },
      "source": [
        "We can see that this drastically increases the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDVpPCY9taGP",
        "outputId": "a560d7d8-d06f-4c45-8e54-d7375ecb3507"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(count_vec.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtBSVdX2taGP"
      },
      "source": [
        "Now the vocabulary also contains word tuples next to the words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6FM3rqYtaGP",
        "outputId": "40bcd0a9-7354-4398-a44e-17012b4085a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'this': 25,\n",
              " 'is': 11,\n",
              " 'the': 18,\n",
              " 'first': 6,\n",
              " 'document': 3,\n",
              " 'in': 8,\n",
              " 'corpus': 2,\n",
              " 'this is': 28,\n",
              " 'is the': 12,\n",
              " 'the first': 20,\n",
              " 'first document': 7,\n",
              " 'document in': 4,\n",
              " 'in the': 9,\n",
              " 'the corpus': 19,\n",
              " 'second': 16,\n",
              " 'this document': 27,\n",
              " 'document is': 5,\n",
              " 'the second': 21,\n",
              " 'second document': 17,\n",
              " 'and': 0,\n",
              " 'third': 23,\n",
              " 'one': 14,\n",
              " 'and this': 1,\n",
              " 'the third': 22,\n",
              " 'third one': 24,\n",
              " 'one in': 15,\n",
              " 'in this': 10,\n",
              " 'this corpus': 26,\n",
              " 'is this': 13,\n",
              " 'this the': 29}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vec.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w35dwI6taGP"
      },
      "source": [
        "The encodings look very similar but are larger due to the larger vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL65ZW-jtaGQ",
        "outputId": "ba7fc835-6a0e-445f-d3cb-f26e5d65d252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 2 1 1 0 0 0 0 1 0 0 1 0]\n",
            " [0 0 1 2 1 1 0 0 1 1 0 1 1 0 0 0 1 1 2 1 0 1 0 0 0 1 0 1 0 0]\n",
            " [1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 2 1 0 1 0]\n",
            " [0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 2 1 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "X = count_vec.transform(corpus)\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXCmYYbJtaGQ"
      },
      "source": [
        "### Vectorize dataset\n",
        "Now we want to encode the real text. We put an upper limit on the vocabulary size. Since there are a lot of unique words in our corpus there are a lot of combinations of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "_smoVFShtaGQ"
      },
      "outputs": [],
      "source": [
        "max_features=100000\n",
        "ngrams=(1,3)\n",
        "\n",
        "count_vec = CountVectorizer(max_features=max_features, ngram_range=ngrams, **vec_default_settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKB4q0F1taGQ"
      },
      "source": [
        "In the example above we used the `fit` and `transform` function. We can avoid these two steps with the combined function `fit_transform`. First we need to split the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "gtXIrmdHtaGQ"
      },
      "outputs": [],
      "source": [
        "text_train = df_imdb.loc[df_imdb['train_label']=='train', 'text_processed_stemmed']\n",
        "text_test = df_imdb.loc[df_imdb['train_label']=='test', 'text_processed_stemmed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ujJ47N_ItaGQ"
      },
      "outputs": [],
      "source": [
        "X_train = count_vec.fit_transform(text_train)\n",
        "X_test = count_vec.transform(text_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8pDocNStaGR"
      },
      "source": [
        "This yields a vocabulary with `100000` entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UC0Rn7LtaGR",
        "outputId": "ca20e0f8-3a98-48d1-cdf7-d4e9b7345f02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(count_vec.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hzQTWAtaGR"
      },
      "source": [
        "Looking at the shape of the returned matrix we see that it still has as many rows as the input but now has `100000` entries per row (the feature vector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppbkzwzwtaGR",
        "outputId": "3a038510-65ac-4aa7-b5c8-492c1c09e1c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 100000)"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yScg4XKjtaGR"
      },
      "source": [
        "## Part 4: Sentiment classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SnXjA8ftaGR"
      },
      "source": [
        "Now that we featurised the text, we can train a model on it. In this section we will use a Na√Øve Bayes classifier to determine whether a review is positive or negative. (For an explanation of the classifier, please read [this review](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c).) \n",
        "\n",
        "First, we split the labels of the training and test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "oJ2Yb29JtaGR"
      },
      "outputs": [],
      "source": [
        "y_train = df_imdb.loc[df_imdb['train_label']=='train', 'sentiment']\n",
        "y_test = df_imdb.loc[df_imdb['train_label']=='test', 'sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpEWAutLtaGS"
      },
      "source": [
        "We can initialise a Na√Øve Bayes classifier the same way we initialised the Random Forest models. Also the fit/predict interface is the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "udsKs_6QtaGS"
      },
      "outputs": [],
      "source": [
        "nb_clf = MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUK0v3zptaGS",
        "outputId": "86f3cf6b-0f56-42f1-a493-7bea6e8c6080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 78.1 ms\n",
            "Wall time: 94.3 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "nb_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "NOUPs0q0taGS"
      },
      "outputs": [],
      "source": [
        "y_pred = nb_clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZNHFyVJtaGS"
      },
      "source": [
        "We can calculate the prediction accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XezHHGsHtaGS",
        "outputId": "f193a500-ac14-4e79-9730-4f8eaf3bdf4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8524"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u9iSgc7taGS"
      },
      "source": [
        "### Random Forest\n",
        "We can compare the Na√Øve Bayes model with a Random Forest on the same task with the same input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "elDxiJK9taGS"
      },
      "outputs": [],
      "source": [
        "rf_clf = RandomForestClassifier(n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h90DSnAxtaGT",
        "outputId": "e0e4b535-9a3a-4ae3-e0b5-5423b9f80d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 2min 4s\n",
            "Wall time: 18 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_jobs=-1)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "rf_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbGTqltZtaGT",
        "outputId": "fa822f7b-8a37-4569-8a25-d0c34876aafc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.85716"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = rf_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxw0c3lvtaGT"
      },
      "source": [
        "We can see that we get similar performance while being **~1000x faster**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQBnKpiDtaGU"
      },
      "source": [
        "### Exercise 2\n",
        "Retrain the model with a TF-IDF encoding instead of the count encoding. Experiment with the n-gram setting and run the experiment with the following settings: `(1,1)`, `(1,2)`, `(1,3)` and `(1,4)`. Which n-gram setting gives you the highest sentiment classification accuracy? And ideas why this setting gives the highest accuracy? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vec.fit(corpus)\n",
        "X = tfidf_vec.transform(corpus)\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_features=100000\n",
        "ngrams=(1,3)\n",
        "\n",
        "count_vec = CountVectorizer(max_features=max_features, ngram_range=ngrams, **vec_default_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = count_vec.fit_transform(text_train)\n",
        "X_test = count_vec.transform(text_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rslEcqwhtaGE"
      ],
      "name": "lesson12_nlp-intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
