{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "\n",
    "#folders\n",
    "data_folder = \"data\"\n",
    "\n",
    "#machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "#model scoring\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optim = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_folder, \"df_main.csv\"))\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot styles\n",
    "plt_style_c = px.colors.sequential.haline #complex\n",
    "plt_style_s = px.colors.diverging.Portland #simple\n",
    "\n",
    "#defualt plot size \n",
    "size = {\n",
    "    \"width\" : 1500 ,\n",
    "    \"height\" : 750 ,\n",
    "}\n",
    "\n",
    "#function for plotting\n",
    "def scale_show(fig):\n",
    "\n",
    "    #set font\n",
    "    fig.update_layout(\n",
    "        font = dict(size=16),\n",
    "        title_font = dict(size=20),\n",
    "        xaxis_title_font = dict(size=18),\n",
    "        yaxis_title_font = dict(size=18),\n",
    "    )\n",
    "\n",
    "    #set size\n",
    "    fig.update_layout(\n",
    "        width=1500,\n",
    "        height=750,\n",
    "    )\n",
    "\n",
    "    #show\n",
    "    fig.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(): #parent\n",
    "\n",
    "    def __init__(self, df : object, y_col : list, data_folder : str, results_file : str, model_metric : str, n_jobs : int = 1, window : int = 30):\n",
    "\n",
    "        #save raw df\n",
    "        self.df_raw         = df.copy()\n",
    "\n",
    "        #drop forbidden cols\n",
    "        df = df.copy() #pass by value\n",
    "        df = Base.__drop_forbidden_cols(df, y_col)\n",
    "\n",
    "        #set dataframe for refferencing\n",
    "        self.df             = df.copy() #windowed df, copy because obj is passed by refference\n",
    "\n",
    "        #get and get x_col and y_col\n",
    "        self.y_col          = y_col\n",
    "        self.x_col          = list(df.drop(labels = y_col, axis = 1, inplace = False).columns.to_list())\n",
    "\n",
    "        #misc params\n",
    "        self.random_state   = 42\n",
    "        self.n_jobs         = n_jobs\n",
    "        self.data_folder    = data_folder\n",
    "        self.results_file   = os.path.join(data_folder,results_file)\n",
    "        self.model_metric   = model_metric\n",
    "\n",
    "        #ann parameters\n",
    "        self.acitvation_func        = \"selu\"\n",
    "        self.solver                 = tf.keras.optimizers.legacy.SGD(learning_rate=0.0001) #tf.keras.optimizers.SGD(learning_rate=0.0001) #tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.initializer            = tf.keras.initializers.HeNormal(seed = self.random_state)\n",
    "        self.n_epochs               = 5\n",
    "\n",
    "        #windowing parameters\n",
    "        self.x_window       = window #number of shifting window input features\n",
    "\n",
    "        self.__setup()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __setup(self):\n",
    "\n",
    "        #order was chosen to minimize data loss, at the cost of more needed processing power\n",
    "\n",
    "        #data preparation\n",
    "        self.__windowing()\n",
    "        self.__split_data()\n",
    "        self.__standardize_data()\n",
    "\n",
    "        #setup of metrics and results\n",
    "        self.__set_assesment()\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def __drop_forbidden_cols(df, y_col):\n",
    "\n",
    "        forbidden_cols = ['date','t2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', 't2m_t1_cat', 't2m_t2_cat']\n",
    "\n",
    "        #prevent y_cols from being dropped from the data frame\n",
    "        for y in y_col:\n",
    "            if y in forbidden_cols:\n",
    "                forbidden_cols.remove(y)\n",
    "\n",
    "        #drop forbidden cols, to prevent adding future information to the time series\n",
    "        print(f\"Removed forbidden cols:\\n{forbidden_cols}\")\n",
    "        df.drop(labels = forbidden_cols, axis = 1, inplace = True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __windowing(self):\n",
    "        \"\"\"creates the windowed data frame\"\"\"\n",
    "\n",
    "        self.x_col_windowed = self.x_col.copy() #copy, becaus lists are past by refference\n",
    "\n",
    "        #input fetures: x\n",
    "        for i in range(1, self.x_window + 1):\n",
    "            for x_col in self.x_col: #inefficient but works just fine\n",
    "\n",
    "                x_col_i             = f\"{x_col}_-{i}\"\n",
    "                self.df[x_col_i]    = df[x_col].shift(i)\n",
    "\n",
    "                self.x_col_windowed.append(x_col_i)\n",
    "\n",
    "        #clean na columns, which were caused by the shifts\n",
    "        self.df.dropna(inplace = True)\n",
    "        print(f\"\\nApplying shifitng window:\\nx_window: -{self.x_window}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def __split_data(self):\n",
    "\n",
    "        #reset index for splitting data\n",
    "        self.df.reset_index(inplace = True, drop = True)\n",
    "        length = self.df.shape[0]\n",
    "\n",
    "        #setting split value fractions\n",
    "        valid_frac_0      = 0.1\n",
    "        test_frac_1       = 0.05\n",
    "        train_frac_2      = 0.7\n",
    "        valid_frac_3      = 0.1\n",
    "        test_frac_4       = 0.05\n",
    "\n",
    "        #get end indexes\n",
    "        index_end_list = []\n",
    "        cum_frac = 0\n",
    "\n",
    "        for frac in [valid_frac_0, test_frac_1, train_frac_2, valid_frac_3, test_frac_4]:\n",
    "            cum_frac += frac\n",
    "            index_end_list.append(round(length * cum_frac))\n",
    "\n",
    "        #get indexes (ugly code)\n",
    "        df_indexes = self.df.index.tolist()\n",
    "        train_i     = df_indexes[index_end_list[1] : index_end_list[2]]\n",
    "        valid_i     = df_indexes[ : index_end_list[0]]                      + df_indexes[index_end_list[2] : index_end_list[3]]\n",
    "        test_i      = df_indexes[index_end_list[0] : index_end_list[1]]     + df_indexes[index_end_list[3] : index_end_list[4]]\n",
    "\n",
    "        #get df from indexes\n",
    "        self.df_train_x = self.df[self.x_col_windowed].loc[self.df.index.isin(train_i)]\n",
    "        self.df_train_y = self.df[self.y_col].loc[self.df.index.isin(train_i)]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_valid_x = self.df[self.x_col_windowed].loc[self.df.index.isin(valid_i)]\n",
    "        self.df_valid_y = self.df[self.y_col].loc[self.df.index.isin(valid_i)]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_test_x = self.df[self.x_col_windowed].loc[self.df.index.isin(test_i)]\n",
    "        self.df_test_y = self.df[self.y_col].loc[self.df.index.isin(test_i)]\n",
    "\n",
    "        #check\n",
    "        print(\"\\nSplitting data:\")\n",
    "        for df, df_type in zip (\n",
    "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
    "            \"train,valid,test\".split(\",\")\n",
    "            ):\n",
    "\n",
    "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
    "\n",
    "        #set data for plotting in raw df\n",
    "        self.df_raw[\"set\"] = None\n",
    "        for index_items, set_type in zip([train_i, valid_i, test_i],[\"train\", \"valid\", \"test\"]):\n",
    "            self.df_raw.loc[self.df_raw.index.isin(index_items), \"set\"] = set_type\n",
    "\n",
    "        return\n",
    "\n",
    "    def plot_set_distribution(self, plotter, style, plt_style):\n",
    "\n",
    "        if (style == \"histogram\") and (self.model_metric == \"c\"): #only plotlable with classificaiton model\n",
    "\n",
    "            fig = px.histogram(\n",
    "                data_frame = self.df_raw,\n",
    "                x = \"set\",\n",
    "                color = \"t2m_t2_cat\",\n",
    "                histfunc = \"count\",\n",
    "\n",
    "                barmode = \"group\",\n",
    "                title = \"Categorical distribution of sets\",\n",
    "                color_discrete_sequence = plt_style,\n",
    "            )\n",
    "\n",
    "        elif (style == \"scatter\"):\n",
    "\n",
    "            fig = px.scatter(\n",
    "                data_frame = self.df_raw,\n",
    "                x = \"date\",\n",
    "                y = \"t2m\",\n",
    "                color = \"set\",\n",
    "\n",
    "                title = \"Trend distribution of sets\",\n",
    "                color_discrete_sequence = plt_style,\n",
    "            )\n",
    "\n",
    "        plotter(fig)\n",
    "\n",
    "    def __split_data_deprecated(self):\n",
    "\n",
    "        #df length\n",
    "        length = self.df.shape[0]\n",
    "\n",
    "        #setting split values\n",
    "        valid_frac     = 0.2\n",
    "        test_frac      = 0.1\n",
    "\n",
    "        #get indexes\n",
    "        train_end       = round(length * (1 - (valid_frac + test_frac)))\n",
    "        valid_end       = round(length * (1 - (test_frac)))\n",
    "        test_end        = round(length * (1))\n",
    "\n",
    "        #create train df\n",
    "        self.df_train_x = self.df[self.x_col_windowed].iloc[:train_end]\n",
    "        self.df_train_y = self.df[self.y_col].iloc[:train_end]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_valid_x = self.df[self.x_col_windowed].iloc[train_end:valid_end]\n",
    "        self.df_valid_y = self.df[self.y_col].iloc[train_end:valid_end]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_test_x = self.df[self.x_col_windowed].iloc[valid_end:test_end]\n",
    "        self.df_test_y = self.df[self.y_col].iloc[valid_end:test_end]\n",
    "\n",
    "        #check\n",
    "        print(\"\\nSplitting data:\")\n",
    "        for df, df_type in zip (\n",
    "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
    "            \"train,valid,test\".split(\",\")\n",
    "            ):\n",
    "\n",
    "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def __standardize_data(self):\n",
    "\n",
    "        label_cat = [0,1]; label_cat.sort()\n",
    "        self.standardizing_values = {\n",
    "            \"x\" : {},\n",
    "            \"y\" : {},\n",
    "            #    col1 : {\"mean\" : value, \"std\"  : value},\n",
    "            #    col2 : {\"mean\" : value, \"std\"  : value},\n",
    "            #}\n",
    "            #\"y\" : ...\n",
    "        }\n",
    "\n",
    "        print(\"\\nStandardizing values:\")\n",
    "        for col in self.df.columns:\n",
    "\n",
    "            distinct_values = list(self.df[col].unique())\n",
    "            distinct_values.sort()\n",
    "\n",
    "            if label_cat == distinct_values: #skip categorical values\n",
    "                continue\n",
    "\n",
    "            #get mean and std for all columns across both data both data frames\n",
    "            if col in self.x_col_windowed:\n",
    "\n",
    "                self.standardizing_values[\"x\"][col]             = {}\n",
    "                self.standardizing_values[\"x\"][col][\"mean\"]     = self.df_train_x[col].mean()\n",
    "                self.standardizing_values[\"x\"][col][\"std\"]      = self.df_train_x[col].std()\n",
    "\n",
    "            elif col in self.y_col:\n",
    "\n",
    "                self.standardizing_values[\"y\"][col]             = {}\n",
    "                self.standardizing_values[\"y\"][col][\"mean\"]     = self.df_train_y[col].mean()\n",
    "                self.standardizing_values[\"y\"][col][\"std\"]      = self.df_train_y[col].std()\n",
    "\n",
    "        #apply values\n",
    "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
    "            for col in self.standardizing_values[col_type].keys():\n",
    "\n",
    "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
    "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
    "                df[col] = (df[col] - mean) / std #standardization\n",
    "\n",
    "        #check sum\n",
    "        print(f\"Checksum train x: {self.df_train_x[list(self.standardizing_values['x'].keys())].mean().round(2).sum()}\")\n",
    "        print(f\"Checksum train y: {self.df_train_y[list(self.standardizing_values['y'].keys())].mean().round(2).sum()}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def unstandardize_data(self):\n",
    "\n",
    "        #apply values\n",
    "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
    "            for col in self.standardizing_values[col_type].keys():\n",
    "\n",
    "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
    "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
    "                df[col] = df[col] * std + mean #reversed standardization\n",
    "\n",
    "        return\n",
    "\n",
    "    def __set_assesment(self):\n",
    "\n",
    "        #used as a custom scoring metric for regression models\n",
    "        def r_squared(y_true, y_pred):\n",
    "            ss_res = K.sum(K.square(y_true - y_pred))\n",
    "            ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "            return 1 - ss_res / (ss_tot + K.epsilon())\n",
    "\n",
    "        #set loss functions and metrics based on model type\n",
    "\n",
    "        if self.model_metric == \"c\":\n",
    "            self.get_model_score = self.__get_model_score_c\n",
    "            self.kears_metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "            self.loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        elif self.model_metric == \"r\":\n",
    "            self.get_model_score = self.__get_model_score_r\n",
    "            self.kears_metrics = [tf.keras.metrics.MeanSquaredError(), r_squared]\n",
    "            self.loss_func = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"only allowed model metrics are: r, c\")\n",
    "\n",
    "    def __get_model_score_c(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
    "\n",
    "        #default\n",
    "        get_conf_mat = False\n",
    "        mat_labels = [0,1]\n",
    "\n",
    "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "            get_conf_mat = True #only get confuciton matrix when a single model is created\n",
    "        if model is None:\n",
    "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
    "            return\n",
    "\n",
    "        score = {}\n",
    "\n",
    "        #create predictions\n",
    "        #y_train_pred    = model.evaluate(self.df_train_x, self.df_train_y)\n",
    "        #y_valid_pred    = model.evaluate(self.df_valid_x, self.df_valid_y)\n",
    "        #y_test_pred     = model.evaluate(self.df_test_x, self.df_test_y)\n",
    "\n",
    "        y_train_pred    = (model.predict(self.df_train_x) > 0.5).astype(int)\n",
    "        y_valid_pred    = (model.predict(self.df_valid_x) > 0.5).astype(int)\n",
    "        y_test_pred     = (model.predict(self.df_test_x) > 0.5).astype(int)\n",
    "\n",
    "        #seperate t1 and t2 for individual scoring\n",
    "        for raw_key, y_pred, y in zip(\n",
    "            [\"train\",           \"valid\",            \"test\"],\n",
    "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
    "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
    "        ):\n",
    "\n",
    "            #not fetting test accurarcy if not set\n",
    "            if (get_test_score == False) and raw_key == \"test\":\n",
    "                continue\n",
    "\n",
    "            #split\n",
    "            y_pred_t1 = y_pred[:,0]\n",
    "            y_pred_t2 = y_pred[:,1]\n",
    "\n",
    "            y_t1 = y[self.y_col[0]]\n",
    "            y_t2 = y[self.y_col[1]]\n",
    "\n",
    "            #get acc\n",
    "            score[f\"{raw_key}_accuracy_t1\"]     = round(accuracy_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
    "            score[f\"{raw_key}_accuracy_t2\"]     = round(accuracy_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
    "            score[f\"{raw_key}_accuracy\"]        = round(accuracy_score(y_true = y, y_pred = y_pred),3)\n",
    "\n",
    "            #get conf mat\n",
    "            if get_conf_mat is True:\n",
    "                score[f\"{raw_key}_mat_t1\"]     = confusion_matrix(y_true = y_t1, y_pred = y_pred_t1, labels = mat_labels)\n",
    "                score[f\"{raw_key}_mat_t2\"]     = confusion_matrix(y_true = y_t2, y_pred = y_pred_t2, labels = mat_labels)\n",
    "                #score[f\"{raw_key}_mat\"]        = confusion_matrix(y_true = y, y_pred = y_pred) #multi labels are not supported\n",
    "\n",
    "        #return metrics\n",
    "        if get_conf_mat is True:\n",
    "            self.score = score\n",
    "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
    "            return\n",
    "\n",
    "        return score\n",
    "\n",
    "    def __get_model_score_r(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
    "        \"\"\"not yet fitted to keras\"\"\"\n",
    "\n",
    "        set_score = False\n",
    "\n",
    "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "            set_score = True\n",
    "        if model is None:\n",
    "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
    "            return\n",
    "\n",
    "        score = {}\n",
    "\n",
    "        #create predictions\n",
    "        y_train_pred    = model.predict(self.df_train_x)\n",
    "        y_valid_pred    = model.predict(self.df_valid_x)\n",
    "        y_test_pred     = model.predict(self.df_test_x)\n",
    "\n",
    "        #seperate t1 and t2 for individual scoring\n",
    "        for raw_key, y_pred, y in zip(\n",
    "            [\"train\",           \"valid\",            \"test\"],\n",
    "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
    "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
    "        ):\n",
    "\n",
    "            #not fetting test accurarcy if not set\n",
    "            if (get_test_score == False) and raw_key == \"test\":\n",
    "                continue\n",
    "\n",
    "            #split\n",
    "            y_pred_t1 = y_pred[:,0]\n",
    "            y_pred_t2 = y_pred[:,1]\n",
    "\n",
    "            y_t1 = y[self.y_col[0]]\n",
    "            y_t2 = y[self.y_col[1]]\n",
    "\n",
    "            #get r^2\n",
    "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
    "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
    "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
    "\n",
    "            #get rmse\n",
    "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
    "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
    "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
    "\n",
    "        #return metrics\n",
    "        if set_score:\n",
    "            self.score = score\n",
    "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
    "            return\n",
    "\n",
    "        return score\n",
    "\n",
    "    def plot_confusion_mat(self, set = \"valid\"):\n",
    "        \"\"\"set = 'train', 'valid', 'test'\"\"\"\n",
    "\n",
    "        mat_keys = [key for key in self.score.keys() if (\"mat\" in key) and (set in key)]\n",
    "\n",
    "        for mat_key in mat_keys:\n",
    "\n",
    "            mat = self.score[mat_key]\n",
    "            title = str(mat_key).replace(\"_mat_\", \" \")\n",
    "\n",
    "            fig  = px.imshow(\n",
    "                mat,\n",
    "                color_continuous_scale = px.colors.sequential.haline_r,\n",
    "                text_auto = True,\n",
    "            )\n",
    "\n",
    "            #labels and layout\n",
    "            fig.update_layout(\n",
    "\n",
    "                title = f\"Confusion matrix: {title}\",\n",
    "\n",
    "                width=500,\n",
    "                height=500,\n",
    "                \n",
    "                xaxis_title=\"Predicted label\",\n",
    "                yaxis_title=\"True label\",\n",
    "\n",
    "                xaxis = dict(\n",
    "                    tickmode = 'array',\n",
    "                    tickvals = [0,1],\n",
    "                    ticktext = [\"above\", \"below\"]\n",
    "                ),\n",
    "\n",
    "                yaxis = dict(\n",
    "                    tickmode = 'array',\n",
    "                    tickvals = [0,1],\n",
    "                    ticktext = [\"above\", \"below\"],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            #set font\n",
    "            fig.update_layout(\n",
    "                font = dict(size=16),\n",
    "                title_font = dict(size=20),\n",
    "                xaxis_title_font = dict(size=18),\n",
    "                yaxis_title_font = dict(size=18),\n",
    "            )\n",
    "\n",
    "            fig.show()\n",
    "\n",
    "    def save_result(self, param, score):\n",
    "\n",
    "        #merge and create a dataframe\n",
    "        param.update(score); data = param\n",
    "        df_result = pd.DataFrame([data])\n",
    "\n",
    "        #create results file and set header length as param to negate reading file\n",
    "        if os.path.isfile(self.results_file) is True:\n",
    "            df_saved_result = pd.read_csv(self.results_file)\n",
    "            df_result = df_saved_result.append(df_result)\n",
    "\n",
    "        df_result.to_csv(self.results_file, index = False)\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_results(self):\n",
    "\n",
    "        df = pd.read_csv(self.results_file)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "      if(logs.get(\"accuracy\") < 0.001):\n",
    "          print(\"\\nMAEthreshold reached. Training stopped.\")\n",
    "          self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RN(Base):\n",
    "\n",
    "    def run_optim(self):\n",
    "\n",
    "        self.model = None #clear any models if there should be one\n",
    "\n",
    "        arch_list = self.__generate_arch_list(\n",
    "            arch_log               = 2,\n",
    "            lin_arch_scaling       = 2,\n",
    "            n_layers               = 5,\n",
    "        )\n",
    "\n",
    "        for arch in arch_list:\n",
    "            print( f\"Progress of optim:\\t{round((arch_list.index(arch) / len(arch_list)) * 100,1)}\",end = \"\\r\")\n",
    "\n",
    "            #refromat to save all comparison data\n",
    "            if arch[0] == arch[-1]:\n",
    "                shape = \"linear\"\n",
    "            elif arch[0] > arch[-1]:\n",
    "                shape = \"cone\"\n",
    "            elif arch[0] < arch[-1]:\n",
    "                shape = \"cone_r\"\n",
    "\n",
    "            param = {\n",
    "                \"hidden_layer_sizes\"        : str(arch),\n",
    "                \"n_layers\"                  : len(arch),\n",
    "                \"n_neurons\"                 : sum(arch),\n",
    "                \"shape\"                     : shape,\n",
    "            }\n",
    "\n",
    "            #create model scoring to evalute models\n",
    "            score = self.create_model(arch = arch, single_model = False)\n",
    "            self.save_result(param = param, score = score)\n",
    "\n",
    "        print(\"Optim successfull. Read results with self.get_results()\")\n",
    "        return\n",
    "\n",
    "    def __generate_arch_list(self, arch_log = 2, lin_arch_scaling = 2, n_layers = 5):\n",
    "\n",
    "        arch_list : list = []\n",
    "        n_features = len(self.df_train_x.columns.tolist())\n",
    "\n",
    "        #nodes_pow_2 = [3 ** (cone_arch_base_power + p) for p in range(1, n_layers + 1)][::-1]\n",
    "        nodes_log_2 = [int(n_features * (1 / arch_log ** i)) for i in range(1,n_layers + 1)]\n",
    "\n",
    "        for n_layer in range(1, n_layers + 1):\n",
    "\n",
    "            #linear\n",
    "            for size in  [n_features / (i * lin_arch_scaling) for i in range(1,4)]:\n",
    "                arch_lin = [int(size)] * n_layer\n",
    "                arch_list.append(arch_lin)\n",
    "\n",
    "            #cone\n",
    "            arch_cone = nodes_log_2[:n_layer]\n",
    "            arch_list.append(arch_cone)\n",
    "\n",
    "            #cone r\n",
    "            arch_cone_r = arch_cone[::-1]\n",
    "            arch_list.append(arch_cone_r)\n",
    "\n",
    "        return arch_list\n",
    "\n",
    "    def create_model(self, arch, single_model = True):\n",
    "\n",
    "        shape = [len(self.df_train_x.columns.tolist())]\n",
    "\n",
    "        #init model\n",
    "        model = tf.keras.models.Sequential()\n",
    "\n",
    "        #dynamicaly scaling input layer\n",
    "        model.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.expand_dims(x, axis=-1),\n",
    "            input_shape=shape, #working: [None]\n",
    "        ))\n",
    "        \"\"\"\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units = len(self.df_train_x.columns.tolist()),\n",
    "            input_dim = 1,\n",
    "            activation = self.acitvation_func,\n",
    "        ))\n",
    "        \"\"\"\n",
    "\n",
    "        #add LSTM layers as hidden layers\n",
    "        for i in range(len(arch)):\n",
    "\n",
    "            if (i + 1) == len(arch):\n",
    "                return_sequences = False\n",
    "            else:\n",
    "                return_sequences = True\n",
    "\n",
    "            #hidden layers\n",
    "            model.add(tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(\n",
    "                    arch[i],\n",
    "                    #kernel_initializer = self.initializer,\n",
    "                    activation = self.acitvation_func,\n",
    "                    return_sequences = return_sequences,\n",
    "                )\n",
    "            ))\n",
    "\n",
    "        #add a Dense output layer\n",
    "        model.add(tf.keras.layers.Dense(units = 2, activation = self.acitvation_func)) #untis = 2, two binary classes\n",
    "\n",
    "        model = self.__compile_model(model)\n",
    "\n",
    "        #set according metrics\n",
    "        if single_model is True:\n",
    "            self.model = model\n",
    "            print(self.model)\n",
    "            return None\n",
    "\n",
    "        elif single_model is False:\n",
    "            score = self.get_model_score(model)\n",
    "            return score\n",
    "\n",
    "    def __compile_model(self, model):\n",
    "\n",
    "        model.compile(\n",
    "            loss = self.loss_func,\n",
    "            optimizer = self.solver,\n",
    "            metrics =  self.kears_metrics #accuracy\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            x = self.df_train_x,\n",
    "            y = self.df_train_y,\n",
    "            #validation_data = (self.df_valid_x, self.df_valid_y),\n",
    "            shuffle = False, #keep in order because it is time series data\n",
    "            epochs = self.n_epochs,\n",
    "            callbacks =[EarlyStopping()]\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnc_obj = RN(\n",
    "    df              = df,\n",
    "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "    n_jobs          = 5, #not applicable for keras models\n",
    "\n",
    "    data_folder     = data_folder,\n",
    "    results_file    = \"optim_reults_rnc.csv\",\n",
    "\n",
    "    model_metric    =  \"c\", # r = regression, c = classification\n",
    "    window          = 30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_optim:\n",
    "    rnc_obj.run_optim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnc_obj.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Networkplotter():\n",
    "\n",
    "    def plot_ann(arch = [10,3,2,1], title = \"ANN architecture\"):\n",
    "\n",
    "        structure = Networkplotter.create_neurons(arch) #df\n",
    "        connections = Networkplotter.create_connections(structure, arch) #dict\n",
    "        Networkplotter.draw_network(structure, connections, arch, title)\n",
    "\n",
    "    def create_neurons(arch):\n",
    "\n",
    "        structure = {\n",
    "            \"layer_pos\"  : [],\n",
    "            \"neuron_pos\" : [],\n",
    "        }\n",
    "\n",
    "        max_neurons = max(arch)\n",
    "        mid_pos = max_neurons / 2\n",
    "\n",
    "        for i in range(len(arch)):\n",
    "\n",
    "            neuron_pos = mid_pos - (arch[i] / 2)\n",
    "\n",
    "            for neuron in range(arch[i]):\n",
    "\n",
    "                structure[\"layer_pos\"].append(i),\n",
    "                structure[\"neuron_pos\"].append(neuron_pos)\n",
    "                neuron_pos += 1\n",
    "\n",
    "        return pd.DataFrame(structure)\n",
    "\n",
    "    def create_connections(structure, arch):\n",
    "\n",
    "        connections = {\n",
    "            \"x\" :   [], #(x1,x2), (x1,x2), layer_pos\n",
    "            \"y\" :   [], #(y1,y2), (y1,y2), neuron_pos\n",
    "        }\n",
    "\n",
    "        relevant_layers = list(range(len(arch)))[:-1]\n",
    "        relevant_neurons = structure.loc[structure[\"layer_pos\"].isin(relevant_layers)]\n",
    "\n",
    "        for i in range(relevant_neurons.shape[0]):\n",
    "\n",
    "            x1 = structure.iloc[i][\"layer_pos\"]\n",
    "            y1 = structure.iloc[i][\"neuron_pos\"]\n",
    "            x2 = x1 + 1\n",
    "\n",
    "            for j in structure.loc[structure[\"layer_pos\"] == x2].index.tolist():\n",
    "                y2 = float(structure.iloc[j][\"neuron_pos\"])\n",
    "\n",
    "                connections[\"x\"].append((x1,x2))\n",
    "                connections[\"y\"].append((y1,y2))\n",
    "\n",
    "        return connections\n",
    "\n",
    "    def draw_network(structure, connections, arch, title):\n",
    "\n",
    "        width   = len(arch) * 150\n",
    "        height  = 700\n",
    "        structure[\"size\"] = 1\n",
    "\n",
    "        fig_base = px.scatter(\n",
    "            data_frame = structure,\n",
    "            x = \"layer_pos\",\n",
    "            y = \"neuron_pos\",\n",
    "            size_max = 10,\n",
    "            size = \"size\",\n",
    "\n",
    "            title = title,\n",
    "\n",
    "            width = width,\n",
    "            height = height,\n",
    "            #color = \"neuron_pos\",\n",
    "            labels = {\"layer_pos\" : \"layer\", \"neuron_pos\" : \"\",}\n",
    "        )\n",
    "\n",
    "        data = fig_base.data\n",
    "        for i in range(len(list(connections[\"x\"]))):\n",
    "\n",
    "            fig_base.add_shape(\n",
    "                type='line',\n",
    "                x0 = connections[\"x\"][i][0], y0 = connections[\"y\"][i][0],\n",
    "                x1 = connections[\"x\"][i][1], y1 = connections[\"y\"][i][1],\n",
    "                line=dict(color=\"lightgrey\", width=2),\n",
    "                layer = \"below\",\n",
    "            )\n",
    "\n",
    "        tick_text = list(range(len(arch)))\n",
    "        tick_text[0] = \"Input layer\"\n",
    "        tick_text[-1] = \"Output layer\"\n",
    "\n",
    "        fig_base.update_layout(\n",
    "            xaxis = dict(\n",
    "                tickmode = 'array',\n",
    "                tickvals = list(range(len(arch))),\n",
    "                ticktext = tick_text,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #unlcean code\n",
    "        fig_base.update_yaxes(showticklabels=False)\n",
    "        fig_base.update_layout(\n",
    "            xaxis=dict(showgrid=False),\n",
    "            yaxis=dict(showgrid=False)\n",
    "        )\n",
    "        fig_base.update_layout({\n",
    "            \"plot_bgcolor\": \"rgba(255, 255, 255, 255)\",\n",
    "            \"paper_bgcolor\": \"rgba(255, 255, 255, 255)\",\n",
    "            })\n",
    "\n",
    "        fig_base.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_arch = [len(rnc_obj.df_train_x.columns.tolist()), 496, 496, 496, 4]\n",
    "ann_arch = [int(item / 30) if int(item / 30) > 0 else 1 for item in ann_arch]\n",
    "\n",
    "print(ann_arch)\n",
    "\n",
    "Networkplotter.plot_ann(\n",
    "    arch = ann_arch,\n",
    "    title = \"ANN architecture: RNN classifier\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
