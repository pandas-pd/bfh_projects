{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data gathering\n",
    "All data will be aggreagte to daily for furhter aggregation and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "\n",
    "#folders\n",
    "data_folder = \"data\"\n",
    "#data_folder = os.path.join(\"D:\",\"bthe_downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot styles\n",
    "plt_style_c = px.colors.sequential.haline #complex\n",
    "plt_style_s = px.colors.diverging.Portland #simple\n",
    "\n",
    "#defualt plot size \n",
    "size = {\n",
    "    \"width\" : 1500 ,\n",
    "    \"height\" : 750 ,\n",
    "}\n",
    "\n",
    "#function for plotting\n",
    "def scale_show(fig):\n",
    "\n",
    "    #set font\n",
    "    fig.update_layout(\n",
    "        font = dict(size=16),\n",
    "        title_font = dict(size=20),\n",
    "        xaxis_title_font = dict(size=18),\n",
    "        yaxis_title_font = dict(size=18),\n",
    "    )\n",
    "\n",
    "    #set size\n",
    "    fig.update_layout(\n",
    "        width=1500,\n",
    "        height=750,\n",
    "    )\n",
    "\n",
    "    #show\n",
    "    fig.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only used for initial data downliading and parsing\n",
    "\n",
    "try:\n",
    "    from unittest import result\n",
    "    import cdsapi #additional file needed to run. See docu\n",
    "    import requests\n",
    "    import xarray as xr\n",
    "except:\n",
    "    print(\"libs import failed. Not needed, unless th era5 data is to be downloaded anew and recompile the .nc to .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general parsing and aggregating of files\n",
    "save_data                   = False\n",
    "\n",
    "#chapter 0.1\n",
    "t2_run_era5_download        = False\n",
    "t2m_compile_df              = False\n",
    "\n",
    "#chapter 0.2\n",
    "soi_run_era5_download       = False\n",
    "soi_compile_df              = False\n",
    "\n",
    "#chapter 0.5\n",
    "pv_run_era5_download        = False\n",
    "pv_compile_df               = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom and dynamic aggregation funciton\n",
    "\n",
    "def dynamic_aggregation(df, grouping_col):\n",
    "    \"\"\"change to daily intervall\"\"\"\n",
    "\n",
    "    #get mean and std\n",
    "    df_mean = df.groupby([grouping_col], as_index = True).mean()\n",
    "    df_std = df.groupby([grouping_col], as_index = True).std()\n",
    "\n",
    "    #combine\n",
    "    df = df_mean.join(other = df_std, lsuffix=\"_mean\", rsuffix='_std')\n",
    "    df = df.round(2)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 General weather and temperature data (t2m)\n",
    "source: https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define borders\n",
    "north   = 47.8\n",
    "east    = 10.5\n",
    "south   = 45.8\n",
    "west    = 6.0\n",
    "\n",
    "lons = [west, west, east, east, west]\n",
    "lats = [south, north, north, south, south]\n",
    "\n",
    "#lons = [6,6,10.5,10.5,6]\n",
    "#lats = [45.8,47.8,47.8,45.8,45.8]\n",
    "\n",
    "#create plot \n",
    "fig = go.Figure(go.Scattermapbox(\n",
    "    mode = \"markers+lines\",\n",
    "    lon = lons,\n",
    "    lat = lats,\n",
    "    marker = {'size': 10})\n",
    ")\n",
    "\n",
    "#adjust view\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {\n",
    "        'center': {'lon': 8.4, 'lat': 46.85},\n",
    "        'style': \"carto-positron\",\n",
    "        'zoom': 5})\n",
    "\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data source: https://cds.climate.copernicus.eu/cdsapp#!/home\n",
    "\n",
    "class Wrapper():\n",
    "\n",
    "    #class variables\n",
    "    folder_name : str = None\n",
    "\n",
    "    #functionality\n",
    "    def main(all_vars : bool, data_folder):\n",
    "\n",
    "        Wrapper.folder_name = os.path.join(data_folder,\"raw_t2m\")\n",
    "\n",
    "        start_year : int        = 1979 #1979\n",
    "        end_year : int          = 2023 #2023\n",
    "\n",
    "        years : list            = Wrapper.generate_year_list(start = start_year, end = end_year)\n",
    "        variables : list        = Wrapper.generate_var_list(all = all_vars)\n",
    "\n",
    "        #main loop for downloading data\n",
    "        Wrapper.log(f\"Downloading startet for range: {start_year} - {end_year}\")\n",
    "\n",
    "        for year in years:\n",
    "            \n",
    "            print(f\"Processing {year}\")\n",
    "            result : str = Wrapper.request(year, variables, all_vars)\n",
    "\n",
    "        #tranforms and saves data as a csv for later processing in pandas\n",
    "        Wrapper.generate_df()\n",
    "\n",
    "        return\n",
    "\n",
    "    def generate_year_list(start:int, end:int):\n",
    "\n",
    "        year_list_str : list = [str(year) for year in range(start,end)]\n",
    "        return year_list_str\n",
    "\n",
    "    def generate_var_list( all : bool):\n",
    "\n",
    "        if all == True:\n",
    "            return [\n",
    "                '10m_u_component_of_wind',\n",
    "                '10m_v_component_of_wind',\n",
    "                '2m_temperature',\n",
    "                'clear_sky_direct_solar_radiation_at_surface',\n",
    "                'surface_pressure',]\n",
    "        else:\n",
    "            return ['2m_temperature']\n",
    "\n",
    "    def generate_df():\n",
    "\n",
    "        downloads = Wrapper.folder_name\n",
    "        files = os.listdir(downloads)\n",
    "        files = [file for file in files if file[-3:] == \".nc\"]\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            #open .nc files\n",
    "            file = os.path.join(downloads,file)\n",
    "\n",
    "            ds = xr.open_dataset(file)\n",
    "            df = ds.to_dataframe()\n",
    "\n",
    "            #save df\n",
    "            file = os.path.basename(file)\n",
    "            name = f\"{file[:-3]}.csv\"\n",
    "            path = os.path.join(Wrapper.folder_name,name)\n",
    "            df.to_csv(path)\n",
    "\n",
    "    def download_data(result:str, year:str, all_vars:bool):\n",
    "        \"\"\"[deprecated]\"\"\"\n",
    "\n",
    "        #genearte download and saving path\n",
    "        path : str          = Wrapper.download_path()\n",
    "        file_name : str     = f\"t2m_{year}_allvars_{all_vars}.nc\" #type nasCat data\n",
    "        file_path :str      = os.path.join(path,file_name)\n",
    "        print(file_path)\n",
    "\n",
    "        #get download link\n",
    "        try:\n",
    "            link_start : int    = result.index(\"location=\") + len (\"location=\")\n",
    "            url : str           = result[link_start:-1]\n",
    "        except:\n",
    "            Wrapper.log(f\"{year}: The api response does not contain a  download link\")\n",
    "            return\n",
    "\n",
    "        #retrieve data from web page and save it\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            Wrapper.log(f\"{year}: The download url is not valid\")\n",
    "            return\n",
    "\n",
    "        open(file_path, \"wb\").write(response.content)\n",
    "\n",
    "        return\n",
    "\n",
    "    def download_path():\n",
    "\n",
    "        folder_name = Wrapper.folder_name\n",
    "\n",
    "        if os.path.isdir(folder_name) == False:\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        download_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), folder_name)\n",
    "\n",
    "        return download_path\n",
    "\n",
    "    def file_path(year, all_vars):\n",
    "\n",
    "        #genearte download and saving path\n",
    "        path : str          = Wrapper.folder_name\n",
    "        file_name : str     = f\"t2m_{year}_allvars_{all_vars}.nc\" #type nasCat data\n",
    "        file_path :str      = os.path.join(path,file_name)\n",
    "\n",
    "        print(file_path)\n",
    "        return file_path\n",
    "\n",
    "    def log(message : str):\n",
    "\n",
    "        #create log entry\n",
    "        log_time : str = datetime.now()\n",
    "        message = f\"{log_time},{message}\\n\"\n",
    "\n",
    "        #write log entry\n",
    "        file_object = open('era5_log.txt', 'a')\n",
    "        file_object.write(message)\n",
    "        file_object.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    def request(year:list, variable:list, all_vars:bool):\n",
    "        # see: https://www.latlong.net/\n",
    "\n",
    "        c = cdsapi.Client()\n",
    "        file_path = Wrapper.file_path(year = year, all_vars = all_vars)\n",
    "\n",
    "        request = c.retrieve(\n",
    "            'reanalysis-era5-single-levels',\n",
    "            {\n",
    "                'product_type': 'reanalysis',\n",
    "                'variable': variable,\n",
    "                'year': year,\n",
    "                'month': [\n",
    "                    '01', '02', '03',\n",
    "                    '04', '05', '06',\n",
    "                    '07', '08', '09',\n",
    "                    '10', '11', '12',\n",
    "                ],\n",
    "                'day': [\n",
    "                    '01', '02', '03',\n",
    "                    '04', '05', '06',\n",
    "                    '07', '08', '09',\n",
    "                    '10', '11', '12',\n",
    "                    '13', '14', '15',\n",
    "                    '16', '17', '18',\n",
    "                    '19', '20', '21',\n",
    "                    '22', '23', '24',\n",
    "                    '25', '26', '27',\n",
    "                    '28', '29', '30',\n",
    "                    '31',\n",
    "                ],\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00',\n",
    "                    '03:00', '04:00', '05:00',\n",
    "                    '06:00', '07:00', '08:00',\n",
    "                    '09:00', '10:00', '11:00',\n",
    "                    '12:00', '13:00', '14:00',\n",
    "                    '15:00', '16:00', '17:00',\n",
    "                    '18:00', '19:00', '20:00',\n",
    "                    '21:00', '22:00', '23:00',\n",
    "                ],\n",
    "                'area': [47.8, 6, 45.8,10.5,],\n",
    "                'format': 'netcdf',\n",
    "            },\n",
    "            file_path\n",
    "        )\n",
    "\n",
    "        return str(request)\n",
    "\n",
    "if t2_run_era5_download is True:\n",
    "    Wrapper.main(all_vars = True, data_folder = data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregates to weekly interval data\n",
    "\n",
    "class Agg_t2m():\n",
    "\n",
    "    data = None\n",
    "    t2m_data :str = None\n",
    "\n",
    "    def main(data_folder):\n",
    "\n",
    "        Agg_t2m.data = data_folder\n",
    "        Agg_t2m.t2m_data = os.path.join(data_folder,\"raw_t2m\")\n",
    "\n",
    "        files : list        = os.listdir(Agg_t2m.t2m_data)\n",
    "        csv_files : list    = [file for file in files if file[-4:] == \".csv\"]\n",
    "\n",
    "        dfs : list          = []\n",
    "\n",
    "        for csv in csv_files:\n",
    "\n",
    "            print(f\"Processing: {csv}\", end = \"\\r\")\n",
    "            df = Agg_t2m.lonlat_mean(csv = csv)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = Agg_t2m.merge(dfs)\n",
    "        df = Agg_t2m.aggregate(df)\n",
    "        Agg_t2m.save(df, data_folder)\n",
    "\n",
    "        del dfs, df\n",
    "        return\n",
    "\n",
    "    def lonlat_mean(csv : str):\n",
    "\n",
    "        #average over lon and lattitude\n",
    "        df = pd.read_csv(os.path.join(Agg_t2m.t2m_data, csv))\n",
    "        df = df.drop(labels = [\"longitude\",\"latitude\"], axis = 1)\n",
    "        df_avg_lon_lat = df.groupby(\"time\").mean()\n",
    "\n",
    "        del df #free up memory\n",
    "        return df_avg_lon_lat\n",
    "\n",
    "    def aggregate(df):\n",
    "\n",
    "        #generate gorup index as date\n",
    "        df.reset_index(drop = False, inplace = True)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "        df[\"date\"] = df[\"time\"].dt.date\n",
    "\n",
    "        #set date as index\n",
    "        df.set_index(\"date\", drop = True, inplace = True)\n",
    "\n",
    "        #drop time column\n",
    "        df.drop(labels = [\"time\"], inplace = True, axis = 1)\n",
    "\n",
    "        #aggreagte data\n",
    "        df = df.groupby([\"date\"], as_index = True).mean()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def merge(dfs):\n",
    "\n",
    "        df = pd.concat(dfs)\n",
    "        #df.set_index(keys = \"date\", inplace = True)\n",
    "        df.sort_index(inplace = True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def save(df, data_folder):\n",
    "\n",
    "        df.to_csv(os.path.join(data_folder, \"df_t2m.csv\"))\n",
    "        return\n",
    "\n",
    "if t2m_compile_df is True:\n",
    "    Agg_t2m.main(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t2m = pd.read_csv(os.path.join(data_folder, \"df_t2m.csv\"), index_col = \"date\")\n",
    "df_t2m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t2m.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Southern oscilation index (SOI)\n",
    "source:\n",
    "- http://www.bom.gov.au/climate/mjo/\n",
    "- https://www.climate.gov/news-features/understanding-climate/climate-variability-southern-oscillation-index\n",
    "\n",
    "formula:\n",
    "- http://www.bom.gov.au/climate/glossary/soi.shtml\n",
    "- https://www.ncei.noaa.gov/access/monitoring/enso/soi\n",
    "\n",
    "\n",
    "The Southern Oscillation Index (SOI) is calculated using the atmospheric pressure difference between Tahiti and Darwin, Australia. The most common approach is to use monthly mean sea level pressure values for these two locations, which are then standardized and combined to create the SOI.\n",
    "\n",
    "- Tahiti: 17.5째S / 149.5째W\n",
    "- Darwin,  12.5째S / 131.5째E\n",
    "\n",
    "or\n",
    "\n",
    "- Tahiti (https://www.latlong.net/place/papeete-french-polynesia-30701.html):\n",
    "    - lat = -17.53\n",
    "    - lon = -149.56\n",
    "- Darwin (https://www.latlong.net/place/darwin-northern-territory-australia-5517.html):\n",
    "    - lat = -12.46\n",
    "    - lon = 130.84\n",
    "\n",
    "Here is a source for more information on the calculation of the SOI:\n",
    "https://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/soi.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_data = {\n",
    "    \"place\"     : [\"tahiti\",    \"darwin\"],\n",
    "    \"lat\"       : [-17.53,        -12.46],\n",
    "    \"lon\"       : [-149.56,       130.84],\n",
    "    \"size\"      : [1, 1],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(lonlat_data)\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    data_frame= df,\n",
    "    lat=\"lat\",\n",
    "    lon=\"lon\",\n",
    "    size = \"size\",\n",
    "    hover_name = \"place\",\n",
    "    size_max = 15,\n",
    "    color_continuous_scale = plt_style_s,\n",
    ")\n",
    "\n",
    "\n",
    "#adjust view\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {\n",
    "        'center': {'lon': 178, 'lat':-18},\n",
    "        'style': \"carto-positron\",\n",
    "        'zoom': 2})\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lat = north / south\n",
    "#lon = east / west\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the sea surface preassure data from copernicus\n",
    "\n",
    "class Wrapper():\n",
    "\n",
    "    #class variables\n",
    "    folder_name : str = None\n",
    "\n",
    "    #functionality\n",
    "    def main(data_folder):\n",
    "\n",
    "        Wrapper.folder_name = os.path.join(data_folder,\"raw_soi\")\n",
    "\n",
    "        start_year : int        = 1979 #1979\n",
    "        end_year : int          = 2023 #2024\n",
    "\n",
    "        years : list            = Wrapper.generate_year_list(start = start_year, end = end_year)\n",
    "        variables : list        = Wrapper.generate_var_list()\n",
    "\n",
    "        #set the two cites as previuously defined\n",
    "        cities = {\n",
    "            \"tahiti\" : [-17.50, -149.60, -17.50, -149.55,],\n",
    "            \"darwin\" : [-12.45, 130.80, -12.50, 130.85,],\n",
    "            }\n",
    "\n",
    "        #main loop for downloading data\n",
    "        Wrapper.log(f\"Downloading startet for range: {start_year} - {end_year}\")\n",
    "\n",
    "        for year in years:\n",
    "            for key in cities:\n",
    "                print(f\"Processing {year}, {key}\")\n",
    "                result : str = Wrapper.request(year = year, variable = variables, city = key , area = cities[key])\n",
    "\n",
    "        #tranforms and saves data as a csv for later processing in pandas\n",
    "        Wrapper.generate_df()\n",
    "\n",
    "        return\n",
    "\n",
    "    def generate_year_list(start:int, end:int):\n",
    "\n",
    "        year_list_str : list = [str(year) for year in range(start,end)]\n",
    "        return year_list_str\n",
    "\n",
    "    def generate_var_list():\n",
    "\n",
    "        return [\"mean_sea_level_pressure\"]\n",
    "\n",
    "    def generate_df():\n",
    "\n",
    "        downloads = Wrapper.folder_name\n",
    "        files = os.listdir(downloads)\n",
    "        files = [file for file in files if file[-3:] == \".nc\"]\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            #open .nc files\n",
    "            file = os.path.join(downloads,file)\n",
    "            ds = xr.open_dataset(file)\n",
    "            df = ds.to_dataframe()\n",
    "\n",
    "            #save df\n",
    "            file = os.path.basename(file)\n",
    "            name = f\"{file[:-3]}.csv\"\n",
    "            path = os.path.join(Wrapper.folder_name,name)\n",
    "            df.to_csv(path)\n",
    "\n",
    "    def download_path():\n",
    "\n",
    "        folder_name = Wrapper.folder_name\n",
    "\n",
    "        if os.path.isdir(folder_name) == False:\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        download_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), folder_name)\n",
    "\n",
    "        return download_path\n",
    "\n",
    "    def file_path(year, city):\n",
    "\n",
    "        #genearte download and saving path\n",
    "        path : str          = Wrapper.folder_name\n",
    "        file_name : str     = f\"soi_{year}_{city}.nc\" #type nasCat data\n",
    "        file_path :str      = os.path.join(path,file_name)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    def log(message : str):\n",
    "\n",
    "        #create log entry\n",
    "        log_time : str = datetime.now()\n",
    "        message = f\"{log_time},{message}\\n\"\n",
    "\n",
    "        #write log entry\n",
    "        file_object = open('era5_log.txt', 'a')\n",
    "        file_object.write(message)\n",
    "        file_object.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    def request(year:list, variable:list, area : list, city : str):\n",
    "        # see: https://www.latlong.net/\n",
    "\n",
    "        c = cdsapi.Client()\n",
    "        file_path = Wrapper.file_path(year = year, city = city)\n",
    "\n",
    "        request = c.retrieve(\n",
    "            'reanalysis-era5-single-levels',\n",
    "            {\n",
    "                'product_type': 'reanalysis',\n",
    "                'format': 'netcdf',\n",
    "                'variable': variable,\n",
    "                'area': area,\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00',\n",
    "                    '03:00', '04:00', '05:00',\n",
    "                    '06:00', '07:00', '08:00',\n",
    "                    '09:00', '10:00', '11:00',\n",
    "                    '12:00', '13:00', '14:00',\n",
    "                    '15:00', '16:00', '17:00',\n",
    "                    '18:00', '19:00', '20:00',\n",
    "                    '21:00', '22:00', '23:00',\n",
    "                ],\n",
    "                'day': [\n",
    "                    '01', '02', '03',\n",
    "                    '04', '05', '06',\n",
    "                    '07', '08', '09',\n",
    "                    '10', '11', '12',\n",
    "                    '13', '14', '15',\n",
    "                    '16', '17', '18',\n",
    "                    '19', '20', '21',\n",
    "                    '22', '23', '24',\n",
    "                    '25', '26', '27',\n",
    "                    '28', '29', '30',\n",
    "                    '31',\n",
    "                ],\n",
    "                'month': [\n",
    "                    '01', '02', '03',\n",
    "                    '04', '05', '06',\n",
    "                    '07', '08', '09',\n",
    "                    '10', '11', '12',\n",
    "                ],\n",
    "                'year': year,\n",
    "            },\n",
    "            file_path\n",
    "        )\n",
    "\n",
    "        return str(request)\n",
    "\n",
    "if soi_run_era5_download is True:\n",
    "    Wrapper.main(data_folder = data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compile_soi():\n",
    "\n",
    "    data = None\n",
    "    t2m_data :str = None\n",
    "\n",
    "    def main(data_folder):\n",
    "\n",
    "        Compile_soi.data = data_folder\n",
    "        Compile_soi.t2m_data = os.path.join(data_folder,\"raw_soi\")\n",
    "\n",
    "        files : list        = os.listdir(Compile_soi.t2m_data)\n",
    "\n",
    "        csv_files_tahiti : list    = [file for file in files if file[-4:] == \".csv\" and \"tahiti\" in file]\n",
    "        csv_files_darwin : list    = [file for file in files if file[-4:] == \".csv\" and \"darwin\" in file]\n",
    "\n",
    "        dfs : list          = []\n",
    "\n",
    "        for csv_tahiti, csv_darwin in zip(csv_files_tahiti, csv_files_darwin):\n",
    "            print(f\"processing:{csv_tahiti}, {csv_darwin}\")\n",
    "\n",
    "            #remove lon lat dependency\n",
    "            df_tahiti = Compile_soi.lonlat_mean(csv = csv_tahiti)\n",
    "            df_darwin = Compile_soi.lonlat_mean(csv = csv_darwin)\n",
    "\n",
    "            #combine the dataframes\n",
    "            df = Compile_soi.combine(df_tahiti = df_tahiti, df_darwin = df_darwin) #also aggreagte the data to daily interval\n",
    "            del df_tahiti, df_darwin\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = Compile_soi.merge(dfs)\n",
    "        df = Compile_soi.calculate_soi(df)\n",
    "        Compile_soi.save(df, Compile_soi.data)\n",
    "\n",
    "        return\n",
    "\n",
    "    def lonlat_mean(csv : str):\n",
    "\n",
    "        #average over lon and lattitude\n",
    "        df = pd.read_csv(os.path.join(Compile_soi.t2m_data, csv))\n",
    "        df = df.drop(labels = [\"longitude\",\"latitude\"], axis = 1)\n",
    "        df_avg_lon_lat = df.groupby(\"time\").mean()\n",
    "\n",
    "        del df #free up memory\n",
    "        return df_avg_lon_lat\n",
    "\n",
    "    def combine(df_tahiti, df_darwin):\n",
    "\n",
    "        #combine into one data frame\n",
    "        df = df_tahiti.join(other = df_darwin, lsuffix = \"_darwin\", rsuffix = \"_tahiti\", on = \"time\")\n",
    "\n",
    "        #aggregate\n",
    "        df = Compile_soi.aggregate(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def aggregate(df):\n",
    "\n",
    "        #generate gorup index as date\n",
    "        df.reset_index(drop = False, inplace = True)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "        df[\"date\"] = df[\"time\"].dt.date\n",
    "\n",
    "        #set date as index\n",
    "        df.set_index(\"date\", drop = True, inplace = True)\n",
    "\n",
    "        #drop time column\n",
    "        df.drop(labels = [\"time\"], inplace = True, axis = 1)\n",
    "\n",
    "        #aggreagte data\n",
    "        df = df.groupby([\"date\"], as_index = True).mean()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def merge(dfs):\n",
    "\n",
    "        df = pd.concat(dfs)\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def calculate_soi(df):\n",
    "        \"\"\"sorce of formula: http://www.bom.gov.au/climate/glossary/soi.shtml\"\"\"\n",
    "        # soi = 10 * (p_diff - p_diff_mean) / (p_diff_std)\n",
    "\n",
    "        #boilerplate\n",
    "        df[\"day_index\"] = df.index\n",
    "        df[\"day_index\"] = pd.to_datetime(df.index, errors='coerce')\n",
    "        df[\"day_index\"] = df[\"day_index\"].dt.strftime('%d-%m')\n",
    "\n",
    "        #get pressure diff\n",
    "        df[\"p_diff\"] = df[\"msl_tahiti\"] - df[\"msl_darwin\"]\n",
    "\n",
    "        #get long term values\n",
    "        df_long_term = df.groupby([\"day_index\"], as_index = False).agg(\n",
    "            p_diff_mean     = (\"p_diff\", \"mean\"),\n",
    "            p_diff_std      = (\"p_diff\", \"std\"),\n",
    "        )\n",
    "\n",
    "        #delete this later\n",
    "        df_long_term.fillna(20, inplace = True)\n",
    "\n",
    "        #join data\n",
    "        df[\"day_index\"] = df[\"day_index\"].astype(\"string\"); df_long_term[\"day_index\"] = df_long_term[\"day_index\"].astype(\"string\")\n",
    "        df.reset_index(inplace = True)\n",
    "        df = pd.merge(df, df_long_term, on = \"day_index\", how = \"left\")\n",
    "\n",
    "        #calculate soi\n",
    "        df[\"soi\"] = (df[\"p_diff\"] - df[\"p_diff_mean\"]) / df[\"p_diff_std\"]\n",
    "        df[\"soi\"] = df[\"soi\"] * (-1) #inverse\n",
    "\n",
    "        #clean up\n",
    "        df.drop(labels = [\"day_index\", \"p_diff\", \"msl_tahiti\", \"msl_darwin\", \"p_diff_mean\", \"p_diff_std\"], axis = 1, inplace = True)\n",
    "\n",
    "        #set index\n",
    "        df.set_index(\"date\", inplace = True, drop = True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def save(df, data_folder):\n",
    "\n",
    "        df.to_csv(os.path.join(data_folder, \"df_soi.csv\"))\n",
    "        return\n",
    "\n",
    "if soi_compile_df is True:\n",
    "    Compile_soi.main(data_folder = data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soi = pd.read_csv(os.path.join(data_folder, \"df_soi.csv\"))\n",
    "df_soi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complicate, but the normal way throws an unsolveable error\n",
    "df_soi[\"year\"] = pd.DatetimeIndex(df_soi[\"date\"]).year\n",
    "df_soi[\"month\"] = pd.DatetimeIndex(df_soi[\"date\"]).month\n",
    "\n",
    "#get mean\n",
    "df_soi = df_soi.groupby([\"year\", \"month\"], as_index = False).mean()\n",
    "df_soi[\"day\"] = \"01\"\n",
    "df_soi[\"index\"] = pd.to_datetime(df_soi[[\"year\", \"month\", \"day\"]])\n",
    "\n",
    "df_soi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare to monthly values of soi by trusted source\n",
    "#source: https://www.cpc.ncep.noaa.gov/data/indices/soi\n",
    "\n",
    "#read compare file \n",
    "df_enso_raw = pd.read_csv(os.path.join(data_folder, \"raw_enso\", \"raw_enso.csv\"))\n",
    "\n",
    "\n",
    "#new columns\n",
    "enso_dict : dict = {\n",
    "    \"index\" : [],\n",
    "    \"year\" : [],\n",
    "    \"month\" : [],\n",
    "    \"enso\" : [],\n",
    "}\n",
    "\n",
    "#iterrate over df to retrieve values\n",
    "for year in df_enso_raw[\"year\"].to_list():\n",
    "    for month in df_enso_raw.columns.to_list()[1:]:\n",
    "\n",
    "        enso : float        = float(df_enso_raw.loc[df_enso_raw[\"year\"] == year][str(month)])\n",
    "        index : str =       f\"{year}-{month}\"\n",
    "\n",
    "        enso_dict[\"index\"].append(index)\n",
    "        enso_dict[\"year\"].append(int(year))\n",
    "        enso_dict[\"month\"].append(int(month))\n",
    "        enso_dict[\"enso\"].append(enso)\n",
    "\n",
    "#create new df\n",
    "df_enso = pd.DataFrame(data = enso_dict)\n",
    "df_enso.set_index(\"index\", drop = True, inplace = True)\n",
    "\n",
    "#set column\n",
    "df_enso[\"day\"] = \"01\"\n",
    "df_enso[\"index\"] = pd.to_datetime(df_enso[[\"year\", \"month\", \"day\"]])\n",
    "\n",
    "df_enso.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create comparison\n",
    "df_soi_comp = pd.DataFrame()\n",
    "df_soi_comp[\"index\"] = df_soi[\"index\"]\n",
    "\n",
    "df_soi_comp[\"soi_calculated\"] = df_soi[\"soi\"].tolist()\n",
    "df_soi_comp[\"soi_source\"] = df_enso[\"enso\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for visual check\n",
    "#finding: they seem to be scaled differntly, but the shape is the same\n",
    "fig = px.line(\n",
    "    data_frame = df_soi_comp,\n",
    "    x = \"index\",\n",
    "    y = [\"soi_calculated\", \"soi_source\"],\n",
    "    title = \"SOI comparison\",\n",
    "    color_discrete_sequence = plt_style_s\n",
    ")\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_soi_comp[\"sacling\"]  = df_soi_comp[\"soi_source\"].abs() / df_soi_comp[\"soi_calculated\"].abs()\n",
    "scale_factor : float = df_soi_comp[\"sacling\"].mean()\n",
    "\n",
    "df_soi_comp[\"soi_source\"] = df_soi_comp[\"soi_source\"] / scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    data_frame = df_soi_comp,\n",
    "    x = \"index\",\n",
    "    y = [\"soi_calculated\", \"soi_source\"],\n",
    "    title = \"SOI comparison scaled\",\n",
    "    color_discrete_sequence = plt_style_s\n",
    ")\n",
    "\n",
    "scale_show(fig)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Madden julien oscillation (MJO)\n",
    "- Source: http://www.bom.gov.au/climate/mjo/\n",
    "- Note:\n",
    "    - The data was downloaded and prepared manually as a .csv file (reformating).\n",
    "    - The datapoint from the year 1978 are missing. A broken up date time series does not make sense. The enso also starts from the year 1979. Therefore the data from year < 1979 is being dropped\n",
    "    - The aggreagtion will be done in two ways:\n",
    "        - Values of the last day of month\n",
    "        - Aggregation:\n",
    "            - RMM1, RMM2, apmlitude: std, mean\n",
    "            - phase: mode\n",
    "- Range: 1979 - 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mjo_raw = pd.read_csv(os.path.join(data_folder,\"raw_mjo\",\"raw_mjo.csv\"))\n",
    "df_mjo_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding the mjo data\n",
    "fig = px.line(\n",
    "    data_frame = df_mjo_raw.loc[310:390],\n",
    "    x = \"RMM1\",\n",
    "    y = \"RMM2\",\n",
    "    color = \"month\",\n",
    "\n",
    "    title = \"MJO\",\n",
    "    color_discrete_sequence = plt_style_s,\n",
    "    width = 500,\n",
    "    height = 500,\n",
    "\n",
    "    range_x = (3,-3),\n",
    "    range_y = (3,-3),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    data_frame = df_mjo_raw,\n",
    "    y = \"amplitude\",\n",
    "\n",
    "    title = \"MJO\",\n",
    "    color_discrete_sequence = plt_style_s,\n",
    "    **size,\n",
    ")\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded cols\n",
    "df_mjo_raw.drop(axis = 1, labels = [\"Unnamed: 0\",\"unnamed\", \"MissingValue=1.E36or999\"], inplace = True)\n",
    "df_mjo = df_mjo_raw.loc[df_mjo_raw[\"year\"] >= 1979]\n",
    "df_mjo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    data_frame = df_mjo,\n",
    "    y = \"amplitude\",\n",
    "\n",
    "    title = \"MJO\",\n",
    "    color_discrete_sequence = plt_style_s,\n",
    "    **size,\n",
    ")\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mjo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mjo.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up and prep data\n",
    "df_mjo.reset_index(inplace = True, drop = True)\n",
    "df_mjo['date'] = pd.to_datetime(df_mjo[[\"year\", \"month\", \"day\"]])\n",
    "df_mjo.drop(labels = [\"year\", \"month\", \"day\"], axis = 1, inplace = True)\n",
    "\n",
    "#set index\n",
    "df_mjo.set_index(\"date\", drop = True, inplace = True)\n",
    "\n",
    "#clean up cols\n",
    "df_mjo = df_mjo.add_prefix(prefix = \"mjo_\")\n",
    "df_mjo.columns = [x.lower() for x in df_mjo.columns]\n",
    "\n",
    "#sort\n",
    "df_mjo.sort_index(inplace = True)\n",
    "\n",
    "df_mjo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data is True:\n",
    "    df_mjo.to_csv(os.path.join(data_folder, \"df_mjo.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Arctic oscillation index (AO)\n",
    "- source: https://ftp.cpc.ncep.noaa.gov/cwlinks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao_raw = pd.read_csv(os.path.join(data_folder, \"raw_ao\", \"norm_daily_ao_cda_z1000_19500101_current.csv\"))\n",
    "df_ao_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    data_frame = df_ao_raw,\n",
    "    y = \"ao_index_cdas\",\n",
    "\n",
    "    title = \"AO index\",\n",
    "    **size,\n",
    "    color_discrete_sequence = plt_style_s\n",
    ")\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao = df_ao_raw.rename(mapper = {\"ao_index_cdas\":\"ao\"}, inplace = False, axis = 1)\n",
    "df_ao.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao[df_ao.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    data_frame = df_ao.loc[(df_ao[\"year\"] == 2003) & (df_ao[\"month\"].isin([4,5]))],\n",
    "    y = \"ao\",\n",
    "    title = \"AO 2003\",\n",
    "    **size,\n",
    "    color_discrete_sequence = plt_style_s,\n",
    ")\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use interpolation to fill na value\n",
    "df_ao.interpolate(method = \"linear\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    data_frame = df_ao.loc[(df_ao[\"year\"] == 2003) & (df_ao[\"month\"].isin([4,5]))],\n",
    "    y = \"ao\",\n",
    "    title = \"ao 2003\",\n",
    "    **size,\n",
    "    color_discrete_sequence = plt_style_s,\n",
    ")\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up and prep data\n",
    "df_ao.reset_index(inplace = True, drop = True)\n",
    "df_ao[\"date\"] = pd.to_datetime(df_ao[[\"year\", \"month\", \"day\"]])\n",
    "df_ao.drop(labels = [\"year\", \"month\", \"day\"], axis = 1, inplace = True)\n",
    "\n",
    "#set index\n",
    "df_ao.set_index(\"date\", drop = True, inplace = True)\n",
    "\n",
    "#aggregagation not needed\n",
    "\n",
    "#sort\n",
    "df_ao.sort_index(inplace = True)\n",
    "\n",
    "df_ao.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data is True:\n",
    "    df_ao.to_csv(os.path.join(data_folder, \"df_ao.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 North atlantic oscilation (NAO)\n",
    "source: https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/nao.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nao_raw = pd.read_csv(os.path.join(data_folder, \"raw_nao\",\"norm_daily_nao_cdas_z500_19500101_current.csv\"))\n",
    "df_nao_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    data_frame = df_nao_raw,\n",
    "    y = \"nao_index_cdas\",\n",
    "    title = \"NAO\",\n",
    "    color_discrete_sequence = plt_style_s,\n",
    ")\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nao_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nao_raw[df_nao_raw.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill na with interpolation\n",
    "df_nao = df_nao_raw.interpolate(method = \"linear\")\n",
    "df_nao.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up and prep data\n",
    "df_nao.reset_index(inplace = True, drop = True)\n",
    "df_nao[\"date\"] = pd.to_datetime(df_nao[[\"year\", \"month\", \"day\"]])\n",
    "df_nao.drop(labels = [\"year\", \"month\", \"day\"], axis = 1, inplace = True)\n",
    "\n",
    "#set index\n",
    "df_nao.set_index(\"date\", drop = True, inplace = True)\n",
    "\n",
    "#aggregagation not needed\n",
    "\n",
    "#rename col\n",
    "df_nao.rename(mapper = {\"nao_index_cdas\" : \"nao\"}, axis = 1, inplace = True)\n",
    "\n",
    "#sort\n",
    "df_nao.sort_index(inplace = True)\n",
    "\n",
    "df_nao.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data is True:\n",
    "    df_nao.to_csv(os.path.join(data_folder, \"df_nao.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.6 Polar vortex data (PV)\n",
    "\n",
    "relevant preassure level: 1000 hPa level (see bthe, chapter 2.4 on polar vortex and ao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define borders\n",
    "north   = 90\n",
    "south   = 45.0\n",
    "west    = 8.0\n",
    "east    = 8.0\n",
    "\n",
    "\n",
    "lons = [west, west, east, east, west]\n",
    "lats = [south, north, north, south, south]\n",
    "\n",
    "#lons = [6,6,10.5,10.5,6]\n",
    "#lats = [45.8,47.8,47.8,45.8,45.8]\n",
    "\n",
    "#create plot \n",
    "fig = go.Figure(go.Scattermapbox(\n",
    "    mode = \"markers+lines\",\n",
    "    lon = lons,\n",
    "    lat = lats,\n",
    "    marker = {'size': 10})\n",
    ")\n",
    "\n",
    "#adjust view\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {\n",
    "        'center': {'lon': 8.4, 'lat': 60},\n",
    "        'style': \"carto-positron\",\n",
    "        'zoom': 3})\n",
    "\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resolution of data\n",
    "#define borders\n",
    "\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for lat in range(round(south) , round(north)):\n",
    "    for lon in [8,9]:\n",
    "        lons.append(lon)\n",
    "        lats.append(lat)\n",
    "\n",
    "#lons = [6,6,10.5,10.5,6]\n",
    "#lats = [45.8,47.8,47.8,45.8,45.8]\n",
    "\n",
    "print(f\"datapoint per time step: {len(lons)}\")\n",
    "print(f\"image resolution: {len(set(lons))} x {len(set(lats))}\")\n",
    "\n",
    "#create plot \n",
    "fig = go.Figure(go.Scattermapbox(\n",
    "    mode = \"markers\",\n",
    "    lon = lons,\n",
    "    lat = lats,\n",
    "    marker = {'size': 10})\n",
    ")\n",
    "\n",
    "#adjust view\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {\n",
    "        'center': {'lon': 8.4, 'lat': 60},\n",
    "        'style': \"carto-positron\",\n",
    "        'zoom': 3})\n",
    "\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data source: https://cds.climate.copernicus.eu/cdsapp#!/home\n",
    "\n",
    "class Wrapper():\n",
    "\n",
    "    #class variables\n",
    "    folder_name : str = None\n",
    "\n",
    "    #functionality\n",
    "    def main(data_folder):\n",
    "\n",
    "        Wrapper.folder_name = os.path.join(data_folder,\"raw_pv\")\n",
    "\n",
    "        start_year : int        = 1979 #1979\n",
    "        end_year : int          = 2023 #2023\n",
    "\n",
    "        years : list            = Wrapper.generate_year_list(start = start_year, end = end_year)\n",
    "        variable : list        = Wrapper.generate_var_list()\n",
    "\n",
    "        #main loop for downloading data\n",
    "        Wrapper.log(f\"Downloading startet for range: {start_year} - {end_year}\")\n",
    "\n",
    "        for year in years:\n",
    "            for month in range(1,13):\n",
    "\n",
    "                print(f\"Processing {year}-{month}\")\n",
    "\n",
    "                try:\n",
    "                    emailer.message(f\"Downloading pv: {year}-{month}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                result : str = Wrapper.request(year = year, month = month ,variable = variable)\n",
    "\n",
    "        #tranforms and saves data as a csv for later processing in pandas\n",
    "        Wrapper.generate_df()\n",
    "\n",
    "        return\n",
    "\n",
    "    def generate_year_list(start:int, end:int):\n",
    "\n",
    "        year_list_str : list = [str(year) for year in range(start,end)]\n",
    "        return year_list_str\n",
    "\n",
    "    def generate_var_list():\n",
    "\n",
    "        return ['u_component_of_wind', 'v_component_of_wind','temperature']\n",
    "\n",
    "    def generate_df():\n",
    "\n",
    "        downloads = Wrapper.folder_name\n",
    "        files = os.listdir(downloads)\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            #open .nc files\n",
    "            file = os.path.join(downloads,file)\n",
    "            ds = xr.open_dataset(file)\n",
    "            df = ds.to_dataframe()\n",
    "\n",
    "            #save df\n",
    "            file = os.path.basename(file)\n",
    "            name = f\"{file[:-3]}.csv\"\n",
    "            path = os.path.join(Wrapper.folder_name,name)\n",
    "            df.to_csv(path)\n",
    "\n",
    "    def download_path():\n",
    "\n",
    "        folder_name = Wrapper.folder_name\n",
    "\n",
    "        if os.path.isdir(folder_name) == False:\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        download_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), folder_name)\n",
    "\n",
    "        return download_path\n",
    "\n",
    "    def file_path(year, month):\n",
    "\n",
    "        #genearte download and saving path\n",
    "        path : str          = Wrapper.folder_name\n",
    "        file_name : str     = f\"pv_{year}-{month}.nc\" #type nasCat data\n",
    "        file_path :str      = os.path.join(path,file_name)\n",
    "\n",
    "        print(file_path)\n",
    "        return file_path\n",
    "\n",
    "    def log(message : str):\n",
    "\n",
    "        #create log entry\n",
    "        log_time : str = datetime.now()\n",
    "        message = f\"{log_time},{message}\\n\"\n",
    "\n",
    "        #write log entry\n",
    "        file_object = open('era5_log.txt', 'a')\n",
    "        file_object.write(message)\n",
    "        file_object.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    def request(year:int, month : int, variable:list):\n",
    "        # see: https://www.latlong.net/\n",
    "\n",
    "        c = cdsapi.Client()\n",
    "        file_path = Wrapper.file_path(year = year, month = month)\n",
    "\n",
    "        print(year)\n",
    "\n",
    "        request = c.retrieve(\n",
    "            'reanalysis-era5-pressure-levels',\n",
    "            {\n",
    "                'product_type': 'reanalysis',\n",
    "                'format': 'netcdf',\n",
    "                'pressure_level': ['10', '20', '30' ,'50', '70', '100',], #now it should be correct\n",
    "                'variable': variable,\n",
    "                'area': [\n",
    "                    90, 8, 45,\n",
    "                    9,\n",
    "                ],\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00',\n",
    "                    '03:00', '04:00', '05:00',\n",
    "                    '06:00', '07:00', '08:00',\n",
    "                    '09:00', '10:00', '11:00',\n",
    "                    '12:00', '13:00', '14:00',\n",
    "                    '15:00', '16:00', '17:00',\n",
    "                    '18:00', '19:00', '20:00',\n",
    "                    '21:00', '22:00', '23:00',\n",
    "                ],\n",
    "                'day': [\n",
    "                    '01', '02', '03',\n",
    "                    '04', '05', '06',\n",
    "                    '07', '08', '09',\n",
    "                    '10', '11', '12',\n",
    "                    '13', '14', '15',\n",
    "                    '16', '17', '18',\n",
    "                    '19', '20', '21',\n",
    "                    '22', '23', '24',\n",
    "                    '25', '26', '27',\n",
    "                    '28', '29', '30',\n",
    "                    '31',\n",
    "                ],\n",
    "                'month': month,\n",
    "                'year': year,\n",
    "            },\n",
    "            file_path\n",
    "        )\n",
    "\n",
    "        return str(request)\n",
    "\n",
    "if pv_run_era5_download is True:\n",
    "    Wrapper.main(data_folder = data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agg_pv():\n",
    "\n",
    "    data = None\n",
    "    pv_data :str = None\n",
    "\n",
    "    def main(data_folder):\n",
    "\n",
    "        Agg_pv.data = data_folder\n",
    "        Agg_pv.pv_data = os.path.join(data_folder,\"raw_pv\")\n",
    "\n",
    "        files : list        = os.listdir(Agg_pv.pv_data)\n",
    "        csv_files : list    = [file for file in files if file[-4:] == \".csv\"]\n",
    "\n",
    "        dfs : list          = []\n",
    "\n",
    "        for csv in csv_files:\n",
    "\n",
    "            print(f\"Processing: {csv}\", end = \"\\r\")\n",
    "            df = pd.read_csv(os.path.join(Agg_pv.pv_data, csv))\n",
    "            df = Agg_pv.aggregate(df)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = Agg_pv.merge(dfs)\n",
    "        \n",
    "        #add base infos\n",
    "        df = Agg_pv.calculate_wind_speeds(df)\n",
    "        df = Agg_pv.calculate_wind_direction(df)\n",
    "\n",
    "        Agg_pv.save(df, data_folder)\n",
    "\n",
    "        del dfs, df\n",
    "        return\n",
    "\n",
    "    def aggregate(df):\n",
    "\n",
    "        #generate gorup index as date\n",
    "        df.reset_index(drop = False, inplace = True)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "        df[\"date\"] = df[\"time\"].dt.date\n",
    "\n",
    "        #set date as index\n",
    "        df.set_index(\"date\", drop = True, inplace = True)\n",
    "\n",
    "        #drop time column\n",
    "        df.drop(labels = [\"time\"], inplace = True, axis = 1)\n",
    "\n",
    "        #define grid resolution\n",
    "        df[\"longitude\"] = df[\"longitude\"].round()\n",
    "        df[\"latitude\"] = df[\"latitude\"].round()\n",
    "\n",
    "        #aggreagte data\n",
    "        df = df.groupby([\"date\", \"longitude\",\"latitude\", \"level\"], as_index = True).mean()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def merge(dfs):\n",
    "\n",
    "        df = pd.concat(dfs)\n",
    "        #df.sort_values(by = \"date\", inplace = True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def calculate_wind_speeds(df):\n",
    "\n",
    "        df['speed'] = np.sqrt(df['u']**2 + df['v']**2)\n",
    "        return df\n",
    "\n",
    "    def calculate_wind_direction(df):\n",
    "\n",
    "        df['direction'] = np.rad2deg(np.arctan2(df['u'], df['v'])) % 360\n",
    "        df['direction'] = (df['direction'] + 90) % 360\n",
    "\n",
    "        return df\n",
    "\n",
    "    def save(df, data_folder):\n",
    "\n",
    "        df.rename(mapper = {\n",
    "            \"longitude\" : \"lon\",\n",
    "            \"latitude\" : \"lat\",\n",
    "            \"u\" : \"wind_u\",\n",
    "            \"v\" : \"wind_v\",\n",
    "            \"t\" : \"temp\",\n",
    "        })\n",
    "\n",
    "        if \"index\" in df.index.to_list():\n",
    "            df.drop(labels = \"index\", axis = 1, inplace = True)\n",
    "\n",
    "        df.to_csv(os.path.join(data_folder, \"df_pv.csv\"))\n",
    "        return\n",
    "\n",
    "if pv_compile_df is True:\n",
    "    Agg_pv.main(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv = pd.read_csv(os.path.join(data_folder, \"df_pv.csv\"), index_col = \"date\")\n",
    "df_pv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_pv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.7 Merge all (execpt custom polar vortex, date base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_contents = os.listdir(data_folder)\n",
    "csv_files = [\"df_ao.csv\", \"df_nao.csv\", \"df_mjo.csv\", \"df_soi.csv\", \"df_t2m.csv\"]\n",
    "\n",
    "dfs : list = []\n",
    "print(csv_files)\n",
    "\n",
    "#read files\n",
    "for csv in csv_files:\n",
    "\n",
    "    csv_path = os.path.join(data_folder, csv)\n",
    "    df = pd.read_csv(csv_path, index_col = \"date\")\n",
    "    print(df.columns)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "#merge files\n",
    "df = dfs[0].join(other = dfs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "df.reset_index(drop = False, inplace = True)\n",
    "\n",
    "#presumed cleaned data range\n",
    "start = \"1979-01-01\"\n",
    "end = \"2022-12-31\"\n",
    "\n",
    "#set\n",
    "df = df.loc[df[\"date\"].between(start, end)]\n",
    "df.set_index(\"date\", drop = True, inplace = True)\n",
    "\n",
    "#clean up\n",
    "[df.drop(labels = [col], inplace = True, axis = 1) for col in df.columns.tolist() if col == \"index\"]\n",
    "\n",
    "#chech\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addint date time values to df for plotting\n",
    "df[\"date_temp\"] = pd.to_datetime(df.index)\n",
    "\n",
    "#set values\n",
    "df[\"year\"] = df[\"date_temp\"].dt.year\n",
    "df[\"month\"] = df[\"date_temp\"].dt.month\n",
    "df[\"day\"] = df[\"date_temp\"].dt.day\n",
    "\n",
    "df.drop(labels = \"date_temp\", inplace = True, axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save master data frame\n",
    "if save_data is True:\n",
    "    df.to_csv(os.path.join(data_folder, \"df.csv\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6bbfc4e4f578cb9f7c85fe350d2fab0be0faacc19ccc874c1f1be2572a1188f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
