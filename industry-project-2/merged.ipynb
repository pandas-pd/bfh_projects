{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_gDRHEtn-Vd"
      },
      "source": [
        "# 0. Data gathering\n",
        "All data will be aggreagte to daily for furhter aggregation and feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFjur4iAn-Vf"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\"\n",
        "#data_folder = os.path.join(\"D:\",\"bthe_downloads\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37jQFw6Sn-Vi"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    fig.update_layout(\n",
        "        width=1500,\n",
        "        height=750,\n",
        "    )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY5wUk5Qn-Vk"
      },
      "outputs": [],
      "source": [
        "#only used for initial data downliading and parsing\n",
        "\n",
        "try:\n",
        "    from unittest import result\n",
        "    import cdsapi #additional file needed to run. See docu\n",
        "    import requests\n",
        "    import xarray as xr\n",
        "except:\n",
        "    print(\"libs import failed. Not needed, unless th era5 data is to be downloaded anew and recompile the .nc to .csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyVWbfuHn-Vl"
      },
      "outputs": [],
      "source": [
        "#general parsing and aggregating of files\n",
        "save_data                   = False\n",
        "\n",
        "#chapter 0.1\n",
        "t2_run_era5_download        = False\n",
        "t2m_compile_df              = False\n",
        "\n",
        "#chapter 0.2\n",
        "soi_run_era5_download       = False\n",
        "soi_compile_df              = False\n",
        "\n",
        "#chapter 0.5\n",
        "pv_run_era5_download        = False\n",
        "pv_compile_df               = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WvD1bKpn-Vm"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsiTEw-Jn-Vp"
      },
      "outputs": [],
      "source": [
        "#custom and dynamic aggregation funciton\n",
        "\n",
        "def dynamic_aggregation(df, grouping_col):\n",
        "    \"\"\"change to daily intervall\"\"\"\n",
        "\n",
        "    #get mean and std\n",
        "    df_mean = df.groupby([grouping_col], as_index = True).mean()\n",
        "    df_std = df.groupby([grouping_col], as_index = True).std()\n",
        "\n",
        "    #combine\n",
        "    df = df_mean.join(other = df_std, lsuffix=\"_mean\", rsuffix='_std')\n",
        "    df = df.round(2)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHd4FX2hn-Vq"
      },
      "source": [
        "## 0.1 General weather and temperature data (t2m)\n",
        "source: https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol6unxgln-Vr"
      },
      "outputs": [],
      "source": [
        "#define borders\n",
        "north   = 47.8\n",
        "east    = 10.5\n",
        "south   = 45.8\n",
        "west    = 6.0\n",
        "\n",
        "lons = [west, west, east, east, west]\n",
        "lats = [south, north, north, south, south]\n",
        "\n",
        "#lons = [6,6,10.5,10.5,6]\n",
        "#lats = [45.8,47.8,47.8,45.8,45.8]\n",
        "\n",
        "#create plot\n",
        "fig = go.Figure(go.Scattermapbox(\n",
        "    mode = \"markers+lines\",\n",
        "    lon = lons,\n",
        "    lat = lats,\n",
        "    marker = {'size': 10})\n",
        ")\n",
        "\n",
        "#adjust view\n",
        "fig.update_layout(\n",
        "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
        "    mapbox = {\n",
        "        'center': {'lon': 8.4, 'lat': 46.85},\n",
        "        'style': \"carto-positron\",\n",
        "        'zoom': 5})\n",
        "\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXSTVxcUn-Vs"
      },
      "outputs": [],
      "source": [
        "#data source: https://cds.climate.copernicus.eu/cdsapp#!/home\n",
        "\n",
        "class Wrapper():\n",
        "\n",
        "    #class variables\n",
        "    folder_name : str = None\n",
        "\n",
        "    #functionality\n",
        "    def main(all_vars : bool, data_folder):\n",
        "\n",
        "        Wrapper.folder_name = os.path.join(data_folder,\"raw_t2m\")\n",
        "\n",
        "        start_year : int        = 1979 #1979\n",
        "        end_year : int          = 2023 #2023\n",
        "\n",
        "        years : list            = Wrapper.generate_year_list(start = start_year, end = end_year)\n",
        "        variables : list        = Wrapper.generate_var_list(all = all_vars)\n",
        "\n",
        "        #main loop for downloading data\n",
        "        Wrapper.log(f\"Downloading startet for range: {start_year} - {end_year}\")\n",
        "\n",
        "        for year in years:\n",
        "\n",
        "            print(f\"Processing {year}\")\n",
        "            result : str = Wrapper.request(year, variables, all_vars)\n",
        "\n",
        "        #tranforms and saves data as a csv for later processing in pandas\n",
        "        Wrapper.generate_df()\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate_year_list(start:int, end:int):\n",
        "\n",
        "        year_list_str : list = [str(year) for year in range(start,end)]\n",
        "        return year_list_str\n",
        "\n",
        "    def generate_var_list( all : bool):\n",
        "\n",
        "        if all == True:\n",
        "            return [\n",
        "                '10m_u_component_of_wind',\n",
        "                '10m_v_component_of_wind',\n",
        "                '2m_temperature',\n",
        "                'clear_sky_direct_solar_radiation_at_surface',\n",
        "                'surface_pressure',]\n",
        "        else:\n",
        "            return ['2m_temperature']\n",
        "\n",
        "    def generate_df():\n",
        "\n",
        "        downloads = Wrapper.folder_name\n",
        "        files = os.listdir(downloads)\n",
        "        files = [file for file in files if file[-3:] == \".nc\"]\n",
        "\n",
        "        for file in files:\n",
        "\n",
        "            #open .nc files\n",
        "            file = os.path.join(downloads,file)\n",
        "\n",
        "            ds = xr.open_dataset(file)\n",
        "            df = ds.to_dataframe()\n",
        "\n",
        "            #save df\n",
        "            file = os.path.basename(file)\n",
        "            name = f\"{file[:-3]}.csv\"\n",
        "            path = os.path.join(Wrapper.folder_name,name)\n",
        "            df.to_csv(path)\n",
        "\n",
        "    def download_data(result:str, year:str, all_vars:bool):\n",
        "        \"\"\"[deprecated]\"\"\"\n",
        "\n",
        "        #genearte download and saving path\n",
        "        path : str          = Wrapper.download_path()\n",
        "        file_name : str     = f\"t2m_{year}_allvars_{all_vars}.nc\" #type nasCat data\n",
        "        file_path :str      = os.path.join(path,file_name)\n",
        "        print(file_path)\n",
        "\n",
        "        #get download link\n",
        "        try:\n",
        "            link_start : int    = result.index(\"location=\") + len (\"location=\")\n",
        "            url : str           = result[link_start:-1]\n",
        "        except:\n",
        "            Wrapper.log(f\"{year}: The api response does not contain a  download link\")\n",
        "            return\n",
        "\n",
        "        #retrieve data from web page and save it\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "        except:\n",
        "            Wrapper.log(f\"{year}: The download url is not valid\")\n",
        "            return\n",
        "\n",
        "        open(file_path, \"wb\").write(response.content)\n",
        "\n",
        "        return\n",
        "\n",
        "    def download_path():\n",
        "\n",
        "        folder_name = Wrapper.folder_name\n",
        "\n",
        "        if os.path.isdir(folder_name) == False:\n",
        "            os.makedirs(folder_name)\n",
        "\n",
        "        download_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), folder_name)\n",
        "\n",
        "        return download_path\n",
        "\n",
        "    def file_path(year, all_vars):\n",
        "\n",
        "        #genearte download and saving path\n",
        "        path : str          = Wrapper.folder_name\n",
        "        file_name : str     = f\"t2m_{year}_allvars_{all_vars}.nc\" #type nasCat data\n",
        "        file_path :str      = os.path.join(path,file_name)\n",
        "\n",
        "        print(file_path)\n",
        "        return file_path\n",
        "\n",
        "    def log(message : str):\n",
        "\n",
        "        #create log entry\n",
        "        log_time : str = datetime.now()\n",
        "        message = f\"{log_time},{message}\\n\"\n",
        "\n",
        "        #write log entry\n",
        "        file_object = open('era5_log.txt', 'a')\n",
        "        file_object.write(message)\n",
        "        file_object.close()\n",
        "\n",
        "        return\n",
        "\n",
        "    def request(year:list, variable:list, all_vars:bool):\n",
        "        # see: https://www.latlong.net/\n",
        "\n",
        "        c = cdsapi.Client()\n",
        "        file_path = Wrapper.file_path(year = year, all_vars = all_vars)\n",
        "\n",
        "        request = c.retrieve(\n",
        "            'reanalysis-era5-single-levels',\n",
        "            {\n",
        "                'product_type': 'reanalysis',\n",
        "                'variable': variable,\n",
        "                'year': year,\n",
        "                'month': [\n",
        "                    '01', '02', '03',\n",
        "                    '04', '05', '06',\n",
        "                    '07', '08', '09',\n",
        "                    '10', '11', '12',\n",
        "                ],\n",
        "                'day': [\n",
        "                    '01', '02', '03',\n",
        "                    '04', '05', '06',\n",
        "                    '07', '08', '09',\n",
        "                    '10', '11', '12',\n",
        "                    '13', '14', '15',\n",
        "                    '16', '17', '18',\n",
        "                    '19', '20', '21',\n",
        "                    '22', '23', '24',\n",
        "                    '25', '26', '27',\n",
        "                    '28', '29', '30',\n",
        "                    '31',\n",
        "                ],\n",
        "                'time': [\n",
        "                    '00:00', '01:00', '02:00',\n",
        "                    '03:00', '04:00', '05:00',\n",
        "                    '06:00', '07:00', '08:00',\n",
        "                    '09:00', '10:00', '11:00',\n",
        "                    '12:00', '13:00', '14:00',\n",
        "                    '15:00', '16:00', '17:00',\n",
        "                    '18:00', '19:00', '20:00',\n",
        "                    '21:00', '22:00', '23:00',\n",
        "                ],\n",
        "                'area': [47.8, 6, 45.8,10.5,],\n",
        "                'format': 'netcdf',\n",
        "            },\n",
        "            file_path\n",
        "        )\n",
        "\n",
        "        return str(request)\n",
        "\n",
        "if t2_run_era5_download is True:\n",
        "    Wrapper.main(all_vars = True, data_folder = data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_B9B4yIn-Vt"
      },
      "outputs": [],
      "source": [
        "#aggregates to weekly interval data\n",
        "\n",
        "class Agg_t2m():\n",
        "\n",
        "    data = None\n",
        "    t2m_data :str = None\n",
        "\n",
        "    def main(data_folder):\n",
        "\n",
        "        Agg_t2m.data = data_folder\n",
        "        Agg_t2m.t2m_data = os.path.join(data_folder,\"raw_t2m\")\n",
        "\n",
        "        files : list        = os.listdir(Agg_t2m.t2m_data)\n",
        "        csv_files : list    = [file for file in files if file[-4:] == \".csv\"]\n",
        "\n",
        "        dfs : list          = []\n",
        "\n",
        "        for csv in csv_files:\n",
        "\n",
        "            print(f\"Processing: {csv}\", end = \"\\r\")\n",
        "            df = Agg_t2m.lonlat_mean(csv = csv)\n",
        "            dfs.append(df)\n",
        "\n",
        "        df = Agg_t2m.merge(dfs)\n",
        "        df = Agg_t2m.aggregate(df)\n",
        "        Agg_t2m.save(df, data_folder)\n",
        "\n",
        "        del dfs, df\n",
        "        return\n",
        "\n",
        "    def lonlat_mean(csv : str):\n",
        "\n",
        "        #average over lon and lattitude\n",
        "        df = pd.read_csv(os.path.join(Agg_t2m.t2m_data, csv))\n",
        "        df = df.drop(labels = [\"longitude\",\"latitude\"], axis = 1)\n",
        "        df_avg_lon_lat = df.groupby(\"time\").mean()\n",
        "\n",
        "        del df #free up memory\n",
        "        return df_avg_lon_lat\n",
        "\n",
        "    def aggregate(df):\n",
        "\n",
        "        #generate gorup index as date\n",
        "        df.reset_index(drop = False, inplace = True)\n",
        "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "        df[\"date\"] = df[\"time\"].dt.date\n",
        "\n",
        "        #set date as index\n",
        "        df.set_index(\"date\", drop = True, inplace = True)\n",
        "\n",
        "        #drop time column\n",
        "        df.drop(labels = [\"time\"], inplace = True, axis = 1)\n",
        "\n",
        "        #aggreagte data\n",
        "        df = df.groupby([\"date\"], as_index = True).mean()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def merge(dfs):\n",
        "\n",
        "        df = pd.concat(dfs)\n",
        "        #df.set_index(keys = \"date\", inplace = True)\n",
        "        df.sort_index(inplace = True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def save(df, data_folder):\n",
        "\n",
        "        df.to_csv(os.path.join(data_folder, \"df_t2m.csv\"))\n",
        "        return\n",
        "\n",
        "if t2m_compile_df is True:\n",
        "    Agg_t2m.main(data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBXWqylnn-Vu"
      },
      "outputs": [],
      "source": [
        "df_t2m = pd.read_csv(os.path.join(data_folder, \"df_t2m.csv\"), index_col = \"date\")\n",
        "df_t2m.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ew9BZg9n-Vu"
      },
      "outputs": [],
      "source": [
        "df_t2m.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-5nVltn-Vv"
      },
      "source": [
        "## 0.2 Southern oscilation index (SOI)\n",
        "source:\n",
        "- http://www.bom.gov.au/climate/mjo/\n",
        "- https://www.climate.gov/news-features/understanding-climate/climate-variability-southern-oscillation-index\n",
        "\n",
        "formula:\n",
        "- http://www.bom.gov.au/climate/glossary/soi.shtml\n",
        "- https://www.ncei.noaa.gov/access/monitoring/enso/soi\n",
        "\n",
        "\n",
        "The Southern Oscillation Index (SOI) is calculated using the atmospheric pressure difference between Tahiti and Darwin, Australia. The most common approach is to use monthly mean sea level pressure values for these two locations, which are then standardized and combined to create the SOI.\n",
        "\n",
        "- Tahiti: 17.5░S / 149.5░W\n",
        "- Darwin,  12.5░S / 131.5░E\n",
        "\n",
        "or\n",
        "\n",
        "- Tahiti (https://www.latlong.net/place/papeete-french-polynesia-30701.html):\n",
        "    - lat = -17.53\n",
        "    - lon = -149.56\n",
        "- Darwin (https://www.latlong.net/place/darwin-northern-territory-australia-5517.html):\n",
        "    - lat = -12.46\n",
        "    - lon = 130.84\n",
        "\n",
        "Here is a source for more information on the calculation of the SOI:\n",
        "https://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/soi.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZqEWjXEn-Vv"
      },
      "outputs": [],
      "source": [
        "lonlat_data = {\n",
        "    \"place\"     : [\"tahiti\",    \"darwin\"],\n",
        "    \"lat\"       : [-17.53,        -12.46],\n",
        "    \"lon\"       : [-149.56,       130.84],\n",
        "    \"size\"      : [1, 1],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(lonlat_data)\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    data_frame= df,\n",
        "    lat=\"lat\",\n",
        "    lon=\"lon\",\n",
        "    size = \"size\",\n",
        "    hover_name = \"place\",\n",
        "    size_max = 15,\n",
        "    color_continuous_scale = plt_style_s,\n",
        ")\n",
        "\n",
        "\n",
        "#adjust view\n",
        "fig.update_layout(\n",
        "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
        "    mapbox = {\n",
        "        'center': {'lon': 178, 'lat':-18},\n",
        "        'style': \"carto-positron\",\n",
        "        'zoom': 2})\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70gSctuon-Vw"
      },
      "outputs": [],
      "source": [
        "#lat = north / south\n",
        "#lon = east / west\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaPMoZ61n-Vx"
      },
      "outputs": [],
      "source": [
        "#fetch the sea surface preassure data from copernicus\n",
        "\n",
        "class Wrapper():\n",
        "\n",
        "    #class variables\n",
        "    folder_name : str = None\n",
        "\n",
        "    #functionality\n",
        "    def main(data_folder):\n",
        "\n",
        "        Wrapper.folder_name = os.path.join(data_folder,\"raw_soi\")\n",
        "\n",
        "        start_year : int        = 1979 #1979\n",
        "        end_year : int          = 2023 #2024\n",
        "\n",
        "        years : list            = Wrapper.generate_year_list(start = start_year, end = end_year)\n",
        "        variables : list        = Wrapper.generate_var_list()\n",
        "\n",
        "        #set the two cites as previuously defined\n",
        "        cities = {\n",
        "            \"tahiti\" : [-17.50, -149.60, -17.50, -149.55,],\n",
        "            \"darwin\" : [-12.45, 130.80, -12.50, 130.85,],\n",
        "            }\n",
        "\n",
        "        #main loop for downloading data\n",
        "        Wrapper.log(f\"Downloading startet for range: {start_year} - {end_year}\")\n",
        "\n",
        "        for year in years:\n",
        "            for key in cities:\n",
        "                print(f\"Processing {year}, {key}\")\n",
        "                result : str = Wrapper.request(year = year, variable = variables, city = key , area = cities[key])\n",
        "\n",
        "        #tranforms and saves data as a csv for later processing in pandas\n",
        "        Wrapper.generate_df()\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate_year_list(start:int, end:int):\n",
        "\n",
        "        year_list_str : list = [str(year) for year in range(start,end)]\n",
        "        return year_list_str\n",
        "\n",
        "    def generate_var_list():\n",
        "\n",
        "        return [\"mean_sea_level_pressure\"]\n",
        "\n",
        "    def generate_df():\n",
        "\n",
        "        downloads = Wrapper.folder_name\n",
        "        files = os.listdir(downloads)\n",
        "        files = [file for file in files if file[-3:] == \".nc\"]\n",
        "\n",
        "        for file in files:\n",
        "\n",
        "            #open .nc files\n",
        "            file = os.path.join(downloads,file)\n",
        "            ds = xr.open_dataset(file)\n",
        "            df = ds.to_dataframe()\n",
        "\n",
        "            #save df\n",
        "            file = os.path.basename(file)\n",
        "            name = f\"{file[:-3]}.csv\"\n",
        "            path = os.path.join(Wrapper.folder_name,name)\n",
        "            df.to_csv(path)\n",
        "\n",
        "    def download_path():\n",
        "\n",
        "        folder_name = Wrapper.folder_name\n",
        "\n",
        "        if os.path.isdir(folder_name) == False:\n",
        "            os.makedirs(folder_name)\n",
        "\n",
        "        download_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), folder_name)\n",
        "\n",
        "        return download_path\n",
        "\n",
        "    def file_path(year, city):\n",
        "\n",
        "        #genearte download and saving path\n",
        "        path : str          = Wrapper.folder_name\n",
        "        file_name : str     = f\"soi_{year}_{city}.nc\" #type nasCat data\n",
        "        file_path :str      = os.path.join(path,file_name)\n",
        "\n",
        "        return file_path\n",
        "\n",
        "    def log(message : str):\n",
        "\n",
        "        #create log entry\n",
        "        log_time : str = datetime.now()\n",
        "        message = f\"{log_time},{message}\\n\"\n",
        "\n",
        "        #write log entry\n",
        "        file_object = open('era5_log.txt', 'a')\n",
        "        file_object.write(message)\n",
        "        file_object.close()\n",
        "\n",
        "        return\n",
        "\n",
        "    def request(year:list, variable:list, area : list, city : str):\n",
        "        # see: https://www.latlong.net/\n",
        "\n",
        "        c = cdsapi.Client()\n",
        "        file_path = Wrapper.file_path(year = year, city = city)\n",
        "\n",
        "        request = c.retrieve(\n",
        "            'reanalysis-era5-single-levels',\n",
        "            {\n",
        "                'product_type': 'reanalysis',\n",
        "                'format': 'netcdf',\n",
        "                'variable': variable,\n",
        "                'area': area,\n",
        "                'time': [\n",
        "                    '00:00', '01:00', '02:00',\n",
        "                    '03:00', '04:00', '05:00',\n",
        "                    '06:00', '07:00', '08:00',\n",
        "                    '09:00', '10:00', '11:00',\n",
        "                    '12:00', '13:00', '14:00',\n",
        "                    '15:00', '16:00', '17:00',\n",
        "                    '18:00', '19:00', '20:00',\n",
        "                    '21:00', '22:00', '23:00',\n",
        "                ],\n",
        "                'day': [\n",
        "                    '01', '02', '03',\n",
        "                    '04', '05', '06',\n",
        "                    '07', '08', '09',\n",
        "                    '10', '11', '12',\n",
        "                    '13', '14', '15',\n",
        "                    '16', '17', '18',\n",
        "                    '19', '20', '21',\n",
        "                    '22', '23', '24',\n",
        "                    '25', '26', '27',\n",
        "                    '28', '29', '30',\n",
        "                    '31',\n",
        "                ],\n",
        "                'month': [\n",
        "                    '01', '02', '03',\n",
        "                    '04', '05', '06',\n",
        "                    '07', '08', '09',\n",
        "                    '10', '11', '12',\n",
        "                ],\n",
        "                'year': year,\n",
        "            },\n",
        "            file_path\n",
        "        )\n",
        "\n",
        "        return str(request)\n",
        "\n",
        "if soi_run_era5_download is True:\n",
        "    Wrapper.main(data_folder = data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJR9rQYRn-Vy"
      },
      "outputs": [],
      "source": [
        "class Compile_soi():\n",
        "\n",
        "    data = None\n",
        "    t2m_data :str = None\n",
        "\n",
        "    def main(data_folder):\n",
        "\n",
        "        Compile_soi.data = data_folder\n",
        "        Compile_soi.t2m_data = os.path.join(data_folder,\"raw_soi\")\n",
        "\n",
        "        files : list        = os.listdir(Compile_soi.t2m_data)\n",
        "\n",
        "        csv_files_tahiti : list    = [file for file in files if file[-4:] == \".csv\" and \"tahiti\" in file]\n",
        "        csv_files_darwin : list    = [file for file in files if file[-4:] == \".csv\" and \"darwin\" in file]\n",
        "\n",
        "        dfs : list          = []\n",
        "\n",
        "        for csv_tahiti, csv_darwin in zip(csv_files_tahiti, csv_files_darwin):\n",
        "            print(f\"processing:{csv_tahiti}, {csv_darwin}\")\n",
        "\n",
        "            #remove lon lat dependency\n",
        "            df_tahiti = Compile_soi.lonlat_mean(csv = csv_tahiti)\n",
        "            df_darwin = Compile_soi.lonlat_mean(csv = csv_darwin)\n",
        "\n",
        "            #combine the dataframes\n",
        "            df = Compile_soi.combine(df_tahiti = df_tahiti, df_darwin = df_darwin) #also aggreagte the data to daily interval\n",
        "            del df_tahiti, df_darwin\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "        df = Compile_soi.merge(dfs)\n",
        "        df = Compile_soi.calculate_soi(df)\n",
        "        Compile_soi.save(df, Compile_soi.data)\n",
        "\n",
        "        return\n",
        "\n",
        "    def lonlat_mean(csv : str):\n",
        "\n",
        "        #average over lon and lattitude\n",
        "        df = pd.read_csv(os.path.join(Compile_soi.t2m_data, csv))\n",
        "        df = df.drop(labels = [\"longitude\",\"latitude\"], axis = 1)\n",
        "        df_avg_lon_lat = df.groupby(\"time\").mean()\n",
        "\n",
        "        del df #free up memory\n",
        "        return df_avg_lon_lat\n",
        "\n",
        "    def combine(df_tahiti, df_darwin):\n",
        "\n",
        "        #combine into one data frame\n",
        "        df = df_tahiti.join(other = df_darwin, lsuffix = \"_darwin\", rsuffix = \"_tahiti\", on = \"time\")\n",
        "\n",
        "        #aggregate\n",
        "        df = Compile_soi.aggregate(df)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def aggregate(df):\n",
        "\n",
        "        #generate gorup index as date\n",
        "        df.reset_index(drop = False, inplace = True)\n",
        "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "        df[\"date\"] = df[\"time\"].dt.date\n",
        "\n",
        "        #set date as index\n",
        "        df.set_index(\"date\", drop = True, inplace = True)\n",
        "\n",
        "        #drop time column\n",
        "        df.drop(labels = [\"time\"], inplace = True, axis = 1)\n",
        "\n",
        "        #aggreagte data\n",
        "        df = df.groupby([\"date\"], as_index = True).mean()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def merge(dfs):\n",
        "\n",
        "        df = pd.concat(dfs)\n",
        "        df.sort_index(inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def calculate_soi(df):\n",
        "        \"\"\"sorce of formula: http://www.bom.gov.au/climate/glossary/soi.shtml\"\"\"\n",
        "        # soi = 10 * (p_diff - p_diff_mean) / (p_diff_std)\n",
        "\n",
        "        #boilerplate\n",
        "        df[\"day_index\"] = df.index\n",
        "        df[\"day_index\"] = pd.to_datetime(df.index, errors='coerce')\n",
        "        df[\"day_index\"] = df[\"day_index\"].dt.strftime('%d-%m')\n",
        "\n",
        "        #get pressure diff\n",
        "        df[\"p_diff\"] = df[\"msl_tahiti\"] - df[\"msl_darwin\"]\n",
        "\n",
        "        #get long term values\n",
        "        df_long_term = df.groupby([\"day_index\"], as_index = False).agg(\n",
        "            p_diff_mean     = (\"p_diff\", \"mean\"),\n",
        "            p_diff_std      = (\"p_diff\", \"std\"),\n",
        "        )\n",
        "\n",
        "        #delete this later\n",
        "        df_long_term.fillna(20, inplace = True)\n",
        "\n",
        "        #join data\n",
        "        df[\"day_index\"] = df[\"day_index\"].astype(\"string\"); df_long_term[\"day_index\"] = df_long_term[\"day_index\"].astype(\"string\")\n",
        "        df.reset_index(inplace = True)\n",
        "        df = pd.merge(df, df_long_term, on = \"day_index\", how = \"left\")\n",
        "\n",
        "        #calculate soi\n",
        "        df[\"soi\"] = (df[\"p_diff\"] - df[\"p_diff_mean\"]) / df[\"p_diff_std\"]\n",
        "        df[\"soi\"] = df[\"soi\"] * (-1) #inverse\n",
        "\n",
        "        #clean up\n",
        "        df.drop(labels = [\"day_index\", \"p_diff\", \"msl_tahiti\", \"msl_darwin\", \"p_diff_mean\", \"p_diff_std\"], axis = 1, inplace = True)\n",
        "\n",
        "        #set index\n",
        "        df.set_index(\"date\", inplace = True, drop = True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def save(df, data_folder):\n",
        "\n",
        "        df.to_csv(os.path.join(data_folder, \"df_soi.csv\"))\n",
        "        return\n",
        "\n",
        "if soi_compile_df is True:\n",
        "    Compile_soi.main(data_folder = data_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_4xBnfin-Vz"
      },
      "outputs": [],
      "source": [
        "df_soi = pd.read_csv(os.path.join(data_folder, \"df_soi.csv\"))\n",
        "df_soi.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUITEc5sn-Vz"
      },
      "outputs": [],
      "source": [
        "#complicate, but the normal way throws an unsolveable error\n",
        "df_soi[\"year\"] = pd.DatetimeIndex(df_soi[\"date\"]).year\n",
        "df_soi[\"month\"] = pd.DatetimeIndex(df_soi[\"date\"]).month\n",
        "\n",
        "#get mean\n",
        "df_soi = df_soi.groupby([\"year\", \"month\"], as_index = False).mean()\n",
        "df_soi[\"day\"] = \"01\"\n",
        "df_soi[\"index\"] = pd.to_datetime(df_soi[[\"year\", \"month\", \"day\"]])\n",
        "\n",
        "df_soi.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GtdSlu9n-V0"
      },
      "outputs": [],
      "source": [
        "#compare to monthly values of soi by trusted source\n",
        "#source: https://www.cpc.ncep.noaa.gov/data/indices/soi\n",
        "\n",
        "#read compare file\n",
        "df_enso_raw = pd.read_csv(os.path.join(data_folder, \"raw_enso\", \"raw_enso.csv\"))\n",
        "\n",
        "\n",
        "#new columns\n",
        "enso_dict : dict = {\n",
        "    \"index\" : [],\n",
        "    \"year\" : [],\n",
        "    \"month\" : [],\n",
        "    \"enso\" : [],\n",
        "}\n",
        "\n",
        "#iterrate over df to retrieve values\n",
        "for year in df_enso_raw[\"year\"].to_list():\n",
        "    for month in df_enso_raw.columns.to_list()[1:]:\n",
        "\n",
        "        enso : float        = float(df_enso_raw.loc[df_enso_raw[\"year\"] == year][str(month)])\n",
        "        index : str =       f\"{year}-{month}\"\n",
        "\n",
        "        enso_dict[\"index\"].append(index)\n",
        "        enso_dict[\"year\"].append(int(year))\n",
        "        enso_dict[\"month\"].append(int(month))\n",
        "        enso_dict[\"enso\"].append(enso)\n",
        "\n",
        "#create new df\n",
        "df_enso = pd.DataFrame(data = enso_dict)\n",
        "df_enso.set_index(\"index\", drop = True, inplace = True)\n",
        "\n",
        "#set column\n",
        "df_enso[\"day\"] = \"01\"\n",
        "df_enso[\"index\"] = pd.to_datetime(df_enso[[\"year\", \"month\", \"day\"]])\n",
        "\n",
        "df_enso.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aFP4iGQn-V0"
      },
      "outputs": [],
      "source": [
        "#create comparison\n",
        "df_soi_comp = pd.DataFrame()\n",
        "df_soi_comp[\"index\"] = df_soi[\"index\"]\n",
        "\n",
        "df_soi_comp[\"soi_calculated\"] = df_soi[\"soi\"].tolist()\n",
        "df_soi_comp[\"soi_source\"] = df_enso[\"enso\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z22ZNrDen-V1"
      },
      "outputs": [],
      "source": [
        "#plot for visual check\n",
        "#finding: they seem to be scaled differntly, but the shape is the same\n",
        "fig = px.line(\n",
        "    data_frame = df_soi_comp,\n",
        "    x = \"index\",\n",
        "    y = [\"soi_calculated\", \"soi_source\"],\n",
        "    title = \"SOI comparison\",\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83BBXc7tn-V1"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_soi_comp[\"sacling\"]  = df_soi_comp[\"soi_source\"].abs() / df_soi_comp[\"soi_calculated\"].abs()\n",
        "scale_factor : float = df_soi_comp[\"sacling\"].mean()\n",
        "\n",
        "df_soi_comp[\"soi_source\"] = df_soi_comp[\"soi_source\"] / scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz2oyKC_n-V1"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_soi_comp,\n",
        "    x = \"index\",\n",
        "    y = [\"soi_calculated\", \"soi_source\"],\n",
        "    title = \"SOI comparison scaled\",\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88X_cZ5Rn-V2"
      },
      "source": [
        "## 0.3 Madden julien oscillation (MJO)\n",
        "- Source: http://www.bom.gov.au/climate/mjo/\n",
        "- Note:\n",
        "    - The data was downloaded and prepared manually as a .csv file (reformating).\n",
        "    - The datapoint from the year 1978 are missing. A broken up date time series does not make sense. The enso also starts from the year 1979. Therefore the data from year < 1979 is being dropped\n",
        "    - The aggreagtion will be done in two ways:\n",
        "        - Values of the last day of month\n",
        "        - Aggregation:\n",
        "            - RMM1, RMM2, apmlitude: std, mean\n",
        "            - phase: mode\n",
        "- Range: 1979 - 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0bV8DPYn-V2"
      },
      "outputs": [],
      "source": [
        "df_mjo_raw = pd.read_csv(os.path.join(data_folder,\"raw_mjo\",\"raw_mjo.csv\"))\n",
        "df_mjo_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSdEH10Xn-V3"
      },
      "outputs": [],
      "source": [
        "#understanding the mjo data\n",
        "fig = px.line(\n",
        "    data_frame = df_mjo_raw.loc[310:390],\n",
        "    x = \"RMM1\",\n",
        "    y = \"RMM2\",\n",
        "    color = \"month\",\n",
        "\n",
        "    title = \"MJO\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    width = 500,\n",
        "    height = 500,\n",
        "\n",
        "    range_x = (3,-3),\n",
        "    range_y = (3,-3),\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdDdC3TNn-V3"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_mjo_raw,\n",
        "    y = \"amplitude\",\n",
        "\n",
        "    title = \"MJO\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    **size,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGoEj2zon-V3"
      },
      "outputs": [],
      "source": [
        "#drop unneeded cols\n",
        "df_mjo_raw.drop(axis = 1, labels = [\"Unnamed: 0\",\"unnamed\", \"MissingValue=1.E36or999\"], inplace = True)\n",
        "df_mjo = df_mjo_raw.loc[df_mjo_raw[\"year\"] >= 1979]\n",
        "df_mjo.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAWTIFHon-V3"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_mjo,\n",
        "    y = \"amplitude\",\n",
        "\n",
        "    title = \"MJO\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    **size,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMqY5pA5n-V4"
      },
      "outputs": [],
      "source": [
        "df_mjo.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUps30Mgn-V4"
      },
      "outputs": [],
      "source": [
        "df_mjo.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJiENrf3n-V4"
      },
      "outputs": [],
      "source": [
        "#clean up and prep data\n",
        "df_mjo.reset_index(inplace = True, drop = True)\n",
        "df_mjo['date'] = pd.to_datetime(df_mjo[[\"year\", \"month\", \"day\"]])\n",
        "df_mjo.drop(labels = [\"year\", \"month\", \"day\"], axis = 1, inplace = True)\n",
        "\n",
        "#set index\n",
        "df_mjo.set_index(\"date\", drop = True, inplace = True)\n",
        "\n",
        "#clean up cols\n",
        "df_mjo = df_mjo.add_prefix(prefix = \"mjo_\")\n",
        "df_mjo.columns = [x.lower() for x in df_mjo.columns]\n",
        "\n",
        "#sort\n",
        "df_mjo.sort_index(inplace = True)\n",
        "\n",
        "df_mjo.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R00_7Svon-V5"
      },
      "outputs": [],
      "source": [
        "if save_data is True:\n",
        "    df_mjo.to_csv(os.path.join(data_folder, \"df_mjo.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG0kp1Ovn-V5"
      },
      "source": [
        "## 0.4 Arctic oscillation index (AO)\n",
        "- source: https://ftp.cpc.ncep.noaa.gov/cwlinks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytWLo3BCn-V5"
      },
      "outputs": [],
      "source": [
        "df_ao_raw = pd.read_csv(os.path.join(data_folder, \"raw_ao\", \"norm_daily_ao_cda_z1000_19500101_current.csv\"))\n",
        "df_ao_raw.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1-CkovHn-V6"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_ao_raw,\n",
        "    y = \"ao_index_cdas\",\n",
        "\n",
        "    title = \"AO index\",\n",
        "    **size,\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxAmCLnzn-V6"
      },
      "outputs": [],
      "source": [
        "df_ao = df_ao_raw.rename(mapper = {\"ao_index_cdas\":\"ao\"}, inplace = False, axis = 1)\n",
        "df_ao.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAwhEi1in-V6"
      },
      "outputs": [],
      "source": [
        "df_ao.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBbXSeU3n-V6"
      },
      "outputs": [],
      "source": [
        "df_ao[df_ao.isna().any(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wov2iatyn-V7"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_ao.loc[(df_ao[\"year\"] == 2003) & (df_ao[\"month\"].isin([4,5]))],\n",
        "    y = \"ao\",\n",
        "    title = \"AO 2003\",\n",
        "    **size,\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjoAOaGvn-V7"
      },
      "outputs": [],
      "source": [
        "#use interpolation to fill na value\n",
        "df_ao.interpolate(method = \"linear\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEGEO5QGn-V8"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_ao.loc[(df_ao[\"year\"] == 2003) & (df_ao[\"month\"].isin([4,5]))],\n",
        "    y = \"ao\",\n",
        "    title = \"ao 2003\",\n",
        "    **size,\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyVRVepnn-V8"
      },
      "outputs": [],
      "source": [
        "df_ao.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJNIiHb0n-V8"
      },
      "outputs": [],
      "source": [
        "df_ao.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGYu8fMbn-V9"
      },
      "outputs": [],
      "source": [
        "#clean up and prep data\n",
        "df_ao.reset_index(inplace = True, drop = True)\n",
        "df_ao[\"date\"] = pd.to_datetime(df_ao[[\"year\", \"month\", \"day\"]])\n",
        "df_ao.drop(labels = [\"year\", \"month\", \"day\"], axis = 1, inplace = True)\n",
        "\n",
        "#set index\n",
        "df_ao.set_index(\"date\", drop = True, inplace = True)\n",
        "\n",
        "#aggregagation not needed\n",
        "\n",
        "#sort\n",
        "df_ao.sort_index(inplace = True)\n",
        "\n",
        "df_ao.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GitBSNrCn-V9"
      },
      "outputs": [],
      "source": [
        "if save_data is True:\n",
        "    df_ao.to_csv(os.path.join(data_folder, \"df_ao.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL3VDpsfn-V9"
      },
      "source": [
        "## 0.5 North atlantic oscilation (NAO)\n",
        "source: https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/nao.shtml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCYsQmc2n-V-"
      },
      "outputs": [],
      "source": [
        "df_nao_raw = pd.read_csv(os.path.join(data_folder, \"raw_nao\",\"norm_daily_nao_cdas_z500_19500101_current.csv\"))\n",
        "df_nao_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDtteh8Gn-V-"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_nao_raw,\n",
        "    y = \"nao_index_cdas\",\n",
        "    title = \"NAO\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPE-rUJ5n-V-"
      },
      "outputs": [],
      "source": [
        "df_nao_raw.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eeIvwH9n-V_"
      },
      "outputs": [],
      "source": [
        "df_nao_raw[df_nao_raw.isna().any(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkoQlu8Sn-V_"
      },
      "outputs": [],
      "source": [
        "#fill na with interpolation\n",
        "df_nao = df_nao_raw.interpolate(method = \"linear\")\n",
        "df_nao.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9N7kL2Bn-V_"
      },
      "outputs": [],
      "source": [
        "#clean up and prep data\n",
        "df_nao.reset_index(inplace = True, drop = True)\n",
        "df_nao[\"date\"] = pd.to_datetime(df_nao[[\"year\", \"month\", \"day\"]])\n",
        "df_nao.drop(labels = [\"year\", \"month\", \"day\"], axis = 1, inplace = True)\n",
        "\n",
        "#set index\n",
        "df_nao.set_index(\"date\", drop = True, inplace = True)\n",
        "\n",
        "#aggregagation not needed\n",
        "\n",
        "#rename col\n",
        "df_nao.rename(mapper = {\"nao_index_cdas\" : \"nao\"}, axis = 1, inplace = True)\n",
        "\n",
        "#sort\n",
        "df_nao.sort_index(inplace = True)\n",
        "\n",
        "df_nao.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGijoaoEn-WA"
      },
      "outputs": [],
      "source": [
        "if save_data is True:\n",
        "    df_nao.to_csv(os.path.join(data_folder, \"df_nao.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2qM4pQpn-WA"
      },
      "source": [
        "## 0.6 Polar vortex data (PV)\n",
        "\n",
        "relevant preassure level: 1000 hPa level (see bthe, chapter 2.4 on polar vortex and ao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2NcUEOmn-WA"
      },
      "outputs": [],
      "source": [
        "#define borders\n",
        "north   = 90\n",
        "south   = 45.0\n",
        "west    = 8.0\n",
        "east    = 8.0\n",
        "\n",
        "\n",
        "lons = [west, west, east, east, west]\n",
        "lats = [south, north, north, south, south]\n",
        "\n",
        "#lons = [6,6,10.5,10.5,6]\n",
        "#lats = [45.8,47.8,47.8,45.8,45.8]\n",
        "\n",
        "#create plot\n",
        "fig = go.Figure(go.Scattermapbox(\n",
        "    mode = \"markers+lines\",\n",
        "    lon = lons,\n",
        "    lat = lats,\n",
        "    marker = {'size': 10})\n",
        ")\n",
        "\n",
        "#adjust view\n",
        "fig.update_layout(\n",
        "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
        "    mapbox = {\n",
        "        'center': {'lon': 8.4, 'lat': 60},\n",
        "        'style': \"carto-positron\",\n",
        "        'zoom': 3})\n",
        "\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eWlDdDkn-WA"
      },
      "outputs": [],
      "source": [
        "#resolution of data\n",
        "#define borders\n",
        "\n",
        "lons = []\n",
        "lats = []\n",
        "\n",
        "for lat in range(round(south) , round(north)):\n",
        "    for lon in [8,9]:\n",
        "        lons.append(lon)\n",
        "        lats.append(lat)\n",
        "\n",
        "#lons = [6,6,10.5,10.5,6]\n",
        "#lats = [45.8,47.8,47.8,45.8,45.8]\n",
        "\n",
        "print(f\"datapoint per time step: {len(lons)}\")\n",
        "print(f\"image resolution: {len(set(lons))} x {len(set(lats))}\")\n",
        "\n",
        "#create plot\n",
        "fig = go.Figure(go.Scattermapbox(\n",
        "    mode = \"markers\",\n",
        "    lon = lons,\n",
        "    lat = lats,\n",
        "    marker = {'size': 10})\n",
        ")\n",
        "\n",
        "#adjust view\n",
        "fig.update_layout(\n",
        "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
        "    mapbox = {\n",
        "        'center': {'lon': 8.4, 'lat': 60},\n",
        "        'style': \"carto-positron\",\n",
        "        'zoom': 3})\n",
        "\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0E5LhQyn-WB"
      },
      "outputs": [],
      "source": [
        "#data source: https://cds.climate.copernicus.eu/cdsapp#!/home\n",
        "\n",
        "class Wrapper():\n",
        "\n",
        "    #class variables\n",
        "    folder_name : str = None\n",
        "\n",
        "    #functionality\n",
        "    def main(data_folder):\n",
        "\n",
        "        Wrapper.folder_name = os.path.join(data_folder,\"raw_pv\")\n",
        "\n",
        "        start_year : int        = 1979 #1979\n",
        "        end_year : int          = 2023 #2023\n",
        "\n",
        "        years : list            = Wrapper.generate_year_list(start = start_year, end = end_year)\n",
        "        variable : list        = Wrapper.generate_var_list()\n",
        "\n",
        "        #main loop for downloading data\n",
        "        Wrapper.log(f\"Downloading startet for range: {start_year} - {end_year}\")\n",
        "\n",
        "        for year in years:\n",
        "            for month in range(1,13):\n",
        "\n",
        "                print(f\"Processing {year}-{month}\")\n",
        "\n",
        "                try:\n",
        "                    emailer.message(f\"Downloading pv: {year}-{month}\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                result : str = Wrapper.request(year = year, month = month ,variable = variable)\n",
        "\n",
        "        #tranforms and saves data as a csv for later processing in pandas\n",
        "        Wrapper.generate_df()\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate_year_list(start:int, end:int):\n",
        "\n",
        "        year_list_str : list = [str(year) for year in range(start,end)]\n",
        "        return year_list_str\n",
        "\n",
        "    def generate_var_list():\n",
        "\n",
        "        return ['u_component_of_wind', 'v_component_of_wind','temperature']\n",
        "\n",
        "    def generate_df():\n",
        "\n",
        "        downloads = Wrapper.folder_name\n",
        "        files = os.listdir(downloads)\n",
        "\n",
        "        for file in files:\n",
        "\n",
        "            #open .nc files\n",
        "            file = os.path.join(downloads,file)\n",
        "            ds = xr.open_dataset(file)\n",
        "            df = ds.to_dataframe()\n",
        "\n",
        "            #save df\n",
        "            file = os.path.basename(file)\n",
        "            name = f\"{file[:-3]}.csv\"\n",
        "            path = os.path.join(Wrapper.folder_name,name)\n",
        "            df.to_csv(path)\n",
        "\n",
        "    def download_path():\n",
        "\n",
        "        folder_name = Wrapper.folder_name\n",
        "\n",
        "        if os.path.isdir(folder_name) == False:\n",
        "            os.makedirs(folder_name)\n",
        "\n",
        "        download_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), folder_name)\n",
        "\n",
        "        return download_path\n",
        "\n",
        "    def file_path(year, month):\n",
        "\n",
        "        #genearte download and saving path\n",
        "        path : str          = Wrapper.folder_name\n",
        "        file_name : str     = f\"pv_{year}-{month}.nc\" #type nasCat data\n",
        "        file_path :str      = os.path.join(path,file_name)\n",
        "\n",
        "        print(file_path)\n",
        "        return file_path\n",
        "\n",
        "    def log(message : str):\n",
        "\n",
        "        #create log entry\n",
        "        log_time : str = datetime.now()\n",
        "        message = f\"{log_time},{message}\\n\"\n",
        "\n",
        "        #write log entry\n",
        "        file_object = open('era5_log.txt', 'a')\n",
        "        file_object.write(message)\n",
        "        file_object.close()\n",
        "\n",
        "        return\n",
        "\n",
        "    def request(year:int, month : int, variable:list):\n",
        "        # see: https://www.latlong.net/\n",
        "\n",
        "        c = cdsapi.Client()\n",
        "        file_path = Wrapper.file_path(year = year, month = month)\n",
        "\n",
        "        print(year)\n",
        "\n",
        "        request = c.retrieve(\n",
        "            'reanalysis-era5-pressure-levels',\n",
        "            {\n",
        "                'product_type': 'reanalysis',\n",
        "                'format': 'netcdf',\n",
        "                'pressure_level': ['10', '20', '30' ,'50', '70', '100',], #now it should be correct\n",
        "                'variable': variable,\n",
        "                'area': [\n",
        "                    90, 8, 45,\n",
        "                    9,\n",
        "                ],\n",
        "                'time': [\n",
        "                    '00:00', '01:00', '02:00',\n",
        "                    '03:00', '04:00', '05:00',\n",
        "                    '06:00', '07:00', '08:00',\n",
        "                    '09:00', '10:00', '11:00',\n",
        "                    '12:00', '13:00', '14:00',\n",
        "                    '15:00', '16:00', '17:00',\n",
        "                    '18:00', '19:00', '20:00',\n",
        "                    '21:00', '22:00', '23:00',\n",
        "                ],\n",
        "                'day': [\n",
        "                    '01', '02', '03',\n",
        "                    '04', '05', '06',\n",
        "                    '07', '08', '09',\n",
        "                    '10', '11', '12',\n",
        "                    '13', '14', '15',\n",
        "                    '16', '17', '18',\n",
        "                    '19', '20', '21',\n",
        "                    '22', '23', '24',\n",
        "                    '25', '26', '27',\n",
        "                    '28', '29', '30',\n",
        "                    '31',\n",
        "                ],\n",
        "                'month': month,\n",
        "                'year': year,\n",
        "            },\n",
        "            file_path\n",
        "        )\n",
        "\n",
        "        return str(request)\n",
        "\n",
        "if pv_run_era5_download is True:\n",
        "    Wrapper.main(data_folder = data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_xX4Rx_n-WB"
      },
      "outputs": [],
      "source": [
        "class Agg_pv():\n",
        "\n",
        "    data = None\n",
        "    pv_data :str = None\n",
        "\n",
        "    def main(data_folder):\n",
        "\n",
        "        Agg_pv.data = data_folder\n",
        "        Agg_pv.pv_data = os.path.join(data_folder,\"raw_pv\")\n",
        "\n",
        "        files : list        = os.listdir(Agg_pv.pv_data)\n",
        "        csv_files : list    = [file for file in files if file[-4:] == \".csv\"]\n",
        "\n",
        "        dfs : list          = []\n",
        "\n",
        "        for csv in csv_files:\n",
        "\n",
        "            print(f\"Processing: {csv}\", end = \"\\r\")\n",
        "            df = pd.read_csv(os.path.join(Agg_pv.pv_data, csv))\n",
        "            df = Agg_pv.aggregate(df)\n",
        "            dfs.append(df)\n",
        "\n",
        "        df = Agg_pv.merge(dfs)\n",
        "\n",
        "        #add base infos\n",
        "        df = Agg_pv.calculate_wind_speeds(df)\n",
        "        df = Agg_pv.calculate_wind_direction(df)\n",
        "\n",
        "        Agg_pv.save(df, data_folder)\n",
        "\n",
        "        del dfs, df\n",
        "        return\n",
        "\n",
        "    def aggregate(df):\n",
        "\n",
        "        #generate gorup index as date\n",
        "        df.reset_index(drop = False, inplace = True)\n",
        "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "        df[\"date\"] = df[\"time\"].dt.date\n",
        "\n",
        "        #set date as index\n",
        "        df.set_index(\"date\", drop = True, inplace = True)\n",
        "\n",
        "        #drop time column\n",
        "        df.drop(labels = [\"time\"], inplace = True, axis = 1)\n",
        "\n",
        "        #define grid resolution\n",
        "        df[\"longitude\"] = df[\"longitude\"].round()\n",
        "        df[\"latitude\"] = df[\"latitude\"].round()\n",
        "\n",
        "        #aggreagte data\n",
        "        df = df.groupby([\"date\", \"longitude\",\"latitude\", \"level\"], as_index = True).mean()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def merge(dfs):\n",
        "\n",
        "        df = pd.concat(dfs)\n",
        "        #df.sort_values(by = \"date\", inplace = True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def calculate_wind_speeds(df):\n",
        "\n",
        "        df['speed'] = np.sqrt(df['u']**2 + df['v']**2)\n",
        "        return df\n",
        "\n",
        "    def calculate_wind_direction(df):\n",
        "\n",
        "        df['direction'] = np.rad2deg(np.arctan2(df['u'], df['v'])) % 360\n",
        "        df['direction'] = (df['direction'] + 90) % 360\n",
        "\n",
        "        return df\n",
        "\n",
        "    def save(df, data_folder):\n",
        "\n",
        "        df.rename(mapper = {\n",
        "            \"longitude\" : \"lon\",\n",
        "            \"latitude\" : \"lat\",\n",
        "            \"u\" : \"wind_u\",\n",
        "            \"v\" : \"wind_v\",\n",
        "            \"t\" : \"temp\",\n",
        "        })\n",
        "\n",
        "        if \"index\" in df.index.to_list():\n",
        "            df.drop(labels = \"index\", axis = 1, inplace = True)\n",
        "\n",
        "        df.to_csv(os.path.join(data_folder, \"df_pv.csv\"))\n",
        "        return\n",
        "\n",
        "if pv_compile_df is True:\n",
        "    Agg_pv.main(data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aLjpgA-n-WB"
      },
      "outputs": [],
      "source": [
        "df_pv = pd.read_csv(os.path.join(data_folder, \"df_pv.csv\"), index_col = \"date\")\n",
        "df_pv.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18mme8Ncn-WB"
      },
      "outputs": [],
      "source": [
        "del df_pv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9xPYvyjn-WB"
      },
      "source": [
        "## 0.7 Merge all (execpt custom polar vortex, date base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vLRS616n-WC"
      },
      "outputs": [],
      "source": [
        "save_data = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b79HMffun-WC"
      },
      "outputs": [],
      "source": [
        "folder_contents = os.listdir(data_folder)\n",
        "csv_files = [\"df_ao.csv\", \"df_nao.csv\", \"df_mjo.csv\", \"df_soi.csv\", \"df_t2m.csv\"]\n",
        "\n",
        "dfs : list = []\n",
        "print(csv_files)\n",
        "\n",
        "#read files\n",
        "for csv in csv_files:\n",
        "\n",
        "    csv_path = os.path.join(data_folder, csv)\n",
        "    df = pd.read_csv(csv_path, index_col = \"date\")\n",
        "    print(df.columns)\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "#merge files\n",
        "df = dfs[0].join(other = dfs[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrqMtYMOn-WD"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iE_hZhBn-WD"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThWG6Rj1n-WD"
      },
      "outputs": [],
      "source": [
        "#reset index\n",
        "df.reset_index(drop = False, inplace = True)\n",
        "\n",
        "#presumed cleaned data range\n",
        "start = \"1979-01-01\"\n",
        "end = \"2022-12-31\"\n",
        "\n",
        "#set\n",
        "df = df.loc[df[\"date\"].between(start, end)]\n",
        "df.set_index(\"date\", drop = True, inplace = True)\n",
        "\n",
        "#clean up\n",
        "[df.drop(labels = [col], inplace = True, axis = 1) for col in df.columns.tolist() if col == \"index\"]\n",
        "\n",
        "#chech\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6P2GYshn-WD"
      },
      "outputs": [],
      "source": [
        "#addint date time values to df for plotting\n",
        "df[\"date_temp\"] = pd.to_datetime(df.index)\n",
        "\n",
        "#set values\n",
        "df[\"year\"] = df[\"date_temp\"].dt.year\n",
        "df[\"month\"] = df[\"date_temp\"].dt.month\n",
        "df[\"day\"] = df[\"date_temp\"].dt.day\n",
        "\n",
        "df.drop(labels = \"date_temp\", inplace = True, axis = 1)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97RhpguFn-WE"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoPsZcuqn-WE"
      },
      "outputs": [],
      "source": [
        "#save master data frame\n",
        "if save_data is True:\n",
        "    df.to_csv(os.path.join(data_folder, \"df.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsEvNUatn-WE"
      },
      "source": [
        "# 1. Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyfTH1vYn-WF"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjIB7zcDn-WF"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    fig.update_layout(\n",
        "        width=1500,\n",
        "        height=750,\n",
        "    )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYSu1TcFn-WF"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWWgdgoRn-WF"
      },
      "source": [
        "## 1.0 General"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "591v5JVXn-WG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df.csv\"))\n",
        "#df_pv = pd.read_csv(os.path.join(data_folder, \"df_pv.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5Eogqu7n-WG"
      },
      "outputs": [],
      "source": [
        "df[\"date\"] = pd.to_datetime(df[\"date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv1VH-wFn-WH"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHDt9rrRn-WH"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_aXxQzXn-WH"
      },
      "outputs": [],
      "source": [
        "#df_pv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfGKp9xDn-WH"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27OajtVKn-WI"
      },
      "outputs": [],
      "source": [
        "#df_pv.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYoXjlmrn-WI"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPOECOyOn-WI"
      },
      "outputs": [],
      "source": [
        "#df_pv.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt60sCD2n-WI"
      },
      "outputs": [],
      "source": [
        "# Correlation\n",
        "df_corr = df.corr().round(1)\n",
        "\n",
        "# Mask to matrix\n",
        "mask = np.zeros_like(df_corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# Viz\n",
        "df_corr_viz = df_corr.mask(mask).dropna(how='all').dropna('columns', how='all')\n",
        "\n",
        "fig = px.imshow(\n",
        "\n",
        "    df_corr_viz,\n",
        "    text_auto=True,\n",
        "    color_continuous_scale = plt_style_c,\n",
        "\n",
        "    title = \"Correlation matrix\",\n",
        "    width = 700,\n",
        "    height = 700,\n",
        "    )\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpryxWDmn-WJ"
      },
      "source": [
        "## 1.1 Temperature, pressure, wind speeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D26VnFwBn-WJ"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"month\",\n",
        "    y = \"u10\",\n",
        "\n",
        "    title = \"Wind speed: u10\",\n",
        "    labels = {\"u10\" :\"u10 [m/s]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8URFmqe0n-WJ"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"month\",\n",
        "    y = \"v10\",\n",
        "\n",
        "    title = \"Wind speed: v10\",\n",
        "    labels = {\"v10\" :\"v10 [m/s]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8guSKA-n-WJ"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df,\n",
        "    x = [\"v10\", \"u10\"],\n",
        "    histnorm = \"probability density\",\n",
        "    title = \"Distribution: v10\",\n",
        "\n",
        "    barmode = \"overlay\",\n",
        "    opacity = 0.9,\n",
        "\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSjQ0x_mn-WJ"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"month\",\n",
        "    y = \"t2m\",\n",
        "\n",
        "    title = \"Temperature\",\n",
        "    labels = {\"t2m\" :\"t2m [░k]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVTXeFfjn-WK"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"year\",\n",
        "    y = \"t2m\",\n",
        "\n",
        "    title = \"Temperature\",\n",
        "    labels = {\"t2m\" :\"t2m [░k]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBdwqGcWn-WK"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df.iloc[::10],\n",
        "    x = \"date\",\n",
        "    y = \"t2m\",\n",
        "\n",
        "    title = \"Temperature\",\n",
        "    labels = {\"t2m\" :\"t2m [k]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDObGDZRn-WL"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df.iloc[::2],\n",
        "    x = \"date\",\n",
        "    y = \"t2m\",\n",
        "\n",
        "    title = \"Temperature\",\n",
        "    labels = {\"t2m\" :\"t2m [k]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_c,\n",
        "    trendline = \"ols\",\n",
        "    trendline_color_override = \"red\",\n",
        "\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_kVtQOkn-WL"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df,\n",
        "    x = \"t2m\",\n",
        "    histnorm = \"probability density\",\n",
        "    title = \"Distribution: t2m\",\n",
        "\n",
        "    color = \"month\",\n",
        "    barmode = \"stack\",\n",
        "    opacity = 1,\n",
        "\n",
        "    nbins = 200,\n",
        "\n",
        "    labels = {\"t2m\" : \"t2m [░k]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_c,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d-cYLAan-WM"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df.iloc[15000:],\n",
        "    x = \"date\",\n",
        "    y = \"cdir\",\n",
        "\n",
        "    title = \"Clear sky solar iradiation\",\n",
        "    labels = {\"cdir\" :\"cdir [j/m^2]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1EOTtGrn-WM"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"month\",\n",
        "    y = \"cdir\",\n",
        "\n",
        "    title = \"Clear sky solar iradiation\",\n",
        "    labels = {\"cdir\" :\"cdir [j/m^2]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY8QBejCn-WN"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df.iloc[15000:],\n",
        "    x = \"date\",\n",
        "    y = \"sp\",\n",
        "\n",
        "    title = \"Surface pressure\",\n",
        "    labels = {\"sp\" :\"sp [hpa]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jeDUar-n-WO"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"month\",\n",
        "    y = \"sp\",\n",
        "\n",
        "    title = \"Surface pressure\",\n",
        "    labels = {\"sp\" :\"sp [hpa]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_s\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhCBKwKLn-WP"
      },
      "source": [
        "## 1.2 ENSO / SOI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1rGByI3n-WP"
      },
      "outputs": [],
      "source": [
        "#some code\n",
        "\n",
        "fig = px.line(\n",
        "    data_frame = df.loc[15000:],\n",
        "    x = \"date\",\n",
        "    y = \"soi\",\n",
        "\n",
        "    title = \"SOI\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "fig.add_hline(\n",
        "    y = 0,\n",
        "    line_color=\"red\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB7ooLgpn-WP"
      },
      "source": [
        "## 1.3 MJO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxvhAt20n-WQ"
      },
      "outputs": [],
      "source": [
        "#some code\n",
        "#mjo_rmm1, mjo_rmm2, mjo_phase, mjo_amplitude\n",
        "\n",
        "fig = px.line(\n",
        "    data_frame = df.loc[::2],\n",
        "    x = \"date\",\n",
        "    y = \"mjo_amplitude\",\n",
        "\n",
        "    title = \"MJO amplitude\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "637fKiKjn-WQ"
      },
      "outputs": [],
      "source": [
        "#some code\n",
        "#mjo_rmm1, mjo_rmm2, mjo_phase, mjo_amplitude\n",
        "\n",
        "fig = px.line(\n",
        "    data_frame = df.loc[15000:],\n",
        "    x = \"date\",\n",
        "    y = \"mjo_amplitude\",\n",
        "\n",
        "    title = \"MJO amplitude\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8jryygdn-WR"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df.iloc[300:5000],\n",
        "    x = \"mjo_rmm1\",\n",
        "    y = \"mjo_rmm2\",\n",
        "    color = \"mjo_phase\",\n",
        "\n",
        "    title = \"MJO with pahse\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        "    width = 700,\n",
        "    height = 700,\n",
        "\n",
        "    range_x = [-3,3],\n",
        "    range_y = [-3,3],\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3IBLaVOn-WR"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df.iloc[1000:1100],\n",
        "    x = \"mjo_rmm1\",\n",
        "    y = \"mjo_rmm2\",\n",
        "\n",
        "    title = \"MJO with pahse\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        "    width = 700,\n",
        "    height = 700,\n",
        "\n",
        "    range_x = [-3,3],\n",
        "    range_y = [-3,3],\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1RAP95sn-WR"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df,\n",
        "    x = \"mjo_amplitude\",\n",
        "\n",
        "    histfunc = \"count\",\n",
        "    histnorm = \"probability\",\n",
        "\n",
        "    title = \"MJO amplitude distribution\",\n",
        "    color_discrete_sequence = plt_style_c,\n",
        "    barmode = \"stack\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB43dASWn-WR"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"month\",\n",
        "    y = \"mjo_amplitude\",\n",
        "    title = \"MJO amplitude distribution\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5bSQWyln-WR"
      },
      "source": [
        "## 1.4 AO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9s9b0Pln-WR"
      },
      "outputs": [],
      "source": [
        "#some code\n",
        "fig = px.line(\n",
        "    data_frame = df.iloc[14000:],\n",
        "    y = \"ao\",\n",
        "    x = \"date\",\n",
        "\n",
        "    title = \"AO\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD3xJe2Cn-WS"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df,\n",
        "    x = \"ao\",\n",
        "\n",
        "    histfunc = \"count\",\n",
        "    histnorm = \"probability\",\n",
        "\n",
        "    title = \"AO distribution\",\n",
        "    color_discrete_sequence = plt_style_c,\n",
        "    barmode = \"stack\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMfuyOphn-WT"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"month\",\n",
        "    y = \"ao\",\n",
        "    title = \"AO distribution\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWcaR0aUn-WT"
      },
      "source": [
        "## 1.5 NAO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKluWpLyn-WT"
      },
      "outputs": [],
      "source": [
        "#some code\n",
        "fig = px.line(\n",
        "    data_frame = df.iloc[14000:],\n",
        "    y = \"nao\",\n",
        "    x = \"date\",\n",
        "\n",
        "    title = \"NAO\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKKyLlyFn-WU"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df,\n",
        "    x = \"nao\",\n",
        "\n",
        "    histfunc = \"count\",\n",
        "    histnorm = \"probability\",\n",
        "\n",
        "    title = \"NAO distribution\",\n",
        "    color_discrete_sequence = plt_style_c,\n",
        "    barmode = \"stack\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSIvSR7Jn-WU"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df,\n",
        "    x = \"month\",\n",
        "    y = \"nao\",\n",
        "    title = \"NAO  distribution\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mL0gfOgn-WU"
      },
      "source": [
        "## 1.6 Variouse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMWo-UIRn-WU"
      },
      "outputs": [],
      "source": [
        "#some code\n",
        "fig = px.line(\n",
        "    data_frame = df.iloc[14000:],\n",
        "    y = [df[\"ao\"].iloc[14000:], df[\"mjo_amplitude\"].iloc[14000:], df[\"soi\"].iloc[14000:]],\n",
        "    x = \"date\",\n",
        "\n",
        "    title = \"Climate oscilation indexes\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wan12BOjn-WV"
      },
      "source": [
        "## 1.6 PV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlUZESDZn-WV"
      },
      "outputs": [],
      "source": [
        "#some code\n",
        "df = pd.read_csv(os.path.join(data_folder, \"df_pv.csv\"))\n",
        "df[\"size\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xQwKnhKn-WV"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LA4ZnhDn-WV"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_geo(\n",
        "    df.loc[(df[\"date\"] >= \"1979-01-01\") & (df[\"date\"] <= \"1979-02-01\") & (df[\"level\"] == 100)],\n",
        "    lat=\"latitude\",\n",
        "    lon=\"longitude\",\n",
        "    color=\"speed\",\n",
        "    size = \"size\",\n",
        "    color_continuous_scale = plt_style_s,\n",
        "    size_max=7,\n",
        "    opacity = 0.75,\n",
        "    animation_frame = \"date\",\n",
        "    labels = {\"speed\" : \"wind speed [m/s]\"},\n",
        "    title = \"Polar vortex\"\n",
        "    #zoom=10,\n",
        "    )\n",
        "\n",
        "\n",
        "#adjust view\n",
        "fig.update_layout(\n",
        "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
        "    mapbox = {\n",
        "        'center': {'lon': 8.4, 'lat': 60},\n",
        "        'style': \"carto-positron\",\n",
        "        #'zoom': 3,\n",
        "        #\"projection\": \"albers usa\",\n",
        "    }\n",
        ")\n",
        "\n",
        "#fig.update_geos(projection_type=\"natural earth\")\n",
        "\n",
        "#update markers\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHvzxueSn-WV"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_mapbox(\n",
        "    df.loc[(df[\"date\"] >= \"1979-01-01\") & (df[\"date\"] <= \"1979-02-01\")],\n",
        "    lat=\"latitude\",\n",
        "    lon=\"longitude\",\n",
        "    color=\"speed\",\n",
        "    size = \"size\",\n",
        "    color_continuous_scale = plt_style_s,\n",
        "    size_max=10,\n",
        "    opacity = 0.5,\n",
        "    animation_frame = \"date\",\n",
        "\n",
        "    labels = {\"speed\" : \"wind speed [m/s]\"},\n",
        "    )\n",
        "\n",
        "\n",
        "#adjust view\n",
        "fig.update_layout(\n",
        "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
        "    mapbox = {\n",
        "        'center': {'lon': 8.4, 'lat': 60},\n",
        "        'style': \"carto-positron\",\n",
        "        'zoom': 3,\n",
        "        #\"projection\": \"albers usa\",\n",
        "    }\n",
        ")\n",
        "\n",
        "#fig.update_geos(projection_type=\"natural earth\")\n",
        "\n",
        "#update markers\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib396h3Hn-WW"
      },
      "outputs": [],
      "source": [
        "plot_scaler = 15\n",
        "fig = px.scatter(\n",
        "    data_frame = df.loc[(df[\"date\"] >= \"1979-01-01\") & (df[\"date\"] <= \"1979-01-07\")],\n",
        "    x = \"longitude\",\n",
        "    y = \"latitude\",\n",
        "    color = \"speed\",\n",
        "    size = \"size\",\n",
        "    size_max = 1 * plot_scaler - 3,\n",
        "    opacity = 1,\n",
        "    facet_col = \"date\",\n",
        "    #animation_frame = \"date\",\n",
        "\n",
        "    height = 45 * plot_scaler,\n",
        "    width = (15) * plot_scaler * 6,\n",
        "    color_continuous_scale  = plt_style_s,\n",
        "\n",
        "    title = \"Polar vortex wind speed\",\n",
        "\n",
        "    labels = {\"speed\" : \"speed [m/s]\"},\n",
        ")\n",
        "\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol=\"square\",),\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMTkNkKan-WW"
      },
      "outputs": [],
      "source": [
        "plot_scaler = 15\n",
        "fig = px.scatter(\n",
        "    data_frame = df.loc[(df[\"date\"] >= \"1979-01-01\") & (df[\"date\"] <= \"1979-01-07\")],\n",
        "    x = \"longitude\",\n",
        "    y = \"latitude\",\n",
        "    color = \"t\",\n",
        "    size = \"size\",\n",
        "    size_max = 1 * plot_scaler -3,\n",
        "    opacity = 1,\n",
        "    facet_col = \"date\",\n",
        "    #animation_frame = \"date\",\n",
        "\n",
        "    height = 45 * plot_scaler,\n",
        "    width = (13 + 2) * plot_scaler * 6,\n",
        "    color_continuous_scale  = plt_style_s,\n",
        "\n",
        "    title = \"Polar vortex temperatures\",\n",
        "\n",
        "    labels = {\"t\" : \"t [░k]\"},\n",
        ")\n",
        "\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol=\"square\",),\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DntnircXn-WW"
      },
      "outputs": [],
      "source": [
        "plot_scaler = 15\n",
        "fig = px.scatter(\n",
        "    data_frame = df.loc[(df[\"date\"] >= \"2019-03-01\") & (df[\"date\"] <= \"2019-03-04\")],\n",
        "    y = \"level\",\n",
        "    x = \"latitude\",\n",
        "    color = \"speed\",\n",
        "    size = \"size\",\n",
        "    size_max = 1 * plot_scaler - 3,\n",
        "    opacity = 1,\n",
        "    facet_row = \"date\",\n",
        "    #animation_frame = \"date\",\n",
        "\n",
        "    height = (15 * 4)  * plot_scaler,\n",
        "    width = 60 * plot_scaler,\n",
        "    color_continuous_scale =  plt_style_s,\n",
        "\n",
        "    title = \"Polar vortex wind speed\",\n",
        "\n",
        "    labels = {\"speed\" : \"speed [m/s]\"},\n",
        ")\n",
        "\n",
        "fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol=\"square\",),\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9XgjSrXn-WW"
      },
      "outputs": [],
      "source": [
        "plot_scaler = 15\n",
        "fig = px.scatter(\n",
        "    data_frame = df.loc[(df[\"date\"] >= \"2019-03-01\") & (df[\"date\"] <= \"2019-03-04\")],\n",
        "    y = \"level\",\n",
        "    x = \"latitude\",\n",
        "    color = \"t\",\n",
        "    size = \"size\",\n",
        "    size_max = 1 * plot_scaler - 3,\n",
        "    opacity = 1,\n",
        "    facet_row = \"date\",\n",
        "    #animation_frame = \"date\",\n",
        "\n",
        "    height = (15 * 4)  * plot_scaler,\n",
        "    width = 60 * plot_scaler,\n",
        "    color_continuous_scale =  plt_style_s,\n",
        "\n",
        "    title = \"Polar vortex temperature\",\n",
        "\n",
        "    labels = {\"t\" : \"t [░k]\"},\n",
        ")\n",
        "\n",
        "fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol=\"square\",),\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C61NeJpwn-WW"
      },
      "outputs": [],
      "source": [
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_8c2nydn-WW"
      },
      "source": [
        "# 2. Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feVA5AI0n-WX"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\"\n",
        "\n",
        "#warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSqejatqn-WX"
      },
      "outputs": [],
      "source": [
        "#set if data should be safed\n",
        "save_data = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftfph0AEn-WX"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    fig.update_layout(\n",
        "        width=1500,\n",
        "        height=750,\n",
        "    )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUfyN33bn-WX"
      },
      "source": [
        "## 2.1 Main data frame feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knmGVOVxn-WX"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df.csv\"))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1S3lEcPn-WZ"
      },
      "outputs": [],
      "source": [
        "#clean up\n",
        "df.drop(labels = [col for col in df.columns.tolist() if \"unnamed\" in col.lower()], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYgF76Xzn-WZ"
      },
      "outputs": [],
      "source": [
        "day : int = 24 * 60 * 60 #[sec]\n",
        "year : int = day * 366 #[sec] : 1,\\n2020 was a leap year\n",
        "\n",
        "year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMrx8iv3n-WZ"
      },
      "outputs": [],
      "source": [
        "#creat col\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"date\"]).apply(datetime.timestamp)\n",
        "\n",
        "#calculate values\n",
        "day : int = 24 * 60 * 60 #[sec]\n",
        "year : int = day * 366 #[sec] : 1,\\n2020 was a leap year\n",
        "\n",
        "#set columns\n",
        "df[\"year_sin\"] = np.sin(df[\"timestamp\"] * (2*np.pi / year))\n",
        "df[\"year_cos\"] = np.cos(df[\"timestamp\"] * (2*np.pi / year))\n",
        "\n",
        "#del unneedec col\n",
        "df.drop(labels = [\"timestamp\"], axis = 1, inplace = True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1QAFB5cn-WZ"
      },
      "outputs": [],
      "source": [
        "# add wind speed and direction\n",
        "fig = px.line(\n",
        "    data_frame = df.iloc[15000:],\n",
        "    x = \"date\",\n",
        "    y = [\"year_sin\", \"year_cos\"],\n",
        "\n",
        "    title = \"Year sine and cosine\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr4iIpaAn-Wa"
      },
      "outputs": [],
      "source": [
        "# add rolling mean for ao, soi, t2m\n",
        "\n",
        "cols_rolling_mean = [\"soi\", \"ao\", \"mjo_amplitude\", \"t2m\", \"nao\"]\n",
        "offset = 30 #days\n",
        "\n",
        "for col in cols_rolling_mean:\n",
        "\n",
        "    #get rolling mean\n",
        "    col_name = f\"ma_{col}\"\n",
        "    df[col_name] = df[col].rolling(offset).mean()\n",
        "\n",
        "    #create plot\n",
        "    fig = px.line(\n",
        "        data_frame = df[15000:],\n",
        "        y = [col, col_name],\n",
        "        x = \"date\",\n",
        "        title = f\"Moving average: {col}\",\n",
        "        color_discrete_sequence = plt_style_s\n",
        "    )\n",
        "\n",
        "    scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJg2a3DFn-Wa"
      },
      "outputs": [],
      "source": [
        "# add wind speed\n",
        "df['wind_speed'] = np.sqrt(df['u10']**2 + df['v10']**2)\n",
        "\n",
        "#add wind direction\n",
        "df['wind_direction'] = np.rad2deg(np.arctan2(df['u10'], df['v10'])) % 360\n",
        "df['wind_direction'] = (df['wind_direction'] + 90) % 360\n",
        "\n",
        "df.head().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1eDHESOn-Wa"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df,\n",
        "    x = \"wind_direction\",\n",
        "    histnorm = \"probability density\",\n",
        "    title = \"Wind direction\",\n",
        "\n",
        "    color = \"month\",\n",
        "    barmode = \"stack\",\n",
        "    opacity = 1,\n",
        "\n",
        "    nbins = 180,\n",
        "\n",
        "    labels = {\"wind_direction\" : \"wind direction [░]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_c,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnGn7gEgn-Wa"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df,\n",
        "    x = \"wind_speed\",\n",
        "    histnorm = \"probability density\",\n",
        "    title = \"wind_speed\",\n",
        "\n",
        "    color = \"month\",\n",
        "    barmode = \"stack\",\n",
        "    opacity = 1,\n",
        "\n",
        "    nbins = 180,\n",
        "\n",
        "    labels = {\"wind_speed\" : \"wind speed [m/s]\"},\n",
        "\n",
        "    color_discrete_sequence = plt_style_c,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxTJWJThn-Wa"
      },
      "outputs": [],
      "source": [
        "# add peak identifiers for mjo, ao, soi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2GOQhS2n-Wb"
      },
      "outputs": [],
      "source": [
        "df.set_index(keys = \"date\", inplace = True)\n",
        "\n",
        "if save_data:\n",
        "    df.to_csv(os.path.join(data_folder, \"df_fe.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no6tLouYn-Wb"
      },
      "source": [
        "## 2.2 Polar vortex index engineering\n",
        "\n",
        "read some: https://www.severe-weather.eu/global-weather/strong-polar-vortex-warming-collapse-event-forecast-spring-2022-usa-europe-fa/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kifLYzamn-Wb"
      },
      "source": [
        "analyzing patterns during break down events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejQxfzfpn-Wb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df_pv.csv\"))\n",
        "#df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df[\"size\"] = 1\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mguZLEHbn-Wb"
      },
      "source": [
        "interpolate preassure levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcPrs3Own-Wb"
      },
      "outputs": [],
      "source": [
        "#interplolate data for plotting\n",
        "missing_levels = [40,60,80,90]\n",
        "\n",
        "dates   = df[\"date\"].unique()\n",
        "lons    = df[\"longitude\"].unique()\n",
        "lats    = df[\"latitude\"].unique()\n",
        "lvls    = missing_levels\n",
        "\n",
        "interpolation_data = {\n",
        "    \"date\"          :[],\n",
        "    \"longitude\"     :[],\n",
        "    \"latitude\"      :[],\n",
        "    \"level\"         :[],\n",
        "}\n",
        "\n",
        "#ugly and inefficient code\n",
        "\n",
        "for date in dates:\n",
        "    for lon in lons:\n",
        "        for lat in lats:\n",
        "            for lvl in lvls:\n",
        "\n",
        "                interpolation_data[\"date\"].append(date)\n",
        "                interpolation_data[\"longitude\"].append(lon)\n",
        "                interpolation_data[\"latitude\"].append(lat)\n",
        "                interpolation_data[\"level\"].append(lvl)\n",
        "\n",
        "df_int = pd.DataFrame(interpolation_data)\n",
        "df_int.head()\n",
        "\n",
        "#merge dfs\n",
        "df = pd.concat(objs = [df, df_int])\n",
        "df.sort_values(by = [\"date\", \"longitude\", \"latitude\", \"level\"], inplace = True, axis = 0)\n",
        "\n",
        "#clean up\n",
        "df.reset_index(inplace = True)\n",
        "\n",
        "#interpolate values\n",
        "\n",
        "df[\"interpolated\"] = 0\n",
        "df.loc[df[\"level\"].isin(missing_levels), \"interpolated\"] = 1\n",
        "\n",
        "df.interpolate(metohd = \"linear\", inplace = True)\n",
        "\n",
        "#recalculate speed and dircetion for interpolated values\n",
        "\n",
        "#speed\n",
        "df.loc[df[\"interpolated\"] == 1, \"speed\"] = np.sqrt(df[\"u\"] ** 2 + df[\"v\"] ** 2)\n",
        "\n",
        "df.loc[df[\"interpolated\"] == 1, 'direction'] = np.rad2deg(np.arctan2(df['u'], df['v'])) % 360\n",
        "df.loc[df[\"interpolated\"] == 1, 'direction'] = (df['direction'] + 90) % 360\n",
        "\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDowrzT6n-Wb"
      },
      "outputs": [],
      "source": [
        "if save_data is True:\n",
        "    df_pv_clustering = df[[\"date\", \"longitude\", \"latitude\", \"level\", \"t\", \"speed\"]].loc[df[\"longitude\"] == 8]\n",
        "    df_pv_clustering.set_index(\"date\", drop = True, inplace = True)\n",
        "    df_pv_clustering.to_csv(os.path.join(data_folder, \"df_pv_clustering.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y6v8X57n-Wc"
      },
      "outputs": [],
      "source": [
        "plot_scaler = 20\n",
        "fig = px.scatter(\n",
        "    data_frame = df.loc[(df[\"date\"] >= \"2019-03-01\") & (df[\"date\"] <= \"2019-03-04\")],\n",
        "    y = \"level\",\n",
        "    x = \"latitude\",\n",
        "    color = \"speed\",\n",
        "    size = \"size\",\n",
        "    size_max = 1 * plot_scaler - 1,\n",
        "    opacity = 1,\n",
        "    facet_row = \"date\",\n",
        "    #animation_frame = \"date\",\n",
        "\n",
        "    height = (15 * 4)  * plot_scaler,\n",
        "    width = 60 * plot_scaler,\n",
        "    color_continuous_scale =  plt_style_s,\n",
        "\n",
        "    title = \"Polar vortex wind speed\",\n",
        "\n",
        "    labels = {\"speed\" : \"speed [m/s]\"},\n",
        ")\n",
        "\n",
        "fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol=\"square\",),\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRO8TcsVn-Wc"
      },
      "outputs": [],
      "source": [
        "plot_scaler = 20\n",
        "fig = px.scatter(\n",
        "    data_frame = df.loc[(df[\"date\"] >= \"2019-03-01\") & (df[\"date\"] <= \"2019-03-04\")],\n",
        "    y = \"level\",\n",
        "    x = \"latitude\",\n",
        "    color = \"t\",\n",
        "    size = \"size\",\n",
        "    size_max = 1 * plot_scaler - 1,\n",
        "    opacity = 1,\n",
        "    facet_row = \"date\",\n",
        "    #animation_frame = \"date\",\n",
        "\n",
        "    height = (15 * 4)  * plot_scaler,\n",
        "    width = 60 * plot_scaler,\n",
        "    color_continuous_scale =  plt_style_s,\n",
        "\n",
        "    title = \"Polar vortex temperature\",\n",
        "\n",
        "    labels = {\"t\" : \"t [k░]\"},\n",
        ")\n",
        "\n",
        "fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol=\"square\",),\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5m4dkAKn-Wc"
      },
      "outputs": [],
      "source": [
        "#drop latitude 90. Looks like a cut in the data\n",
        "df.drop(df.loc[df[\"latitude\"] == 90].index, inplace = True)\n",
        "\n",
        "\n",
        "plot_scaler = 20\n",
        "fig = px.scatter(\n",
        "    data_frame = df.loc[(df[\"date\"] >= \"1979-01-23\") & (df[\"date\"] <= \"1979-01-30\")],\n",
        "    y = \"level\",\n",
        "    x = \"latitude\",\n",
        "    color = \"speed\",\n",
        "    size = \"size\",\n",
        "    size_max = 1 * plot_scaler - 1,\n",
        "    opacity = 1,\n",
        "    facet_row = \"date\",\n",
        "    #animation_frame = \"date\",\n",
        "\n",
        "    height = (20 * 6)  * plot_scaler,\n",
        "    width = 60 * plot_scaler,\n",
        "    color_continuous_scale =  plt_style_s,\n",
        "\n",
        "    title = \"Polar vortex wind speed\",\n",
        "\n",
        "    labels = {\"speed\" : \"speed [m/s]\"},\n",
        ")\n",
        "\n",
        "fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol=\"square\",),\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k05cJ1NGn-Wc"
      },
      "source": [
        "Costuom index 1 (wind speed only)\n",
        " - Convolution on lattitide level (vertical bar with pieces)\n",
        " - Detect border of vortex by wind speeds (get most constant latitude by std, invert it, multiply by wind speed)\n",
        " - value_0 = border_height (0 .. n, 0 = switzerland, n = north pole)\n",
        " - value_1 = temperature  at height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPnJDQn_n-Wd"
      },
      "outputs": [],
      "source": [
        "def pv_i1_w(df):\n",
        "\n",
        "    #drop unneded cols\n",
        "    df_i1 = df[[\"date\", \"latitude\", \"t\", \"speed\"]]\n",
        "\n",
        "    #crate latitude aggregation\n",
        "    df_i1 = df_i1.groupby(by = [\"date\", \"latitude\"], as_index = False).aggregate(\n",
        "        speed_mean = (\"speed\", \"mean\"),\n",
        "        speed_std = (\"speed\", \"std\"),\n",
        "        t_mean =    (\"t\", \"mean\"),\n",
        "    )\n",
        "\n",
        "    #normalize the standard deviation\n",
        "    # source: https://business.blogthinkbig.com/warning-about-normalizing-data/\n",
        "\n",
        "    #get local min and max\n",
        "    df_i1_minmax = df_i1[[\"date\", \"speed_std\"]]\n",
        "    df_i1_minmax = df_i1_minmax.groupby(by = [\"date\"], as_index = False).aggregate(\n",
        "        speed_std_max = (\"speed_std\", \"max\"),\n",
        "        speed_std_min = (\"speed_std\", \"min\"),\n",
        "    )\n",
        "\n",
        "    #append data together\n",
        "    df_i1[\"date\"] = df_i1[\"date\"].astype(str)\n",
        "    df_i1_minmax[\"date\"] = df_i1_minmax[\"date\"].astype(str)\n",
        "\n",
        "    df_i1[\"date\"] = pd.to_datetime(df_i1[\"date\"])\n",
        "    df_i1_minmax[\"date\"] = pd.to_datetime(df_i1_minmax[\"date\"])\n",
        "\n",
        "    df_i1 = df_i1.merge(right = df_i1_minmax, on = \"date\", how = \"left\")\n",
        "\n",
        "    #calculate normalized std\n",
        "    df_i1[\"speed_norm_inv\"] = 1 - (df_i1[\"speed_std\"] - df_i1[\"speed_std_min\"]) / (df_i1[\"speed_std_max\"] - df_i1[\"speed_std_min\"])\n",
        "\n",
        "    #calculate weighted wind speed\n",
        "    df_i1[\"speed_weight\"] = df_i1[\"speed_mean\"] * df_i1[\"speed_norm_inv\"]\n",
        "\n",
        "    #get local max and min for index based on date\n",
        "    df_i1_max = df_i1[[\"date\", \"speed_weight\"]].groupby(by = \"date\", as_index = False).max()\n",
        "    df_i1_max[\"is_max\"] = 1\n",
        "\n",
        "    #get the lattitude at max\n",
        "    df_i1 = df_i1.merge(right = df_i1_max, on = [\"date\", \"speed_weight\"], how = \"left\")\n",
        "    df_i1 = df_i1.loc[df_i1[\"is_max\"] == 1]\n",
        "\n",
        "    #clean up\n",
        "    df_i1 = df_i1[[\"date\", \"latitude\", \"t_mean\"]]\n",
        "    df_i1_w = df_i1; del df_i1\n",
        "\n",
        "    return df_i1_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJgUT3Ihn-Wd"
      },
      "outputs": [],
      "source": [
        "df_i1_w = pv_i1_w(df)\n",
        "df_i1_w.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUrtc3rGn-We"
      },
      "outputs": [],
      "source": [
        "def plot_i1(df, df_i1, plot_param):\n",
        "\n",
        "    dates = [\n",
        "        \"2019-03-01\", \"2019-03-02\", \"2019-03-03\", \"2019-03-04\"\n",
        "        #\"1979-04-01\",\"1979-05-01\",\"1979-06-01\",\n",
        "        #\"1979-07-01\",\"1979-08-01\",\"1979-09-01\",\n",
        "        #\"1979-10-01\",\"1979-11-01\",\"1979-12-01\",\n",
        "    ]\n",
        "\n",
        "    if plot_param == \"t\":\n",
        "        title = \"temperature\"\n",
        "        widht_correction = 2\n",
        "    elif plot_param == \"speed\":\n",
        "        title = \"wind speed\"\n",
        "        widht_correction = 4\n",
        "\n",
        "    for date in dates:\n",
        "\n",
        "        plot_scaler = 20\n",
        "\n",
        "        fig = px.scatter(\n",
        "            data_frame = df.loc[df[\"date\"] == date],\n",
        "            x = \"latitude\",\n",
        "            y = \"level\",\n",
        "            color = plot_param,\n",
        "            size = \"size\",\n",
        "            size_max = 1 * plot_scaler -7,\n",
        "            opacity = 1,\n",
        "            facet_col = \"date\",\n",
        "            #animation_frame = \"date\",\n",
        "\n",
        "            height = 20  * plot_scaler,\n",
        "            width = 60 * plot_scaler,\n",
        "            color_continuous_scale =  plt_style_s,\n",
        "\n",
        "\n",
        "            title = f\"Polar vortex {title}\",\n",
        "\n",
        "            labels = {\"speed\" : \"speed [m/s]\"},\n",
        "        )\n",
        "\n",
        "        fig.update_traces(\n",
        "            marker=dict(symbol=\"square\",),\n",
        "            selector=dict(mode='markers')\n",
        "        )\n",
        "\n",
        "        #indicator\n",
        "        x = float(df_i1.loc[df_i1[\"date\"] == date][\"latitude\"])\n",
        "\n",
        "        fig.add_vline(\n",
        "            x = x\n",
        "        )\n",
        "\n",
        "        #reverse axis\n",
        "        fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZsZJ1_an-We"
      },
      "outputs": [],
      "source": [
        "plot_i1(df, df_i1_w, \"t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfF-xlPVn-We"
      },
      "source": [
        "Costuom index 1 (wind speed + tempereautre)\n",
        " - Convolution on lattitide level (vertical bar with pieces)\n",
        " - Detect border of vortex by wind speeds and temperature (get most constant latitude by std, invert it, multiply by wind speed)\n",
        " - value_0 = border_height (0 .. n, 0 = switzerland, n = north pole)\n",
        " - value_1 = temperature  at height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUo7QFxen-We"
      },
      "outputs": [],
      "source": [
        "def pv_i1_wt(df, speed_weight = 0.3, t_weight = 0.7):\n",
        "\n",
        "    #drop unneded cols\n",
        "    df_i1 = df[[\"date\", \"latitude\", \"t\", \"speed\"]]\n",
        "\n",
        "    #crate latitude aggregation\n",
        "    df_i1 = df_i1.groupby(by = [\"date\", \"latitude\"], as_index = False).aggregate(\n",
        "        speed_mean = (\"speed\", \"mean\"),\n",
        "        speed_std = (\"speed\", \"std\"),\n",
        "        t_mean =    (\"t\", \"mean\"),\n",
        "    )\n",
        "\n",
        "    #normalize the standard deviation\n",
        "    # source: https://business.blogthinkbig.com/warning-about-normalizing-data/\n",
        "\n",
        "    #get local min and max\n",
        "    df_i1_minmax = df_i1[[\"date\", \"speed_std\", \"t_mean\"]]\n",
        "    df_i1_minmax = df_i1_minmax.groupby(by = [\"date\"], as_index = False).aggregate(\n",
        "        speed_std_max   = (\"speed_std\", \"max\"),\n",
        "        speed_std_min   = (\"speed_std\", \"min\"),\n",
        "        t_mean_max      = (\"t_mean\", \"max\"),\n",
        "        t_mean_min      = (\"t_mean\", \"min\"),\n",
        "    )\n",
        "\n",
        "    #append data together\n",
        "    df_i1[\"date\"] = df_i1[\"date\"].astype(str)\n",
        "    df_i1_minmax[\"date\"] = df_i1_minmax[\"date\"].astype(str)\n",
        "\n",
        "    df_i1[\"date\"] = pd.to_datetime(df_i1[\"date\"])\n",
        "    df_i1_minmax[\"date\"] = pd.to_datetime(df_i1_minmax[\"date\"])\n",
        "\n",
        "    df_i1 = df_i1.merge(right = df_i1_minmax, on = \"date\", how = \"left\")\n",
        "\n",
        "    #calculate normalized std\n",
        "    df_i1[\"speed_norm_inv\"] = 1 - (df_i1[\"speed_std\"] - df_i1[\"speed_std_min\"]) / (df_i1[\"speed_std_max\"] - df_i1[\"speed_std_min\"])\n",
        "    df_i1[\"t_norm_inv\"] = 1 - (df_i1[\"t_mean\"] - df_i1[\"t_mean_min\"]) / (df_i1[\"t_mean_max\"] - df_i1[\"t_mean_min\"])\n",
        "\n",
        "    #calculate weighted wind speed\n",
        "    df_i1[\"weight\"] = df_i1[\"speed_mean\"] * (df_i1[\"speed_norm_inv\"] * speed_weight + df_i1[\"t_norm_inv\"] * t_weight)\n",
        "\n",
        "    #get local max and min for index based on date\n",
        "    df_i1_max = df_i1[[\"date\", \"weight\"]].groupby(by = \"date\", as_index = False).max()\n",
        "    df_i1_max[\"is_max\"] = 1\n",
        "\n",
        "    #get the lattitude at max\n",
        "    df_i1 = df_i1.merge(right = df_i1_max, on = [\"date\", \"weight\"], how = \"left\")\n",
        "    df_i1 = df_i1.loc[df_i1[\"is_max\"] == 1]\n",
        "\n",
        "    #clean up\n",
        "    df_i1 = df_i1[[\"date\", \"latitude\", \"t_mean\"]]\n",
        "    df_i1_wt = df_i1; del df_i1\n",
        "\n",
        "    return df_i1_wt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n1UhI29n-Wf"
      },
      "outputs": [],
      "source": [
        "df_i1_wt = pv_i1_wt(df)\n",
        "df_i1_wt.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKu--k39n-Wf"
      },
      "outputs": [],
      "source": [
        "plot_i1(df, df_i1_wt, \"speed\") #\"speed\", \"t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAbtIVkLn-Wg"
      },
      "outputs": [],
      "source": [
        "del df_i1_w, df_i1_wt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEsjCiEcn-Wg"
      },
      "source": [
        " Custom index 2 (edge detection):\n",
        " - detect sharp borders of temperature changes\n",
        " - create a contrast values accors n vertical rows\n",
        " - detect sharpest border, based on threshold\n",
        " - if no value recheas the threshold, a breakdown can be detected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlsbvnqyn-Wg"
      },
      "outputs": [],
      "source": [
        "# Custom index idea 2:\n",
        "# - detect sharp borders of temperature changes\n",
        "# - create a contrast values accors n vertical rows\n",
        "# - detect sharpest border, based on threshold\n",
        "# - if no value recheas the threshold, a breakdown can be detected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvpsumvKn-Wh"
      },
      "outputs": [],
      "source": [
        "class PV_ind_2_v2():\n",
        "\n",
        "    def main(df, n_lat, threshold, break_down_offset, break_down_sensitivity, metric):\n",
        "        \"\"\"metric: [\"t\", \"speed\"]\"\"\"\n",
        "\n",
        "        df_i2 = PV_ind_2_v2.get_delta(df, n_lat, metric)\n",
        "        df_i2 = PV_ind_2_v2.get_local_max_delta(df_i2)\n",
        "        df_i2 = PV_ind_2_v2.apply_threshold(df_i2, threshold)\n",
        "        df_i2 = PV_ind_2_v2.detect_breakdown(df_i2, break_down_offset, break_down_sensitivity)\n",
        "\n",
        "        return df_i2\n",
        "\n",
        "    def get_delta(df, n, metric):\n",
        "\n",
        "        #drop unneded cols\n",
        "        df_i2 = df[[\"date\", \"latitude\", \"t\", \"speed\"]]\n",
        "        del df\n",
        "\n",
        "        #crate latitude aggregation\n",
        "        df_i2 = df_i2.groupby(by = [\"date\", \"latitude\"], as_index = False).aggregate(\n",
        "            speed_mean  = (\"speed\", \"mean\"),\n",
        "            #speed_std   = (\"speed\", \"std\"),\n",
        "            t_mean      = (\"t\", \"mean\"),\n",
        "            #t_std       = (\"t\", \"std\"),\n",
        "        )\n",
        "\n",
        "        df_i2.sort_values(by = [\"date\", \"latitude\"], ascending = [True , False], inplace = True)\n",
        "\n",
        "        #create offshift for border detection\n",
        "        cols = [f\"{metric}_mean\"]\n",
        "\n",
        "        for i in range(1, n+1):\n",
        "            df_i2[f\"{metric}_mean_-{i}\"] = df_i2[f\"{metric}_mean\"].shift(-i)\n",
        "            cols.append(f\"{metric}_mean_{-i}\")\n",
        "\n",
        "        #create deltas for border detection (square values to only get positive values and highlight bigger deltas)\n",
        "        cols_delta = []\n",
        "        for i in range(n):\n",
        "            df_i2[f\"delta_{i}\"] = abs(df_i2[cols[i]] - df_i2[cols[i + 1]])\n",
        "            cols_delta.append(f\"delta_{i}\")\n",
        "\n",
        "        #sum deltas to get border value\n",
        "        df_i2[\"delta\"] = df_i2[cols_delta].mean(axis=1)\n",
        "\n",
        "        #drop lower n cols\n",
        "        lats = df_i2[\"latitude\"].unique()\n",
        "        lats.sort()\n",
        "        lats = lats[:n]\n",
        "\n",
        "        df_i2.loc[df_i2[\"latitude\"].isin(lats), \"delta\"] = None\n",
        "\n",
        "        #clean up\n",
        "        df_i2.drop(labels = cols[1:] + cols_delta, axis = 1, inplace = True)\n",
        "\n",
        "        return df_i2\n",
        "\n",
        "    def get_local_max_delta(df_i2):\n",
        "\n",
        "        #get max values\n",
        "        df_i2_max = df_i2[[\"date\", \"delta\"]].groupby(by = [\"date\"], as_index = False).max()\n",
        "        df_i2_max[\"is_max\"] = 1\n",
        "\n",
        "        #set max in master df\n",
        "        df_i2 = df_i2.merge(right = df_i2_max, on = [\"date\", \"delta\"], how = \"left\")\n",
        "        #df_i2[\"is_max\"] = df_i2[\"is_max\"].fillna(0)\n",
        "        #df_i2[\"is_max\"] = df_i2[\"is_max\"].astype(int)\n",
        "\n",
        "        #clean up\n",
        "        del df_i2_max\n",
        "        return df_i2\n",
        "\n",
        "    def apply_threshold(df_i2, threshold):\n",
        "\n",
        "        #set th as multiplicator\n",
        "        th = threshold + 1\n",
        "\n",
        "        #get mean\n",
        "        df_i2_d = df_i2[[\"date\",\"delta\"]].groupby(by = \"date\", as_index = False).aggregate(\n",
        "            mean_delta = (\"delta\", \"mean\"),\n",
        "        )\n",
        "\n",
        "        #combine and apply th\n",
        "        df_i2 = df_i2.merge(right = df_i2_d, on = \"date\", how = \"left\")\n",
        "        df_i2[\"mean_delta\"] = df_i2[\"mean_delta\"] * th\n",
        "\n",
        "        #compare\n",
        "        df_i2[\"pv_edge\"] = 0\n",
        "        df_i2.loc[(df_i2[\"is_max\"] == 1) & (df_i2[\"delta\"] > df_i2[\"mean_delta\"]), \"pv_edge\"] = 1\n",
        "\n",
        "        #clean up\n",
        "        df_i2.dropna(subset = \"is_max\", inplace = True)\n",
        "        df_i2.drop(labels = [\"is_max\"], axis = 1, inplace = True)\n",
        "\n",
        "        return df_i2\n",
        "\n",
        "    def detect_breakdown(df_i2, break_down_offset, break_down_sensitivity):\n",
        "\n",
        "        #considered months\n",
        "        considered_months = [12,1,2,3,4]\n",
        "        df_i2[\"date\"] = pd.to_datetime(df_i2[\"date\"])\n",
        "\n",
        "        #consider offset by applying rolling mean.\n",
        "        df_i2[\"pv_edge_offset\"] = df_i2[\"pv_edge\"].rolling(break_down_offset).mean()\n",
        "\n",
        "        #break down event\n",
        "\n",
        "        df_i2[\"pv_break_down_event\"] = 0\n",
        "        df_i2.loc[\n",
        "            (df_i2[\"pv_edge_offset\"] <= break_down_sensitivity) &\n",
        "            (df_i2[\"pv_edge_offset\"].shift(1) > break_down_sensitivity) &\n",
        "            (df_i2[\"date\"].dt.month.isin(considered_months))\n",
        "            , \"pv_break_down_event\"] = 1\n",
        "\n",
        "        #clean up\n",
        "        df_i2.drop(labels = \"pv_edge_offset\", axis = 1, inplace = True)\n",
        "\n",
        "        return df_i2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLz3wrHVn-Wi"
      },
      "outputs": [],
      "source": [
        "#params\n",
        "n_lat                   = 3 #lat offset for calultion\n",
        "threshold               = 1 #in % for vortex, multiple which has to be exceeded from popluation mean, to be counted as a pv edge\n",
        "break_down_offset       = 3 #last n span, used to detect a break down\n",
        "break_down_sensitivity  = 0.1 # in %. Sets the barrier or threshold, at which the value crossing it, a breakdown will be detected\n",
        "\n",
        "df_i2_s = PV_ind_2_v2.main(\n",
        "    df = df,\n",
        "    n_lat = n_lat,\n",
        "    threshold = threshold,\n",
        "    break_down_offset = break_down_offset,\n",
        "    break_down_sensitivity = break_down_sensitivity,\n",
        "    metric = \"speed\",\n",
        ")\n",
        "\n",
        "df_i2_s.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD4_Lvqln-Wi"
      },
      "outputs": [],
      "source": [
        "#params\n",
        "n_lat                   = 2 #lat offset for calultion\n",
        "threshold               = 1 #in % for vortex, multiple which has to be exceeded from popluation mean, to be counted as a pv edge\n",
        "break_down_offset       = 3 #last n span, used to detect a break down\n",
        "break_down_sensitivity  = 0.1 # in %. Sets the barrier or threshold, at which the value crossing it, a breakdown will be detected\n",
        "\n",
        "df_i2_t = PV_ind_2_v2.main(\n",
        "    df = df,\n",
        "    n_lat = n_lat,\n",
        "    threshold = threshold,\n",
        "    break_down_offset = break_down_offset,\n",
        "    break_down_sensitivity = break_down_sensitivity,\n",
        "    metric = \"t\",\n",
        ")\n",
        "\n",
        "df_i2_t.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "307jiBGyn-Wj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_i2(df, df_i, plot_param, dates = None):\n",
        "\n",
        "    if dates is None:\n",
        "\n",
        "        dates = [\n",
        "            \"2019-03-01\", \"2019-03-02\", \"2019-03-03\", \"2019-03-04\"\n",
        "            #\"2022-04-01\",\"2022-05-01\",\"2022-06-01\",\n",
        "            #\"2022-07-01\",\"2022-08-01\",\"2022-09-01\",\n",
        "            #\"2022-10-01\",\"2022-11-01\",\"2022-12-01\",\n",
        "        ]\n",
        "\n",
        "    if plot_param == \"t\":\n",
        "        title = \"temperature\"\n",
        "    elif plot_param == \"speed\":\n",
        "        title = \"wind speed\"\n",
        "\n",
        "    for date in dates:\n",
        "\n",
        "        plot_scaler = 20\n",
        "\n",
        "        fig = px.scatter(\n",
        "            data_frame = df.loc[df[\"date\"] == date],\n",
        "            x = \"latitude\",\n",
        "            y = \"level\",\n",
        "            color = plot_param,\n",
        "            size = \"size\",\n",
        "            size_max = 1 * plot_scaler - 3,\n",
        "            opacity = 1,\n",
        "            facet_col = \"date\",\n",
        "            #animation_frame = \"date\",\n",
        "\n",
        "            height = 20  * plot_scaler,\n",
        "            width = 60 * plot_scaler,\n",
        "            color_continuous_scale  = plt_style_s,\n",
        "\n",
        "            title = f\"Polar vortex: {title}\",\n",
        "\n",
        "            labels = {\"speed\" : \"speed [m/s]\", \"t\" : \"t [░k]\"},\n",
        "        )\n",
        "\n",
        "        fig.update_traces(\n",
        "            marker=dict(symbol=\"square\",),\n",
        "            selector=dict(mode='markers')\n",
        "        )\n",
        "\n",
        "        #indicator\n",
        "        x = float(df_i.loc[df_i[\"date\"] == date][\"latitude\"])\n",
        "        #th_exceeded = int(df_i1.loc[df_i1[\"date\"] == date][\"pv_edge\"])\n",
        "\n",
        "        fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "        fig.add_vline(\n",
        "            x = x,\n",
        "            #line_color = \"black\",\n",
        "            #line_width = th_exceeded * 5,\n",
        "        )\n",
        "\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekIRC6Mon-Wj"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx0QE4ysn-Wj"
      },
      "outputs": [],
      "source": [
        "plot_i2(df, df_i2_s, \"speed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdAYSUCwn-Wk"
      },
      "outputs": [],
      "source": [
        "plot_i2(df, df_i2_t, \"t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKVPMsNjn-Wk"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_i2_s.iloc[-5000:],\n",
        "    x = \"date\",\n",
        "    y = \"pv_break_down_event\",\n",
        "\n",
        "    title = \"Polar vortex edge detection\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFiTJrSOn-Wk"
      },
      "source": [
        "Custom index 3 (delta index):\n",
        " - convolution of data at each end with weighted mean, based on distance (twice, one for wind speed, and one for temperature)\n",
        " - get delta of the area as a single value for each metric from the two obainted values\n",
        " - or leave the two values as they are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB-dofe5n-Wk"
      },
      "outputs": [],
      "source": [
        "class PV_ind_3():\n",
        "\n",
        "    def main(df, metric):\n",
        "\n",
        "        df_i3 = PV_ind_3.agg(df, metric)\n",
        "        df_i3 = PV_ind_3.set_weights(df_i3)\n",
        "        df_i3 = PV_ind_3.normalize_metric(df_i3, metric)\n",
        "        df_i3 = PV_ind_3.set_p_values(df_i3, metric)\n",
        "\n",
        "        return df_i3\n",
        "\n",
        "    def agg(df, metric):\n",
        "\n",
        "        #aggreagte data\n",
        "        df_i3 = df[[\"date\", \"latitude\", metric]].groupby(by = [\"date\", \"latitude\"], as_index = False).mean()\n",
        "        #df_i3.rename(mapper = {metric : \"metric\"}, inplace = True, axis = 1)\n",
        "\n",
        "        return df_i3\n",
        "\n",
        "    def normalize_metric(df_i3, metric):\n",
        "\n",
        "        #get local min and max\n",
        "        df_i3_minmax = df_i3[[\"date\", metric]]\n",
        "        df_i3_minmax = df_i3_minmax.groupby(by = \"date\").aggregate(\n",
        "            metric_max      = (metric, \"max\"),\n",
        "            metric_min      = (metric, \"min\"),\n",
        "        )\n",
        "\n",
        "        #append data together\n",
        "        df_i3 = df_i3.merge(right = df_i3_minmax, on = \"date\", how = \"left\")\n",
        "\n",
        "        #calculated normalized metric\n",
        "        df_i3[f\"{metric}_norm\"] = (df_i3[metric] - df_i3[\"metric_min\"]) / (df_i3[\"metric_max\"] - df_i3[\"metric_min\"])\n",
        "\n",
        "        #clean up\n",
        "        df_i3.drop(labels = [\"metric_max\", \"metric_min\"], axis = 1, inplace = True)\n",
        "\n",
        "        return df_i3\n",
        "\n",
        "    def set_weights(df_i3):\n",
        "\n",
        "        #transform\n",
        "        min_lat = float(df_i3[\"latitude\"].min())\n",
        "        df_i3[\"weight_n\"] = df_i3[\"latitude\"] - min_lat\n",
        "\n",
        "        #transform to values between 0 and 1\n",
        "        max_lat = float(df_i3[\"latitude\"].max())\n",
        "\n",
        "        df_i3[\"weight_n\"] = df_i3[\"weight_n\"] / max_lat\n",
        "        df_i3[\"weight_s\"] = 1 - df_i3[\"weight_n\"]\n",
        "\n",
        "        return df_i3\n",
        "\n",
        "    def set_p_values(df_i3, metric):\n",
        "\n",
        "        #get p_value\n",
        "        df_i3[\"p_north\"] = df_i3[f\"{metric}_norm\"] * df_i3[\"weight_n\"]\n",
        "        df_i3[\"p_south\"] = df_i3[f\"{metric}_norm\"] * df_i3[\"weight_s\"]\n",
        "\n",
        "        #get sum\n",
        "        df_i3.drop(labels = [metric, \"weight_n\", \"weight_s\", f\"{metric}_norm\", \"latitude\"], axis = 1, inplace = True)\n",
        "        df_i3 = df_i3.groupby(by = [\"date\"], as_index = False).sum()\n",
        "\n",
        "        #get delta\n",
        "        df_i3[\"p_delta\"] = df_i3[\"p_south\"] - df_i3[\"p_north\"]\n",
        "\n",
        "        return df_i3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbF-yfXYn-Wl"
      },
      "outputs": [],
      "source": [
        "df_i3 = PV_ind_3.main(df, metric = \"t\")\n",
        "df_i3.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLa2BAsjn-Wl"
      },
      "outputs": [],
      "source": [
        "def plot_i3(df, df_i3, plot_param):\n",
        "\n",
        "    dates = [\n",
        "        \"2019-03-01\", \"2019-03-02\", \"2019-03-03\", \"2019-03-04\",\n",
        "        #\"2022-04-01\",\"2022-05-01\",\"2022-06-01\",\n",
        "        #\"2022-07-01\",\"2022-08-01\",\"2022-09-01\",\n",
        "        #\"2022-10-01\",\"2022-11-01\",\"2022-12-01\",\n",
        "    ]\n",
        "\n",
        "    if plot_param == \"t\":\n",
        "        title = \"temperature\"\n",
        "        widht_correction = 2\n",
        "    elif plot_param == \"speed\":\n",
        "        title = \"wind speed\"\n",
        "        widht_correction = 4\n",
        "\n",
        "\n",
        "    for date in dates:\n",
        "\n",
        "        plot_scaler = 19\n",
        "\n",
        "        #indicator\n",
        "        p_delta = round(float(df_i3.loc[df_i3[\"date\"] == date][\"p_delta\"]),2)\n",
        "\n",
        "        fig = px.scatter(\n",
        "            data_frame = df.loc[df[\"date\"] == date],\n",
        "            x = \"latitude\",\n",
        "            y = \"level\",\n",
        "            color = plot_param,\n",
        "            size = \"size\",\n",
        "            size_max = 1 * plot_scaler - 3,\n",
        "            opacity = 1,\n",
        "            facet_col = \"date\",\n",
        "            #animation_frame = \"date\",\n",
        "\n",
        "            height = 20 * plot_scaler,\n",
        "            width = 60 * plot_scaler,\n",
        "            color_continuous_scale  = plt_style_s,\n",
        "\n",
        "            title = f\"Polar vortex: {title}\\nP_delta: {p_delta}\",\n",
        "\n",
        "            labels = {\"speed\" : \"speed [m/s]\", \"t\" : \"t [░k]\"},\n",
        "        )\n",
        "\n",
        "        fig.update_traces(\n",
        "            marker=dict(symbol=\"square\",),\n",
        "            selector=dict(mode='markers')\n",
        "        )\n",
        "\n",
        "        fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG9lwavVn-Wl"
      },
      "outputs": [],
      "source": [
        "plot_i3(df = df, df_i3 = df_i3, plot_param = \"t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5NIOqvyn-Wm"
      },
      "outputs": [],
      "source": [
        "df_i3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLCkzek4n-Wm"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_i3.iloc[-5000:],\n",
        "    x = \"date\",\n",
        "    y = \"p_delta\",\n",
        "\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    title = \"Index 3: Temperature\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU38CJ91n-Wm"
      },
      "source": [
        "Custom index 4 (local max):\n",
        " - get the local max, though all pressure levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy9Oh4g3n-Wm"
      },
      "outputs": [],
      "source": [
        "def pv_ind_4(df, metric, agg):\n",
        "    \"\"\"\n",
        "    metric = [\"speed\", \"t\"]\n",
        "    agg = [\"sum\", \"max\", \"mean\", \"median\"]\n",
        "    \"\"\"\n",
        "\n",
        "    #drop unneded cols\n",
        "    df_i4 = df[[\"date\", \"latitude\", \"t\", \"speed\"]]\n",
        "\n",
        "    #crate latitude aggregation\n",
        "    df_i4 = df_i4.groupby(by = [\"date\", \"latitude\"], as_index = False).aggregate(\n",
        "        speed_agg = (\"speed\", agg),\n",
        "        t_agg =    (\"t\", agg),\n",
        "    )\n",
        "\n",
        "    #normalize the standard deviation\n",
        "    # source: https://business.blogthinkbig.com/warning-about-normalizing-data/\n",
        "\n",
        "    #get local max for metric and agg\n",
        "    func = {\"speed\" : \"max\", \"t\" : \"min\"}\n",
        "\n",
        "    df_i4_max = df_i4[[\"date\",f\"{metric}_agg\"]]\n",
        "    df_i4_max = df_i4_max.groupby(by = [\"date\"], as_index = False).aggregate(\n",
        "        metric_max = (f\"{metric}_agg\", func[metric]),\n",
        "    )\n",
        "\n",
        "    #append data together\n",
        "    df_i4[\"date\"] = df_i4[\"date\"].astype(str)\n",
        "    df_i4_max[\"date\"] = df_i4_max[\"date\"].astype(str)\n",
        "\n",
        "    df_i4[\"date\"] = pd.to_datetime(df_i4[\"date\"])\n",
        "    df_i4_max[\"date\"] = pd.to_datetime(df_i4_max[\"date\"])\n",
        "\n",
        "    df_i4 = df_i4.merge(right = df_i4_max, on = [\"date\"], how = \"left\")\n",
        "\n",
        "    #only get matchin max value\n",
        "    df_i4 = df_i4.loc[df_i4[\"metric_max\"] == df_i4[f\"{metric}_agg\"]]\n",
        "    df_i4.drop(labels = \"metric_max\", axis = 1, inplace = True)\n",
        "\n",
        "    return df_i4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6PEGjFyn-Wm"
      },
      "outputs": [],
      "source": [
        "df_i4_s = pv_ind_4(df, \"speed\", \"mean\")\n",
        "df_i4_s.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDp0JCrrn-Wm"
      },
      "outputs": [],
      "source": [
        "plot_i2(df, df_i4_s, \"speed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBjVYhSMn-Wm"
      },
      "outputs": [],
      "source": [
        "df_i4_t = pv_ind_4(df, \"t\", \"mean\")\n",
        "df_i4_t.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbLUvUNJn-Wn"
      },
      "outputs": [],
      "source": [
        "plot_i2(df, df_i4_t, \"t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LSQTtG-n-Wn"
      },
      "outputs": [],
      "source": [
        "df_i4_t[\"type\"] = \"temp\"\n",
        "df_i4_s[\"type\"] = \"speed\"\n",
        "\n",
        "df_i4 = pd.concat(objs = [df_i4_t, df_i4_s])\n",
        "df_i4.sort_values(by = \"date\", ascending = True, inplace = True)\n",
        "\n",
        "df_i4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpSDl3HWn-Wn"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame  = df_i4.iloc[-2000:],\n",
        "    x = \"date\",\n",
        "    y = \"latitude\",\n",
        "    title = \"Polar vortex: Index 4\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    color = \"type\",\n",
        ")\n",
        "\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y47qb5jQn-Wn"
      },
      "outputs": [],
      "source": [
        "#temperature: index 2\n",
        "#wind speed: index 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwfZbQpDn-Wp"
      },
      "source": [
        "Custom index 5 (SSW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtJFh2sEn-Wp"
      },
      "source": [
        "## 2.3 Index optimization and setting\n",
        "Sudden stratospheric warming events: https://csl.noaa.gov/groups/csl8/sswcompendium/majorevents.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi2FfAxbn-Wp"
      },
      "source": [
        "### 2.3.1 PVT: Polar vortex temp (index 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5172Dhxn-Wp"
      },
      "outputs": [],
      "source": [
        "def plot_i_breakdown(df, plot_param, break_down_date):\n",
        "\n",
        "    if plot_param == \"t\":\n",
        "        title = \"temperature\"\n",
        "    elif plot_param == \"speed\":\n",
        "        title = \"wind speed\"\n",
        "\n",
        "    plot_scaler = 19\n",
        "\n",
        "    fig = px.scatter(\n",
        "        data_frame = df,\n",
        "        x = \"latitude\",\n",
        "        y = \"level\",\n",
        "        color = plot_param,\n",
        "        size = \"size\",\n",
        "        size_max = 1 * plot_scaler - 1,\n",
        "        opacity = 1,\n",
        "        facet_row = \"date\",\n",
        "\n",
        "        height = 15 * plot_scaler * len(df[\"date\"].unique().tolist()),\n",
        "        width = 60 * plot_scaler,\n",
        "        color_continuous_scale  = plt_style_s,\n",
        "\n",
        "        title = f\"Polar vortex breakdown - {break_down_date}: {title}\",\n",
        "\n",
        "        labels = {\"speed\" : \"speed [m/s]\", \"t\" : \"t [k]\"},\n",
        "    )\n",
        "\n",
        "    fig.update_traces(\n",
        "        marker=dict(symbol=\"square\",),\n",
        "        selector=dict(mode='markers')\n",
        "    )\n",
        "\n",
        "    fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUWjMWtQn-Wp"
      },
      "outputs": [],
      "source": [
        "del df_i2_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX_gg2BRn-Wp"
      },
      "outputs": [],
      "source": [
        "#optimize temperatrue (i2)\n",
        "#params\n",
        "n_lat                   = 3 #lat offset for calultion\n",
        "threshold               = 1.0 #in % for vortex, multiple which has to be exceeded from popluation mean, to be counted as a pv edge\n",
        "break_down_offset       = 15 #last n span, used to detect a break down\n",
        "break_down_sensitivity  = 0.1 # in %. Sets the barrier or threshold, at which the value crossing it, a breakdown will be detected\n",
        "\n",
        "df_i2_t = PV_ind_2_v2.main(\n",
        "    df = df,\n",
        "    n_lat = n_lat,\n",
        "    threshold = threshold,\n",
        "    break_down_offset = break_down_offset,\n",
        "    break_down_sensitivity = break_down_sensitivity,\n",
        "    metric = \"t\",\n",
        ")\n",
        "\n",
        "df_i2_t.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn_qRMqVn-Wq"
      },
      "outputs": [],
      "source": [
        "n = -1000\n",
        "\n",
        "fig = px.line(\n",
        "    data_frame = df_i2_t,\n",
        "    x = \"date\",\n",
        "    y = \"pv_break_down_event\",\n",
        "    title = \"PV break down events i2: Temperature\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP4Sl9Wcn-Wq"
      },
      "outputs": [],
      "source": [
        "#add momth\n",
        "df_i2_t[\"month\"] = pd.DatetimeIndex(df_i2_t[\"date\"]).month\n",
        "\n",
        "\n",
        "#plot porbaibilty per month\n",
        "fig = px.histogram(\n",
        "    data_frame = df_i2_t,\n",
        "    x = \"month\",\n",
        "    y = \"pv_break_down_event\",\n",
        "    histfunc = \"sum\",\n",
        "    barmode = \"stack\",\n",
        "    title = \"PV break down events i2 per month: Temperature\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        "    width = 700,\n",
        "    height = 700,\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDPFQHsVn-Wq"
      },
      "outputs": [],
      "source": [
        "#add momth\n",
        "df_i2_t[\"month\"] = pd.DatetimeIndex(df_i2_t[\"date\"]).month\n",
        "\n",
        "\n",
        "#plot porbaibilty per month\n",
        "fig = px.histogram(\n",
        "    data_frame = df_i2_t,\n",
        "    x = \"month\",\n",
        "    y = \"pv_edge\",\n",
        "    histfunc = \"sum\",\n",
        "    barmode = \"stack\",\n",
        "    title = \"PV edges i2: Temperature\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBKSSnIhn-Wr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAKlf79fn-Wr"
      },
      "outputs": [],
      "source": [
        "#plot breakdonw events\n",
        "df_i2_t.reset_index(inplace = True, drop = True)\n",
        "\n",
        "plot_range = 5\n",
        "\n",
        "df_i2_t[\"date\"] = df_i2_t[\"date\"].astype(str) #whyyy?\n",
        "\n",
        "for i in df_i2_t.loc[df_i2_t[\"pv_break_down_event\"] == 1].index[-4:]:\n",
        "    for param in [\"t\", \"speed\"]:\n",
        "\n",
        "        print(i)\n",
        "        break_down_date = df_i2_t.iloc[i][\"date\"]\n",
        "\n",
        "        offset = int(plot_range / 2)\n",
        "        start_date      = df_i2_t.iloc[i - offset][\"date\"]\n",
        "        end_date        = df_i2_t.iloc[i + offset][\"date\"]\n",
        "\n",
        "\n",
        "        df_plot = df.loc[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
        "        plot_i_breakdown(df = df_plot, plot_param = param, break_down_date = break_down_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVAxNYQVn-Wr"
      },
      "outputs": [],
      "source": [
        "#prepare data frame for merging\n",
        "unneded_cols = [\"speed_mean\", \"delta\", \"mean_delta\", \"month\"]\n",
        "\n",
        "for col in unneded_cols:\n",
        "\n",
        "    try:\n",
        "        df_i2_t.drop(labels = col, axis = 1, inplace = True)\n",
        "    except:\n",
        "        print(f\"{col} does not exist\")\n",
        "\n",
        "#add prefix to generate unique values\n",
        "df_i2_t = df_i2_t.add_prefix(\"pvt_\") #polar vortex temperature\n",
        "\n",
        "df_i2_t.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CwJMosPn-Wr"
      },
      "source": [
        "### 2.3.2 PVS: Polar vortex wind speed (index 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4dd9UD_n-Wr"
      },
      "outputs": [],
      "source": [
        "class PV_ind_4():\n",
        "\n",
        "    def main(df, metric, threshold, break_down_offset, break_down_sensitivity):\n",
        "\n",
        "        df_i4 = PV_ind_4.get_local_max(df, metric)\n",
        "        df_i4 = PV_ind_4.apply_threshold(df_i4, df, threshold, metric)\n",
        "        df_i4 = PV_ind_4.detect_breakdown(df_i4, break_down_offset, break_down_sensitivity)\n",
        "\n",
        "        return df_i4\n",
        "\n",
        "    def get_local_max(df, metric):\n",
        "\n",
        "        #drop unneded cols\n",
        "        df_i4 = df[[\"date\", \"latitude\", \"t\", \"speed\"]]\n",
        "\n",
        "        #crate latitude aggregation\n",
        "        df_i4 = df_i4.groupby(by = [\"date\", \"latitude\"], as_index = False).aggregate(\n",
        "            speed_mean = (\"speed\", \"mean\"),\n",
        "            t_mean =    (\"t\", \"mean\"),\n",
        "        )\n",
        "\n",
        "        #get local max for metric and agg\n",
        "        func = {\"speed\" : \"max\", \"t\" : \"min\"}\n",
        "\n",
        "        df_i4_max = df_i4[[\"date\",f\"{metric}_mean\"]]\n",
        "        df_i4_max = df_i4_max.groupby(by = [\"date\"], as_index = False).aggregate(\n",
        "            metric_max = (f\"{metric}_mean\", func[metric]),\n",
        "        )\n",
        "\n",
        "        #append data together\n",
        "        df_i4[\"date\"] = df_i4[\"date\"].astype(str)\n",
        "        df_i4_max[\"date\"] = df_i4_max[\"date\"].astype(str)\n",
        "\n",
        "        df_i4[\"date\"] = pd.to_datetime(df_i4[\"date\"])\n",
        "        df_i4_max[\"date\"] = pd.to_datetime(df_i4_max[\"date\"])\n",
        "\n",
        "        df_i4 = df_i4.merge(right = df_i4_max, on = [\"date\"], how = \"left\")\n",
        "\n",
        "        #only get matching max value\n",
        "        df_i4 = df_i4.loc[df_i4[\"metric_max\"] == df_i4[f\"{metric}_mean\"]]\n",
        "        df_i4.drop(labels = \"metric_max\", axis = 1, inplace = True)\n",
        "\n",
        "        #drop unneded metric\n",
        "        unneded_metric = [\"t\", \"speed\"]\n",
        "        unneded_metric.remove(metric)\n",
        "        df_i4.drop(labels = f\"{unneded_metric[0]}_mean\", axis = 1, inplace = True)\n",
        "\n",
        "        return df_i4\n",
        "\n",
        "    def apply_threshold(df_i4, df, threshold, metric):\n",
        "\n",
        "        #only get needed cols\n",
        "        df_i4_mean = df[[\"date\", metric]]\n",
        "\n",
        "        #set th as multiplicator\n",
        "        th = threshold + 1\n",
        "\n",
        "        #get overall mean metric\n",
        "        df_i4_mean = df_i4_mean.groupby(by = \"date\", as_index = False).mean()\n",
        "\n",
        "        #stupid date time formats keep changing\n",
        "        df_i4_mean['date'] = df_i4_mean['date'].astype('datetime64[ns]')\n",
        "\n",
        "        #merge and apply th\n",
        "        df_i4 = df_i4.merge(right = df_i4_mean, on = \"date\", how = \"left\")\n",
        "        df_i4[metric] = df_i4[metric] * th\n",
        "\n",
        "        #check if th is exceeded\n",
        "        df_i4[\"pv_edge\"] = 0\n",
        "        df_i4.loc[df_i4[f\"{metric}_mean\"] > df_i4[metric], \"pv_edge\"] = 1\n",
        "\n",
        "        #drop unnded cols\n",
        "        df_i4.drop(labels = [metric], inplace = True, axis = 1)\n",
        "\n",
        "        return df_i4\n",
        "\n",
        "    def detect_breakdown(df_i4, break_down_offset, break_down_sensitivity):\n",
        "\n",
        "        #relevant months\n",
        "        considered_months = [12,1,2,3,4]\n",
        "\n",
        "        #consider offset by applying rolling mean.\n",
        "        df_i4[\"pv_edge_offset\"] = df_i4[\"pv_edge\"].rolling(break_down_offset).mean()\n",
        "\n",
        "        #break down event\n",
        "        df_i4[\"pv_break_down_event\"] = 0\n",
        "        df_i4.loc[\n",
        "            (df_i4[\"pv_edge_offset\"] <= break_down_sensitivity) &\n",
        "            (df_i4[\"pv_edge_offset\"].shift(1) > break_down_sensitivity) &\n",
        "            (df_i4[\"date\"].dt.month.isin(considered_months))\n",
        "            , \"pv_break_down_event\"] = 1\n",
        "\n",
        "        #clean up\n",
        "        df_i4.drop(labels = \"pv_edge_offset\", axis = 1, inplace = True)\n",
        "\n",
        "        return df_i4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQRaaJckn-Wr"
      },
      "outputs": [],
      "source": [
        "threshold               = 0.5\n",
        "break_down_offset       = 60 #last n span, used to detect a break down\n",
        "break_down_sensitivity  = 0.875 # in %. Sets the barrier or threshold, at which the value crossing it, a breakdown will be detected\n",
        "\n",
        "df_i4_s = PV_ind_4.main(\n",
        "    df = df,\n",
        "    metric = \"speed\",\n",
        "\n",
        "    threshold = threshold,\n",
        "    break_down_offset = break_down_offset,\n",
        "    break_down_sensitivity = break_down_sensitivity,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRdryuBXn-Wr"
      },
      "outputs": [],
      "source": [
        "df_i4_s.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E3p8-w-n-Wr"
      },
      "outputs": [],
      "source": [
        "n = -1000\n",
        "\n",
        "fig = px.line(\n",
        "    data_frame = df_i4_s,\n",
        "    x = \"date\",\n",
        "    y = \"pv_break_down_event\",\n",
        "    title = \"PV break down events i4: Wind speed\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M67VIcl0n-Ws"
      },
      "outputs": [],
      "source": [
        "#add momth\n",
        "df_i4_s[\"month\"] = pd.DatetimeIndex(df_i4_s[\"date\"]).month\n",
        "\n",
        "\n",
        "#plot porbaibilty per month\n",
        "fig = px.histogram(\n",
        "    data_frame = df_i4_s,\n",
        "    x = \"month\",\n",
        "    y = \"pv_break_down_event\",\n",
        "    histfunc = \"sum\",\n",
        "    barmode = \"stack\",\n",
        "    title = \"PV break down events i4 per month: Wind speed\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        "    height = 700,\n",
        "    width = 700,\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBliaDvPn-Ws"
      },
      "outputs": [],
      "source": [
        "#add momth\n",
        "df_i4_s[\"month\"] = pd.DatetimeIndex(df_i4_s[\"date\"]).month\n",
        "\n",
        "\n",
        "#plot porbaibilty per month\n",
        "fig = px.histogram(\n",
        "    data_frame = df_i4_s,\n",
        "    x = \"month\",\n",
        "    y = \"pv_edge\",\n",
        "    histfunc = \"sum\",\n",
        "    barmode = \"stack\",\n",
        "    title = \"PV edges i4: Temperature\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DsNVScZn-Ws"
      },
      "outputs": [],
      "source": [
        "df_i4_s.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DsDpVWvn-Ws"
      },
      "outputs": [],
      "source": [
        "#plot breakdonw events\n",
        "df_i4_s.reset_index(inplace = True, drop = True)\n",
        "df_i4_s[\"date\"] = df_i4_s[\"date\"].astype(str)\n",
        "\n",
        "plot_range = 5\n",
        "\n",
        "for i in df_i4_s.loc[df_i4_s[\"pv_break_down_event\"] == 1].index[-4:]:\n",
        "    for param in [\"t\", \"speed\"]:\n",
        "\n",
        "        print(i)\n",
        "        break_down_date = \"2022-03-05\" #df_i4_s.iloc[i][\"date\"]\n",
        "\n",
        "        offset = int(plot_range / 2)\n",
        "        start_date      = \"2022-03-03\"#df_i4_s.iloc[i - offset][\"date\"]\n",
        "        end_date        = \"2022-03-07\"#df_i4_s.iloc[i + offset][\"date\"]\n",
        "\n",
        "        df_plot = df.loc[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
        "        plot_i_breakdown(df = df_plot, plot_param = param, break_down_date = break_down_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc8owXU1n-Wt"
      },
      "outputs": [],
      "source": [
        "#remove unneded cols\n",
        "try:\n",
        "    df_i4_s.drop(labels = \"month\", axis = 1, inplace = True)\n",
        "except:\n",
        "    print(\"month col does not exist\")\n",
        "\n",
        "#add prefix\n",
        "df_i4_s = df_i4_s.add_prefix(\"pvs_\")\n",
        "\n",
        "#check\n",
        "df_i4_s.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd26Svjon-Wt"
      },
      "source": [
        "### 2.3.3 Merge and compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUUbj0O-n-Wt"
      },
      "outputs": [],
      "source": [
        "#merge data\n",
        "df_pv = df_i2_t.merge(right = df_i4_s, left_on = \"pvt_date\", right_on = \"pvs_date\", how = \"left\")\n",
        "\n",
        "#clean up\n",
        "df_pv.drop(labels = \"pvs_date\", axis = 1, inplace = True)\n",
        "df_pv.rename(mapper = {\"pvt_date\" : \"date\"}, axis = 1, inplace = True)\n",
        "\n",
        "df_pv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BNtEA05n-Wt"
      },
      "outputs": [],
      "source": [
        "#some plots and correlation matrix\n",
        "df_pv_plot = df_pv.copy()\n",
        "\n",
        "df_pv_plot[\"size\"] = 1\n",
        "\n",
        "\n",
        "fig = px.scatter(\n",
        "    data_frame = df_pv_plot,\n",
        "    x = \"pvs_latitude\",\n",
        "    y = \"pvt_latitude\",\n",
        "    title = \"Temperature and wind speed comparison: latitude\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    opacity = 0.05,\n",
        "    size_max = 17,\n",
        "    size = \"size\",\n",
        "\n",
        "    height = 1000,\n",
        "    width = 1000,\n",
        "\n",
        ")\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol=\"square\",),\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "212KuP6En-Wt"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_pv_plot,\n",
        "    x = \"date\",\n",
        "    y = [\"pvs_pv_break_down_event\", \"pvt_pv_break_down_event\"],\n",
        "\n",
        "    title = \"Break down events\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        "    labels = {\"value\" : \"\"}\n",
        ")\n",
        "\n",
        "newnames = {\"pvs_pv_break_down_event\" : \"pvs breakdwon\", \"pvt_pv_break_down_event\" : \"pvt breakdown\"}\n",
        "fig.for_each_trace(lambda t: t.update(name = newnames[t.name],\n",
        "                                      legendgroup = newnames[t.name],\n",
        "                                      hovertemplate = t.hovertemplate.replace(t.name, newnames[t.name])\n",
        "                                     )\n",
        "                  )\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bANFXown-Wu"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_pv_plot.iloc[-2000:],\n",
        "    x = \"date\",\n",
        "    y = [\"pvs_pv_edge\", \"pvt_pv_edge\"],\n",
        "\n",
        "    title = \"PV edges\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        "    labels = {\"value\" : \"\"}\n",
        ")\n",
        "\n",
        "newnames = {\"pvs_pv_edge\" : \"pvs edge\", \"pvt_pv_edge\" : \"pvt edge\"}\n",
        "fig.for_each_trace(lambda t: t.update(name = newnames[t.name],\n",
        "                                      legendgroup = newnames[t.name],\n",
        "                                      hovertemplate = t.hovertemplate.replace(t.name, newnames[t.name])\n",
        "                                     )\n",
        "                  )\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmGfKmwon-Wu"
      },
      "outputs": [],
      "source": [
        "df_pv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwngTcmRn-Wu"
      },
      "outputs": [],
      "source": [
        "#merge with main dataframe\n",
        "df_fe = pd.read_csv(os.path.join(data_folder, \"df_fe.csv\"))\n",
        "df_fe.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJr7QsNJn-Wu"
      },
      "outputs": [],
      "source": [
        "print(f\"df_fe: {df_fe.shape}\\ndf_pv: {df_pv.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfpbHtSUn-Wv"
      },
      "outputs": [],
      "source": [
        "df_fe = df_fe.merge(right = df_pv, left_on = \"date\", right_on = \"date\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77HYzztZn-Wv"
      },
      "outputs": [],
      "source": [
        "df_fe.head().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r59mOqgdn-Wv"
      },
      "outputs": [],
      "source": [
        "#save main data frame with added pv data\n",
        "if save_data is True:\n",
        "    df_fe.set_index(\"date\", drop = True, inplace = True)\n",
        "    df_fe.to_csv(os.path.join(data_folder, \"df_fe.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q4COIg7n-Wv"
      },
      "source": [
        "## 2.4 Categorizing data for ml models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIb-b67Yn-Ww"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df_fe.csv\"))\n",
        "df[\"date\"] = pd.DataFrame(df[\"date\"])\n",
        "df.head().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzWOP4pCn-Ww"
      },
      "outputs": [],
      "source": [
        "for month in df[\"month\"].unique()[:3]:\n",
        "\n",
        "    fig = px.box(\n",
        "        data_frame = df.loc[(df[\"month\"] == month)],\n",
        "        x = \"day\",\n",
        "        y = \"t2m\",\n",
        "\n",
        "        title = f\"Temperature by month: {month}\",\n",
        "        color_discrete_sequence = plt_style_s,\n",
        "\n",
        "        width = size[\"width\"],\n",
        "        height = size[\"height\"] * 12,\n",
        "    )\n",
        "\n",
        "    scale_show(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNvOjAS1n-Ww"
      },
      "outputs": [],
      "source": [
        "#calculate fortnight periods 1 and 2\n",
        "df[\"t2m_t1\"] = df[\"t2m\"].rolling(14).mean().shift(-14)\n",
        "df[\"t2m_t2\"] = df[\"t2m\"].rolling(14).mean().shift(-28)\n",
        "\n",
        "#set category\n",
        "df_median = df[[\"month\", \"day\", \"t2m_t1\", \"t2m_t2\"]].groupby(by = [\"month\", \"day\"], as_index = False).aggregate(\n",
        "    t2m_t1_mean = (\"t2m_t1\", \"mean\"),\n",
        "    t2m_t2_mean = (\"t2m_t2\", \"mean\"),\n",
        ")\n",
        "\n",
        "#merge df to main\n",
        "df = df.merge(right = df_median, on = [\"month\", \"day\"])\n",
        "df.sort_values(by = \"date\", inplace = True, ascending = True)\n",
        "\n",
        "#set, if value is above or below median\n",
        "df[\"t2m_t1_cat\"] = 0\n",
        "df[\"t2m_t2_cat\"] = 0\n",
        "\n",
        "df.loc[df[\"t2m_t1\"] >= df[\"t2m_t1_mean\"], \"t2m_t1_cat\"] = 1\n",
        "df.loc[df[\"t2m_t2\"] >= df[\"t2m_t2_mean\"], \"t2m_t2_cat\"] = 1\n",
        "\n",
        "#see df\n",
        "df.head(15).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyVEHMQsn-Ww"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df,\n",
        "    x = \"year\",\n",
        "    y  = \"t2m_t1_cat\",\n",
        "    color = \"t2m_t1_cat\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    title = \"Target variable distribution\",\n",
        "    histfunc = \"count\",\n",
        "    barmode= \"stack\"\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUnWl9K9n-Ww"
      },
      "outputs": [],
      "source": [
        "df_target = df[[\"t2m_t1_cat\", \"t2m_t2_cat\"]]\n",
        "df_target[\"same_cat\"] = df_target[\"t2m_t1_cat\"] == df_target[\"t2m_t2_cat\"]\n",
        "\n",
        "fig = px.histogram(\n",
        "    data_frame = df_target,\n",
        "    x = \"same_cat\",\n",
        "    color = \"same_cat\",\n",
        "    histfunc = \"count\",\n",
        "    title = \"Same category in target vector (t1 = t2)\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        "    width = 500,\n",
        "    height = 500,\n",
        ")\n",
        "\n",
        "fig.add_hline(\n",
        "    y = len(df_target.index.tolist()) / 2,\n",
        "    line_width=3,\n",
        "    line_dash=\"dash\",\n",
        "    line_color=\"grey\",\n",
        "\n",
        "    annotation_text = \"q = 0.5\",\n",
        "    annotation_position=\"right\",\n",
        "    annotation_font_color = \"black\",\n",
        ")\n",
        "\n",
        "fig.update_layout(showlegend=False)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLgnxU4an-Wx"
      },
      "outputs": [],
      "source": [
        "df_median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCL3aBjan-Wx"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df_median,\n",
        "    x = df_median.index,\n",
        "    y = \"t2m_t1_mean\",\n",
        "\n",
        "    title = \"Target: t2m_mean\",\n",
        "    color_continuous_scale = plt_style_c,\n",
        "    labels = {\"t2m_t1_mean\" : \"t2m_mean [k]\"},\n",
        "\n",
        "    color = \"month\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCrbtN8Hn-Wx"
      },
      "outputs": [],
      "source": [
        "if save_data is True:\n",
        "\n",
        "    try:\n",
        "        df.set_index(keys = \"date\", inplace = True)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    df.to_csv(os.path.join(data_folder, \"df_main.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxhMe9zPn-Wy"
      },
      "outputs": [],
      "source": [
        "#outlier analysis\n",
        "df_median.iloc[59]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlPpyY7yn-Wy"
      },
      "outputs": [],
      "source": [
        "df.loc[(df[\"day\"].isin([27,28,29])) & (df[\"month\"] == 2)][[\"t2m\", \"sp\", \"cdir\"]].T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tJqa743n-Wy"
      },
      "outputs": [],
      "source": [
        "#the outlier is neglecgtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnOjAAiOn-Wy"
      },
      "source": [
        "## 2.5 t2m and pv_breakdwon correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8AelOFSn-Wy"
      },
      "outputs": [],
      "source": [
        "df_comp = df[[\"date\", \"t2m\", \"pvs_latitude\", \"pvt_latitude\", \"pvt_pv_break_down_event\", \"pvs_pv_break_down_event\", \"t2m_t1_cat\", \"pvs_pv_edge\", \"pvt_pv_edge\"]]\n",
        "df_comp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Viegy4Mn-Wz"
      },
      "outputs": [],
      "source": [
        "# Correlation\n",
        "df_corr = df_comp.corr().round(1)\n",
        "\n",
        "# Mask to matrix\n",
        "mask = np.zeros_like(df_corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# Viz\n",
        "df_corr_viz = df_corr.mask(mask).dropna(how='all').dropna('columns', how='all')\n",
        "\n",
        "fig = px.imshow(\n",
        "\n",
        "    df_corr_viz,\n",
        "    text_auto=True,\n",
        "    color_continuous_scale = plt_style_c,\n",
        "\n",
        "    title = \"Correlation matrix\",\n",
        "    width = 700,\n",
        "    height = 700,\n",
        "    )\n",
        "\n",
        "fig.show()\n",
        "\n",
        "del df_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgG368I7n-Wz"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df_comp,\n",
        "    y = \"t2m\",\n",
        "    x = \"pvs_latitude\",\n",
        "    color = \"pvs_pv_break_down_event\",\n",
        "    range_color = [0,1],\n",
        "    opacity = 0.1,\n",
        "\n",
        "    color_continuous_scale = plt_style_s,\n",
        "    facet_col = \"pvs_pv_edge\",\n",
        "\n",
        "    title = \"t2m and pvs latitude correlation\",\n",
        "    labels = {\"t2m\" : \"t2m [k]\", \"pvs_pv_break_down_event\" : \"breakdown\"},\n",
        "    trendline = \"ols\",\n",
        "    trendline_color_override = \"red\",\n",
        "\n",
        ")\n",
        "\n",
        "fig.add_hline(\n",
        "    y = 273.15,\n",
        "    line_width=3,\n",
        "    line_dash=\"dash\",\n",
        "    line_color=\"dark blue\",\n",
        "\n",
        "    annotation_text = \"273.15\",\n",
        "    annotation_position=\"bottom left\",\n",
        "    annotation_font_color = \"black\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r39wqC2fn-Wz"
      },
      "outputs": [],
      "source": [
        "fig = px.box(\n",
        "    data_frame = df_comp,\n",
        "    y = \"t2m\",\n",
        "    x = \"pvs_latitude\",\n",
        "\n",
        "    title = \"t2m and pvs latitude correlation\",\n",
        "    labels = {\"t2m\" : \"t2m ░k\"},\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6ZmS4LOn-Wz"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df_comp,\n",
        "    y = \"t2m\",\n",
        "    x = \"pvt_latitude\",\n",
        "    color = \"pvt_pv_break_down_event\",\n",
        "        range_color = [0,1],\n",
        "    opacity = 0.1,\n",
        "\n",
        "    color_continuous_scale = plt_style_s,\n",
        "    facet_col = \"pvt_pv_edge\",\n",
        "\n",
        "    title = \"t2m and pvt latitude correlation\",\n",
        "    labels = {\"t2m\" : \"t2m [k]\", \"pvt_pv_break_down_event\" : \"breakdown\"},\n",
        "    trendline = \"ols\",\n",
        "    trendline_color_override = \"red\",\n",
        ")\n",
        "\n",
        "fig.add_hline(\n",
        "    y = 273.15,\n",
        "    line_width=3,\n",
        "    line_dash=\"dash\",\n",
        "    line_color=\"dark blue\",\n",
        "\n",
        "    annotation_text = \"273.15\",\n",
        "    annotation_position=\"bottom left\",\n",
        "    annotation_font_color = \"black\",\n",
        ")\n",
        "\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c73d8jt9n-Wz"
      },
      "outputs": [],
      "source": [
        "n = 7000\n",
        "\n",
        "pvs_breakdown_list = df_comp.iloc[-n:].loc[df_comp[\"pvs_pv_break_down_event\"] == 1][\"date\"].to_list()\n",
        "\n",
        "fig = px.scatter(\n",
        "    data_frame = df_comp.iloc[-n:],\n",
        "    x = \"date\",\n",
        "    y = \"t2m\",\n",
        "    color = \"t2m_t1_cat\",\n",
        "    color_continuous_scale = plt_style_s,\n",
        "    title = \"PVS break down events\",\n",
        ")\n",
        "\n",
        "for date in pvs_breakdown_list:\n",
        "    fig.add_vline(\n",
        "        x = date,\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYH_ALiOn-W0"
      },
      "outputs": [],
      "source": [
        "n = 7000\n",
        "\n",
        "pvt_breakdown_list = df_comp.iloc[-n:].loc[df_comp[\"pvt_pv_break_down_event\"] == 1][\"date\"].to_list()\n",
        "\n",
        "fig = px.scatter(\n",
        "    data_frame = df_comp.iloc[-n:],\n",
        "    x = \"date\",\n",
        "    y = \"t2m\",\n",
        "    color = \"t2m_t1_cat\",\n",
        "    color_continuous_scale = plt_style_s,\n",
        "    title = \"PVT break down events\",\n",
        ")\n",
        "\n",
        "for date in pvt_breakdown_list:\n",
        "    fig.add_vline(\n",
        "        x = date,\n",
        "    )\n",
        "\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYCMPkRIn-W0"
      },
      "outputs": [],
      "source": [
        "#comparison with ssw data\n",
        "#source: https://csl.noaa.gov/groups/csl8/sswcompendium/majorevents.html\n",
        "\n",
        "ssw_events = [\n",
        "    \"1958-01-01\",\"1958-11-01\",\"1960-01-01\",\"1963-01-01\",\"1965-03-01\",\"1965-12-01\",\"1966-02-01\",\"1968-01-01\",\"1968-11-01\",\"1969-03-01\",\"1970-01-01\",\"1971-01-01\",\"1971-03-01\",\"1973-01-01\",\"1977-01-01\",\"1979-02-01\",\"1980-02-01\",\"1981-02-01\",\"1981-03-01\",\"1981-12-01\",\"1984-02-01\",\"1985-01-01\",\"1987-01-01\",\"1987-12-01\",\"1988-03-01\",\"1989-02-01\",\"1998-12-01\",\"1999-02-01\",\"2000-03-01\",\"2001-02-01\",\"2001-12-01\",\"2002-02-01\",\"2003-01-01\",\"2004-01-01\",\"2006-01-01\",\"2007-02-01\",\"2008-02-01\",\"2009-01-01\",\"2010-02-01\",\"2010-03-01\",\"2013-01-01\",\"2018-02-01\",\"2019-01-01\",\n",
        "]\n",
        "\n",
        "\n",
        "df_comp[\"ssw\"] = 0\n",
        "df_comp.loc[df_comp[\"date\"].isin(ssw_events), \"ssw\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsDenx-bn-W0"
      },
      "outputs": [],
      "source": [
        "n = 5000\n",
        "pvs_breakdown_list = df_comp.iloc[-n:].loc[df_comp[\"ssw\"] == 1][\"date\"].to_list()\n",
        "\n",
        "for col in [\"ssw\", \"pvt_pv_break_down_event\", \"pvs_pv_break_down_event\"]:\n",
        "\n",
        "    bd_list = df_comp.iloc[-n:].loc[df_comp[col] == 1][\"date\"].to_list()\n",
        "\n",
        "\n",
        "    fig = px.scatter(\n",
        "        data_frame = df_comp.iloc[-n:],\n",
        "        x = \"date\",\n",
        "        y = df_comp.iloc[-n:][\"t2m_t1_cat\"].rolling(60).mean(),\n",
        "        color = \"t2m_t1_cat\",\n",
        "        color_continuous_scale =  plt_style_s,\n",
        "        title = f\"Break down event: {col}\",\n",
        "\n",
        "        labels = {\"t2m\" : \"t2m [k]\"}\n",
        "    )\n",
        "\n",
        "    for date in bd_list:\n",
        "        fig.add_vline(\n",
        "            x = date,\n",
        "    )\n",
        "\n",
        "\n",
        "    scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3QHoPUan-W0"
      },
      "outputs": [],
      "source": [
        "df_comp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unq0EKJ8n-W1"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_comp,\n",
        "    x = \"date\",\n",
        "    y = [\"pvs_pv_break_down_event\", \"ssw\"],\n",
        "\n",
        "    title = \"Polar vortex break down event (pvs) and SSW comparison\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfRUM9Twn-W1"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df_comp,\n",
        "    x = \"date\",\n",
        "    y = [\"pvt_pv_break_down_event\",\"pvs_pv_break_down_event\" ,\"ssw\"],\n",
        "\n",
        "    title = \"Polar vortex break down event (pvt, pvs) and SSW comparison\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        "    labels = {\"value\" : \"event\", \"pvt_pv_break_down_event\" : \"pvt\",\"pvs_pv_break_down_event\" : \"pvs\"},\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92t7LA66n-W1"
      },
      "outputs": [],
      "source": [
        "df_comp[[\"pvt_pv_break_down_event\", \"pvs_pv_break_down_event\", \"ssw\"]].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXaiWN2fn-W1"
      },
      "outputs": [],
      "source": [
        "#del df_comp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az3OWC9gn-W2"
      },
      "source": [
        "# 3. Modeling - Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAH7k7-Pn-W3"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import itertools\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\"\n",
        "\n",
        "#machine learning\n",
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "from sklearn.ensemble import RandomForestRegressor as rfr\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit as tsp\n",
        "from sklearn.model_selection import GridSearchCV as gscv\n",
        "\n",
        "#warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdgIrQs9n-W3"
      },
      "outputs": [],
      "source": [
        "run_optim = False #runtime: n min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwTnjfZKn-W3"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig, width = 1500, height = 750):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    fig.update_layout(\n",
        "        width=width,\n",
        "        height=height,\n",
        "    )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sY9nTPcn-W4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df_main.csv\"))\n",
        "df.head().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7ijqtAZn-W4"
      },
      "outputs": [],
      "source": [
        "#sources for reasoning\n",
        "\n",
        "#data splitting: https://towardsdatascience.com/time-series-from-scratch-train-test-splits-and-evaluation-metrics-4fd654de1b37\n",
        "\n",
        "#data standardizing (train, test, valid):\n",
        "#   https://stats.stackexchange.com/questions/202287/why-standardization-of-the-testing-set-has-to-be-performed-with-the-mean-and-sd\n",
        "#   https://medium.com/analytics-vidhya/why-it-makes-a-difference-how-to-standardize-training-and-test-set-e95bf350bed3\n",
        "#   https://stats.stackexchange.com/questions/248543/standardize-training-and-validation-data\n",
        "#   https://www.kaggle.com/questions-and-answers/159183\n",
        "\n",
        "#formula for standardizing: https://www.statisticshowto.com/standardized-values-examples/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzdDo6N0n-W4"
      },
      "outputs": [],
      "source": [
        "class Base(): #parent\n",
        "\n",
        "    def __init__(self, df : object, y_col : list, data_folder : str, results_file : str, model_metric : str, n_jobs : int = 1, window : int = 30):\n",
        "\n",
        "        #save raw df\n",
        "        self.df_raw         = df.copy()\n",
        "\n",
        "        #drop forbidden cols\n",
        "        df = df.copy() #pass by value\n",
        "        df = Base.__drop_forbidden_cols(df, y_col)\n",
        "\n",
        "        #set dataframe for refferencing\n",
        "        self.df             = df.copy() #windowed df, copy because obj is passed by refference\n",
        "\n",
        "        #get and get x_col and y_col\n",
        "        self.y_col          = y_col\n",
        "        self.x_col          = list(df.drop(labels = y_col, axis = 1, inplace = False).columns.to_list())\n",
        "\n",
        "        #misc params\n",
        "        self.random_state   = 42\n",
        "        self.n_jobs         = n_jobs\n",
        "        self.data_folder    = data_folder\n",
        "        self.results_file   = os.path.join(data_folder,results_file)\n",
        "        self.model_metric   = model_metric\n",
        "\n",
        "        #windowing parameters\n",
        "        self.x_window       = window #number of shifting window input features\n",
        "\n",
        "        self.__setup()\n",
        "\n",
        "        return\n",
        "\n",
        "    def __setup(self):\n",
        "\n",
        "        #order was chosen to minimize data loss, at the cost of more needed processing power\n",
        "\n",
        "        #data preparation\n",
        "        self.__windowing()\n",
        "        self.__split_data()\n",
        "        self.__standardize_data()\n",
        "\n",
        "        #setup of metrics and results\n",
        "        self.__set_assesment()\n",
        "\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def __drop_forbidden_cols(df, y_col):\n",
        "\n",
        "        forbidden_cols = ['date','t2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', 't2m_t1_cat', 't2m_t2_cat']\n",
        "\n",
        "        #prevent y_cols from being dropped from the data frame\n",
        "        for y in y_col:\n",
        "            if y in forbidden_cols:\n",
        "                forbidden_cols.remove(y)\n",
        "\n",
        "        #drop forbidden cols, to prevent adding future information to the time series\n",
        "        print(f\"Removed forbidden cols:\\n{forbidden_cols}\")\n",
        "        df.drop(labels = forbidden_cols, axis = 1, inplace = True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def __windowing(self):\n",
        "        \"\"\"creates the windowed data frame\"\"\"\n",
        "\n",
        "        self.x_col_windowed = self.x_col.copy() #copy, becaus lists are past by refference\n",
        "\n",
        "        #input fetures: x\n",
        "        for i in range(1, self.x_window + 1):\n",
        "            for x_col in self.x_col: #inefficient but works just fine\n",
        "\n",
        "                x_col_i             = f\"{x_col}_-{i}\"\n",
        "                self.df[x_col_i]    = df[x_col].shift(i)\n",
        "\n",
        "                self.x_col_windowed.append(x_col_i)\n",
        "\n",
        "        #clean na columns, which were caused by the shifts\n",
        "        self.df.dropna(inplace = True)\n",
        "        print(f\"\\nApplying shifitng window:\\nx_window: -{self.x_window}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __split_data(self):\n",
        "\n",
        "        #reset index for splitting data\n",
        "        self.df.reset_index(inplace = True, drop = True)\n",
        "        length = self.df.shape[0]\n",
        "\n",
        "        #setting split value fractions\n",
        "        valid_frac_0      = 0.1\n",
        "        test_frac_1       = 0.05\n",
        "        train_frac_2      = 0.7\n",
        "        valid_frac_3      = 0.1\n",
        "        test_frac_4       = 0.05\n",
        "\n",
        "        #get end indexes\n",
        "        index_end_list = []\n",
        "        cum_frac = 0\n",
        "\n",
        "        for frac in [valid_frac_0, test_frac_1, train_frac_2, valid_frac_3, test_frac_4]:\n",
        "            cum_frac += frac\n",
        "            index_end_list.append(round(length * cum_frac))\n",
        "\n",
        "        #get indexes (ugly code)\n",
        "        df_indexes = self.df.index.tolist()\n",
        "        train_i     = df_indexes[index_end_list[1] : index_end_list[2]]\n",
        "        valid_i     = df_indexes[ : index_end_list[0]]                      + df_indexes[index_end_list[2] : index_end_list[3]]\n",
        "        test_i      = df_indexes[index_end_list[0] : index_end_list[1]]     + df_indexes[index_end_list[3] : index_end_list[4]]\n",
        "\n",
        "        #get df from indexes\n",
        "        self.df_train_x = self.df[self.x_col_windowed].loc[self.df.index.isin(train_i)]\n",
        "        self.df_train_y = self.df[self.y_col].loc[self.df.index.isin(train_i)]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_valid_x = self.df[self.x_col_windowed].loc[self.df.index.isin(valid_i)]\n",
        "        self.df_valid_y = self.df[self.y_col].loc[self.df.index.isin(valid_i)]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_test_x = self.df[self.x_col_windowed].loc[self.df.index.isin(test_i)]\n",
        "        self.df_test_y = self.df[self.y_col].loc[self.df.index.isin(test_i)]\n",
        "\n",
        "        #check\n",
        "        print(\"\\nSplitting data:\")\n",
        "        for df, df_type in zip (\n",
        "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
        "            \"train,valid,test\".split(\",\")\n",
        "            ):\n",
        "\n",
        "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
        "\n",
        "        #set data for plotting in raw df\n",
        "        self.df_raw[\"set\"] = None\n",
        "        for index_items, set_type in zip([train_i, valid_i, test_i],[\"train\", \"valid\", \"test\"]):\n",
        "            self.df_raw.loc[self.df_raw.index.isin(index_items), \"set\"] = set_type\n",
        "\n",
        "        return\n",
        "\n",
        "    def plot_set_distribution(self, plotter, style, plt_style):\n",
        "\n",
        "        if (style == \"histogram\") and (self.model_metric == \"c\"): #only plotlable with classificaiton model\n",
        "\n",
        "            fig = px.histogram(\n",
        "                data_frame = self.df_raw,\n",
        "                x = \"set\",\n",
        "                color = \"t2m_t2_cat\",\n",
        "                histfunc = \"count\",\n",
        "\n",
        "                barmode = \"group\",\n",
        "                title = \"Categorical distribution of sets\",\n",
        "                color_discrete_sequence = plt_style,\n",
        "            )\n",
        "\n",
        "        elif (style == \"scatter\"):\n",
        "\n",
        "            fig = px.scatter(\n",
        "                data_frame = self.df_raw,\n",
        "                x = \"date\",\n",
        "                y = \"t2m\",\n",
        "                color = \"set\",\n",
        "\n",
        "                title = \"Trend distribution of sets\",\n",
        "                color_discrete_sequence = plt_style,\n",
        "            )\n",
        "\n",
        "        plotter(fig)\n",
        "\n",
        "    def __split_data_deprecated(self):\n",
        "\n",
        "        #df length\n",
        "        length = self.df.shape[0]\n",
        "\n",
        "        #setting split values\n",
        "        valid_frac     = 0.2\n",
        "        test_frac      = 0.1\n",
        "\n",
        "        #get indexes\n",
        "        train_end       = round(length * (1 - (valid_frac + test_frac)))\n",
        "        valid_end       = round(length * (1 - (test_frac)))\n",
        "        test_end        = round(length * (1))\n",
        "\n",
        "        #create train df\n",
        "        self.df_train_x = self.df[self.x_col_windowed].iloc[:train_end]\n",
        "        self.df_train_y = self.df[self.y_col].iloc[:train_end]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_valid_x = self.df[self.x_col_windowed].iloc[train_end:valid_end]\n",
        "        self.df_valid_y = self.df[self.y_col].iloc[train_end:valid_end]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_test_x = self.df[self.x_col_windowed].iloc[valid_end:test_end]\n",
        "        self.df_test_y = self.df[self.y_col].iloc[valid_end:test_end]\n",
        "\n",
        "        #check\n",
        "        print(\"\\nSplitting data:\")\n",
        "        for df, df_type in zip (\n",
        "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
        "            \"train,valid,test\".split(\",\")\n",
        "            ):\n",
        "\n",
        "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __standardize_data(self):\n",
        "\n",
        "        label_cat = [0,1]; label_cat.sort()\n",
        "        self.standardizing_values = {\n",
        "            \"x\" : {},\n",
        "            \"y\" : {},\n",
        "            #    col1 : {\"mean\" : value, \"std\"  : value},\n",
        "            #    col2 : {\"mean\" : value, \"std\"  : value},\n",
        "            #}\n",
        "            #\"y\" : ...\n",
        "        }\n",
        "\n",
        "        print(\"\\nStandardizing values:\")\n",
        "        for col in self.df.columns:\n",
        "\n",
        "            distinct_values = list(self.df[col].unique())\n",
        "            distinct_values.sort()\n",
        "\n",
        "            if label_cat == distinct_values: #skip categorical values\n",
        "                continue\n",
        "\n",
        "            #get mean and std for all columns across both data both data frames\n",
        "            if col in self.x_col_windowed:\n",
        "\n",
        "                self.standardizing_values[\"x\"][col]             = {}\n",
        "                self.standardizing_values[\"x\"][col][\"mean\"]     = self.df_train_x[col].mean()\n",
        "                self.standardizing_values[\"x\"][col][\"std\"]      = self.df_train_x[col].std()\n",
        "\n",
        "            elif col in self.y_col:\n",
        "\n",
        "                self.standardizing_values[\"y\"][col]             = {}\n",
        "                self.standardizing_values[\"y\"][col][\"mean\"]     = self.df_train_y[col].mean()\n",
        "                self.standardizing_values[\"y\"][col][\"std\"]      = self.df_train_y[col].std()\n",
        "\n",
        "        #apply values\n",
        "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
        "            for col in self.standardizing_values[col_type].keys():\n",
        "\n",
        "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
        "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
        "                df[col] = (df[col] - mean) / std #standardization\n",
        "\n",
        "        #check sum\n",
        "        print(f\"Checksum train x: {self.df_train_x[list(self.standardizing_values['x'].keys())].mean().round(2).sum()}\")\n",
        "        print(f\"Checksum train y: {self.df_train_y[list(self.standardizing_values['y'].keys())].mean().round(2).sum()}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def unstandardize_data(self):\n",
        "\n",
        "        #apply values\n",
        "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
        "            for col in self.standardizing_values[col_type].keys():\n",
        "\n",
        "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
        "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
        "                df[col] = df[col] * std + mean #reversed standardization\n",
        "\n",
        "        return\n",
        "\n",
        "    def __unstanardize_y(self, y_t1, y_t2):\n",
        "\n",
        "        mean_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"mean\"]\n",
        "        mean_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"mean\"]\n",
        "\n",
        "        std_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"std\"]\n",
        "        std_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"std\"]\n",
        "\n",
        "        y_t1_unst  = y_t1 * std_t1 + mean_t1\n",
        "        y_t2_unst  = y_t2 * std_t1 + mean_t2\n",
        "\n",
        "        return y_t1_unst, y_t2_unst\n",
        "\n",
        "    def __set_assesment(self):\n",
        "\n",
        "        if self.model_metric == \"c\":\n",
        "            self.get_model_score = self.__get_model_score_c\n",
        "\n",
        "        elif self.model_metric == \"r\":\n",
        "            self.get_model_score = self.__get_model_score_r\n",
        "\n",
        "    def __get_model_score_c(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #default\n",
        "        get_conf_mat = False\n",
        "        mat_labels = [0,1]\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        if model is None:\n",
        "            model = self.model\n",
        "            get_conf_mat = True #only get confuciton matrix when a single model is created\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #get acc\n",
        "            score[f\"{raw_key}_accuracy_t1\"]     = round(accuracy_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_accuracy_t2\"]     = round(accuracy_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_accuracy\"]        = round(accuracy_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get conf mat\n",
        "            if get_conf_mat is True:\n",
        "                score[f\"{raw_key}_mat_t1\"]     = confusion_matrix(y_true = y_t1, y_pred = y_pred_t1, labels = mat_labels)\n",
        "                score[f\"{raw_key}_mat_t2\"]     = confusion_matrix(y_true = y_t2, y_pred = y_pred_t2, labels = mat_labels)\n",
        "                #score[f\"{raw_key}_mat\"]        = confusion_matrix(y_true = y, y_pred = y_pred) #multi labels are not supported\n",
        "\n",
        "        #return metrics\n",
        "        if get_conf_mat is True:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return score\n",
        "\n",
        "        return score\n",
        "\n",
        "    def __get_model_score_r(self, model = None, get_test_score = False, unstandardize_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        set_score = False\n",
        "\n",
        "        if model is None: #model is not none when automation is run\n",
        "            model = self.model\n",
        "            set_score = True\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #unstandardize data (ugly code go brrrr)\n",
        "            if unstandardize_score:\n",
        "                y_t1, y_t2              = self.__unstanardize_y(y_t1 = y_t1, y_t2 = y_t2)\n",
        "                y_pred_t1, y_pred_t2    = self.__unstanardize_y(y_t1 = y_pred_t1, y_t2 = y_pred_t2)\n",
        "\n",
        "                y_pred[:,0], y_pred[:,1]                = y_pred_t1, y_pred_t2\n",
        "                y[self.y_col[0]], y[self.y_col[1]]      = y_t1, y_t2\n",
        "\n",
        "            #get r^2\n",
        "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get rmse\n",
        "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
        "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
        "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
        "\n",
        "        #return metrics\n",
        "        if set_score:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def __get_model_score_r_deprecated(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        if model is None:\n",
        "            model = self.model\n",
        "            set_score = True\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #get r^2\n",
        "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get rmse\n",
        "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
        "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
        "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
        "\n",
        "        #return metrics\n",
        "        if set_score:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def plot_confusion_mat(self, set = \"valid\"):\n",
        "        \"\"\"set = 'train', 'valid', 'test'\"\"\"\n",
        "\n",
        "        mat_keys = [key for key in self.score.keys() if (\"mat\" in key) and (set in key)]\n",
        "\n",
        "        for mat_key in mat_keys:\n",
        "\n",
        "            mat = self.score[mat_key]\n",
        "            title = str(mat_key).replace(\"_mat_\", \" \")\n",
        "\n",
        "            fig  = px.imshow(\n",
        "                mat,\n",
        "                color_continuous_scale = px.colors.sequential.haline_r,\n",
        "                text_auto = True,\n",
        "            )\n",
        "\n",
        "            #labels and layout\n",
        "            fig.update_layout(\n",
        "\n",
        "                title = f\"Confusion matrix: {title}\",\n",
        "\n",
        "                width=500,\n",
        "                height=500,\n",
        "\n",
        "                xaxis_title=\"Predicted label\",\n",
        "                yaxis_title=\"True label\",\n",
        "\n",
        "                xaxis = dict(\n",
        "                    tickmode = 'array',\n",
        "                    tickvals = [0,1],\n",
        "                    ticktext = [\"above\", \"below\"]\n",
        "                ),\n",
        "\n",
        "                yaxis = dict(\n",
        "                    tickmode = 'array',\n",
        "                    tickvals = [0,1],\n",
        "                    ticktext = [\"above\", \"below\"],\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            #set font\n",
        "            fig.update_layout(\n",
        "                font = dict(size=16),\n",
        "                title_font = dict(size=20),\n",
        "                xaxis_title_font = dict(size=18),\n",
        "                yaxis_title_font = dict(size=18),\n",
        "            )\n",
        "\n",
        "            fig.show()\n",
        "\n",
        "    def save_result(self, param, score):\n",
        "\n",
        "        #merge and create a dataframe\n",
        "        param.update(score); data = param\n",
        "        df_result = pd.DataFrame([data])\n",
        "\n",
        "        #create results file and set header length as param to negate reading file\n",
        "        if os.path.isfile(self.results_file) is True:\n",
        "            df_saved_result = pd.read_csv(self.results_file)\n",
        "            df_result = df_saved_result.append(df_result)\n",
        "\n",
        "        df_result.to_csv(self.results_file, index = False)\n",
        "\n",
        "        return\n",
        "\n",
        "    def get_results(self):\n",
        "\n",
        "        df = pd.read_csv(self.results_file)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KSkIYcEn-W5"
      },
      "outputs": [],
      "source": [
        "class RF(Base): #child\n",
        "\n",
        "    def run_optim(self, n_trees = 12, n_depth=3, n_leafs = 5):\n",
        "\n",
        "        self.model = None #clear any models if there should be one\n",
        "\n",
        "        param_list = self.create_param_list(\n",
        "            n_trees = n_trees,\n",
        "            n_depth = n_depth,\n",
        "            n_leafs = n_leafs,\n",
        "        )\n",
        "\n",
        "        for param in param_list:\n",
        "\n",
        "            print( f\"Progress of optim:\\t{round((param_list.index(param) / len(param_list)) * 100,1)}\",end = \"\\r\")\n",
        "\n",
        "            score = self.create_model(param = param, single_model = False)\n",
        "            self.save_result(param = param, score = score)\n",
        "\n",
        "        print(\"Optim successfull. Read results with self.get_results()\")\n",
        "        return\n",
        "\n",
        "    def create_param_list(self, n_trees, n_depth, n_leafs):\n",
        "        \"\"\"return list: [{param_1 : value_1},{},]\"\"\"\n",
        "\n",
        "        param_list = []\n",
        "\n",
        "        #get individual numbers\n",
        "        n_estimators        = [2**n for n in range(1,n_trees+1)]\n",
        "        max_depths          = [10*n for n in range(1,n_depth+1)]\n",
        "        min_sample_leafs    = [2*n  for n in range(1,n_leafs+1)]\n",
        "\n",
        "        combinations = list(itertools.product(n_estimators, max_depths, min_sample_leafs))\n",
        "\n",
        "        for combination in combinations:\n",
        "\n",
        "            params = {\n",
        "                \"n_estimators\"      : combination[0],\n",
        "                \"max_depth\"         : combination[1],\n",
        "                \"min_samples_leaf\"  : combination[2],\n",
        "            }\n",
        "\n",
        "            param_list.append(params)\n",
        "\n",
        "        print(f\"\\nGenerated param combinations: {len(param_list)}\")\n",
        "        return param_list\n",
        "\n",
        "\n",
        "    def create_model(self, param, single_model = True):\n",
        "        \"\"\"if single_model == False:\n",
        "            the scores get retuned\n",
        "            self.mode is not set\n",
        "        elif single_model == True:\n",
        "            scores do not get returned\n",
        "            seld.model is set\"\"\"\n",
        "\n",
        "        #create model\n",
        "        if self.model_metric == \"r\":\n",
        "            ml_model = rfr\n",
        "\n",
        "        elif self.model_metric == \"c\":\n",
        "            ml_model = rfc\n",
        "\n",
        "        model = ml_model(\n",
        "            n_jobs          = self.n_jobs,\n",
        "            random_state    = self.random_state,\n",
        "            **param, #unpack the dict and dumps its values\n",
        "        )\n",
        "\n",
        "        #fit model\n",
        "        model.fit(X = self.df_train_x, y = self.df_train_y)\n",
        "\n",
        "        #set according metrics\n",
        "        if single_model is True:\n",
        "            self.model = model\n",
        "            print(self.model)\n",
        "            return\n",
        "\n",
        "        elif single_model is False:\n",
        "            score = self.get_model_score(model)\n",
        "            return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1m4si_8n-W5"
      },
      "source": [
        "## 3.1 RF Calssification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTcowYETn-W5"
      },
      "source": [
        "### 3.1.1 Modeling and hyper parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzzSj73On-W5"
      },
      "outputs": [],
      "source": [
        "rfc_obj = RF(\n",
        "    df              = df,\n",
        "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "    n_jobs          = 4,\n",
        "\n",
        "    data_folder     = data_folder,\n",
        "    results_file    = \"optim_reults_rfc.csv\",\n",
        "\n",
        "    model_metric    =  \"c\" # r = regression, c = classification\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMd2knxon-W6"
      },
      "outputs": [],
      "source": [
        "plot_dist = False\n",
        "\n",
        "if plot_dist:\n",
        "    rfc_obj.plot_set_distribution(plotter = scale_show, style = \"histogram\", plt_style = plt_style_s)\n",
        "    rfc_obj.plot_set_distribution(plotter = scale_show, style = \"scatter\", plt_style = plt_style_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlBcA7kTn-W6"
      },
      "outputs": [],
      "source": [
        "if run_optim is True:\n",
        "    rfc_obj.run_optim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTx0Kon-n-W6"
      },
      "outputs": [],
      "source": [
        "df_results = rfc_obj.get_results()\n",
        "df_results.sort_values(by = \"valid_accuracy\", ascending = False, inplace = True)\n",
        "df_results.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2oy14RTn-W6"
      },
      "outputs": [],
      "source": [
        "df_results.sort_values(by = \"valid_accuracy_t1\", ascending = False, inplace = True)\n",
        "df_results.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qxze_Dan-W6"
      },
      "outputs": [],
      "source": [
        "df_results.sort_values(by = \"valid_accuracy_t2\", ascending = False, inplace = True)\n",
        "df_results.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzlDJszZn-W7"
      },
      "outputs": [],
      "source": [
        "for frame in [\"\", \"_t1\", \"_t2\"]:\n",
        "\n",
        "    title = \"Fitting graph: estimators\"\n",
        "\n",
        "    if frame != \"\":\n",
        "        title = f\"Fitting graph {frame[1:]}: estimators\"\n",
        "\n",
        "    fig = px.scatter(\n",
        "\n",
        "        data_frame = df_results,\n",
        "        x = \"n_estimators\",\n",
        "        y = [f\"train_accuracy{frame}\", f\"valid_accuracy{frame}\"],\n",
        "\n",
        "        color_discrete_sequence = plt_style_s,\n",
        "        title = title,\n",
        "        log_x  = True,\n",
        "\n",
        "        labels = {\"value\": \"accuracy\"},\n",
        "        range_y = [0,1.1]\n",
        "\n",
        "    )\n",
        "\n",
        "    scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdwawDyDn-W7"
      },
      "source": [
        "### 3.1.2 Top model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlDF-OUUn-W8"
      },
      "outputs": [],
      "source": [
        "#creating sinlge model\n",
        "optimal_param = {\n",
        "    \"n_estimators\"      : 128,\n",
        "    \"max_depth\"         : 10,\n",
        "    \"min_samples_leaf\"   : 8\n",
        "}\n",
        "\n",
        "rfc_obj.create_model(param = optimal_param)\n",
        "rfc_obj.get_model_score()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7BUm6Bjn-W8"
      },
      "source": [
        "https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSCc5R_5n-W8"
      },
      "outputs": [],
      "source": [
        "rfc_obj.plot_confusion_mat(set = \"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abtVgdmRn-W8"
      },
      "outputs": [],
      "source": [
        "# confusion mat\n",
        "#add plotting to class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUFjlgeIn-W8"
      },
      "outputs": [],
      "source": [
        "# feature importance and further improvment\n",
        "weights         = rfc_obj.model.feature_importances_\n",
        "cols_window     = rfc_obj.x_col_windowed\n",
        "n_col           = len(rfc_obj.x_col)\n",
        "window_size     = rfc_obj.x_window\n",
        "\n",
        "mat_head    = rfc_obj.x_col\n",
        "mat         = []\n",
        "\n",
        "last_satrt = 0\n",
        "\n",
        "for i in range(1, 1+window_size):\n",
        "\n",
        "    end = i * n_col\n",
        "    mat.append(list(weights[last_satrt:end]))\n",
        "\n",
        "    last_satrt = end\n",
        "\n",
        "df_feature_importance = pd.DataFrame(mat, columns = rfc_obj.x_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1y7MXTRn-W9"
      },
      "outputs": [],
      "source": [
        "df_feature_importance_by_f = df_feature_importance.sum()\n",
        "df_feature_importance_by_f.sort_values(inplace = True, ascending = False)\n",
        "\n",
        "fig = px.histogram(\n",
        "    x = df_feature_importance_by_f,\n",
        "    y = df_feature_importance_by_f.index,\n",
        "    nbins = n_col,\n",
        "    histfunc = \"sum\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    title = \"Feature importance by feature\",\n",
        "\n",
        "    labels = {\"x\" : \"weights\", \"y\" : \"feature\"}\n",
        ")\n",
        "\n",
        "fig.update_yaxes(autorange=\"reversed\")\n",
        "\n",
        "scale_show(fig, width = 750, height = 750)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4amKpc6Un-W9"
      },
      "outputs": [],
      "source": [
        "df_feature_importance_by_t = df_feature_importance.T.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So-1aVSin-W9"
      },
      "outputs": [],
      "source": [
        "df_feature_importance_by_t = df_feature_importance.T.sum()\n",
        "#df_feature_importance_by_t.sort_values(inplace = True, ascending = False)\n",
        "\n",
        "fig = px.histogram(\n",
        "    x = df_feature_importance_by_t,\n",
        "    y = df_feature_importance_by_t.index.astype(str),\n",
        "    nbins = n_col,\n",
        "    histfunc = \"sum\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    title = \"Feature importance by time\",\n",
        "\n",
        "    labels = {\"x\" : \"weights\", \"y\" : \"time offset\"}\n",
        ")\n",
        "\n",
        "fig.update_yaxes(autorange=\"reversed\")\n",
        "\n",
        "scale_show(fig, width = 750, height = 750)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnGqZSwnn-W9"
      },
      "outputs": [],
      "source": [
        "fig = px.imshow(df_feature_importance.T.to_numpy(), color_continuous_scale = plt_style_c)\n",
        "\n",
        "fig.update_layout(\n",
        "\n",
        "    title = f\"Feature importance\",\n",
        "\n",
        "    width=1000,\n",
        "    height=1000,\n",
        "\n",
        "    yaxis_title=\"Feature\",\n",
        "    xaxis_title=\"Time\",\n",
        "\n",
        "    yaxis = dict(\n",
        "        tickmode = 'array',\n",
        "        tickvals = list(range(len(rfc_obj.x_col))),\n",
        "        ticktext =  rfc_obj.x_col,\n",
        "    ),\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    font = dict(size=16),\n",
        "    title_font = dict(size=20),\n",
        "    xaxis_title_font = dict(size=18),\n",
        "    yaxis_title_font = dict(size=18),\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki4tCj4Jn-W9"
      },
      "outputs": [],
      "source": [
        "#apply to testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o3h_MdQn-W-"
      },
      "outputs": [],
      "source": [
        "rfc_obj.get_model_score( get_test_score = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tcJEAELn-W-"
      },
      "outputs": [],
      "source": [
        "rfc_obj.plot_confusion_mat(set = \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRVtUa-bn-W-"
      },
      "source": [
        "## 3.1.3 Exploration of missing features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVeRieIhn-W-"
      },
      "outputs": [],
      "source": [
        "df_mis = df.drop(labels = \"year\", axis = 1, inplace = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67XU8iY-n-W-"
      },
      "outputs": [],
      "source": [
        "rfc_obj_mis = RF(\n",
        "    df              = df_mis,\n",
        "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "    n_jobs          = 4,\n",
        "\n",
        "    data_folder     = data_folder,\n",
        "    results_file    = \"optim_reults_rfc.csv\",\n",
        "\n",
        "    model_metric    =  \"c\" # r = regression, c = classification\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-PwZ3Ngn-W-"
      },
      "outputs": [],
      "source": [
        "rfc_obj_mis.create_model(param = optimal_param)\n",
        "rfc_obj_mis.get_model_score(get_test_score = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmPkSn88n-W_"
      },
      "outputs": [],
      "source": [
        "rfc_obj_mis.plot_confusion_mat(set = \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZvhtp-kn-W_"
      },
      "source": [
        "## 3.2 Dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by-luypHn-W_"
      },
      "outputs": [],
      "source": [
        "#dropping feautres\n",
        "set_0 = df_feature_importance_by_f.index[:5].tolist() + ['date', 't2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', \"t2m_t1_cat\", \"t2m_t2_cat\"]\n",
        "set_1 = df_feature_importance_by_f.index[:10].tolist() + ['date', 't2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', \"t2m_t1_cat\", \"t2m_t2_cat\"]\n",
        "set_2 = df_feature_importance_by_f.index[:15].tolist() + ['date', 't2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', \"t2m_t1_cat\", \"t2m_t2_cat\"]\n",
        "set_3 = df.drop(labels = [\"nao\", \"ao\", \"t2m\", \"mjo_amplitude\", \"sp\", \"day\", \"soi\"], axis = 1, inplace = False).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaGq7YH8n-XA"
      },
      "outputs": [],
      "source": [
        "df_0 = df[set_0]\n",
        "df_1 = df[set_1]\n",
        "df_2 = df[set_2]\n",
        "df_3 = df[set_3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GoElhgln-XA"
      },
      "outputs": [],
      "source": [
        "def magic(df, set_name):\n",
        "\n",
        "    rfc_set = RF(\n",
        "        df              = df,\n",
        "        y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "        n_jobs          = 4,\n",
        "\n",
        "        data_folder     = data_folder,\n",
        "        results_file    = \"optim_reults_rfc.csv\",\n",
        "\n",
        "        model_metric    =  \"c\" # r = regression, c = classification\n",
        "    )\n",
        "\n",
        "    #creating sinlge model\n",
        "    optimal_param = {\n",
        "            \"n_estimators\"      : 128,\n",
        "            \"max_depth\"         : 10,\n",
        "            \"min_samples_leaf\"   : 8\n",
        "        }\n",
        "\n",
        "    rfc_set.create_model(param = optimal_param)\n",
        "\n",
        "    print(f\"\\n{set_name}\")\n",
        "    score = rfc_set.get_model_score(get_test_score = False)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK3EIkGEn-XA"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "for df_set, set_name in zip([df_0, df_1, df_2, df_3], [\"top 5 features\", \"top 10 features\", \"top 15 features\", \"handpicked\"]):\n",
        "\n",
        "    result = magic(df_set, set_name)\n",
        "    results[set_name] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoNDloe1n-XA"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqIJJ7dRn-XB"
      },
      "outputs": [],
      "source": [
        "df_result = pd.DataFrame(results)\n",
        "df_result.drop(labels = [ind for ind in df_result.index if \"mat\" in ind], axis = 0, inplace = True)\n",
        "df_result.T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci9gz6vpn-XB"
      },
      "source": [
        "## 3.3 Overfitting prevention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yy7xVuMn-XB"
      },
      "outputs": [],
      "source": [
        "def magic(df, param):\n",
        "\n",
        "    rfc_param = RF(\n",
        "        df              = df,\n",
        "        y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "        n_jobs          = 4,\n",
        "\n",
        "        data_folder     = data_folder,\n",
        "        results_file    = \"optim_reults_rfc.csv\",\n",
        "\n",
        "        model_metric    =  \"c\" # r = regression, c = classification\n",
        "    )\n",
        "\n",
        "    rfc_param.create_model(param = param)\n",
        "    score = rfc_param.get_model_score(get_test_score = False)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6xF7kZzn-XB"
      },
      "outputs": [],
      "source": [
        "df.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhYliFTTn-XB"
      },
      "outputs": [],
      "source": [
        "#creating sinlge model\n",
        "param_set0 = {\n",
        "        \"n_estimators\"      : 100,\n",
        "        \"max_depth\"         : 10,\n",
        "        \"min_samples_leaf\"  : 8,\n",
        "    }\n",
        "\n",
        "#creating sinlge model\n",
        "param_set1 = {\n",
        "        \"n_estimators\"      : 128,\n",
        "        \"max_depth\"         : 5,\n",
        "        \"min_samples_leaf\"  : 8,\n",
        "    }\n",
        "\n",
        "#creating sinlge model\n",
        "param_set2 = {\n",
        "        \"n_estimators\"      : 128,\n",
        "        \"max_depth\"         : 10,\n",
        "        \"min_samples_leaf\"  : 200,\n",
        "    }\n",
        "\n",
        "#creating sinlge model\n",
        "param_set3 = {\n",
        "        \"n_estimators\"      : 128,\n",
        "        \"max_depth\"         : 10,\n",
        "        \"min_samples_leaf\"  : 8,\n",
        "        \"min_samples_split\" : 200,\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "for param in [param_set0, param_set1, param_set2, param_set3]:\n",
        "\n",
        "    result = magic(df_2, param)\n",
        "    result.update(param)\n",
        "    results.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CmVIMIn-XB"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRrDTY1Un-XB"
      },
      "outputs": [],
      "source": [
        "#overwrite RF param generator\n",
        "class RFOP(RF):\n",
        "\n",
        "    def create_param_list(self, n_trees, n_depth, n_leafs):\n",
        "        \"\"\"return list: [{param_1 : value_1},{},]\"\"\"\n",
        "\n",
        "        param_list = []\n",
        "\n",
        "        #get individual numbers\n",
        "        n_estimators        = [128]\n",
        "        max_depths          = [2*n for n in range(1,n_depth+1)]\n",
        "        min_sample_leafs    = [25*n  for n in range(1,n_leafs+1)]\n",
        "\n",
        "        combinations = list(itertools.product(n_estimators, max_depths, min_sample_leafs))\n",
        "\n",
        "        for combination in combinations:\n",
        "\n",
        "            params = {\n",
        "                \"n_estimators\"      : combination[0],\n",
        "                \"max_depth\"         : combination[1],\n",
        "                \"min_samples_leaf\"  : combination[2],\n",
        "            }\n",
        "\n",
        "            param_list.append(params)\n",
        "\n",
        "        print(f\"\\nGenerated param combinations: {len(param_list)}\")\n",
        "        return param_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R77685an-XO"
      },
      "outputs": [],
      "source": [
        "rfc_op_obj = RFOP(\n",
        "    df              = df,\n",
        "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "    n_jobs          = 4,\n",
        "\n",
        "    data_folder     = data_folder,\n",
        "    results_file    = \"optim_reults_rfc_op.csv\",\n",
        "\n",
        "    model_metric    =  \"c\" # r = regression, c = classification\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfe8_8n7n-XO"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD18PMIAn-XP"
      },
      "outputs": [],
      "source": [
        "if run_optim:\n",
        "     rfc_op_obj.run_optim(n_depth = 7, n_leafs = 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSqJ2piAn-XP"
      },
      "outputs": [],
      "source": [
        "df_results_op = rfc_op_obj.get_results()\n",
        "df_results_op.sort_values(by = \"valid_accuracy\", ascending = False, inplace = True)\n",
        "\n",
        "df_results_op.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-HG5g9on-XP"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df_results_op,\n",
        "    x = \"train_accuracy\",\n",
        "    y = \"valid_accuracy\",\n",
        "    color = \"min_samples_leaf\",\n",
        "    size = \"max_depth\",\n",
        "\n",
        "    title = \"Fitting graph RF Classifier\",\n",
        "    trendline = \"lowess\",\n",
        "    color_continuous_scale = plt_style_c,\n",
        ")\n",
        "\n",
        "fig.add_hline(\n",
        "    y = 0.55,\n",
        ")\n",
        "fig.add_vline(\n",
        "    x = 0.5\n",
        ")\n",
        "\n",
        "fig.add_scatter()\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RYm5Ga3n-XQ"
      },
      "outputs": [],
      "source": [
        "optimal_param = {\n",
        "    \"n_estimators\" : 128,\n",
        "    \"max_depth\": 8,\n",
        "    \"min_samples_leaf\": 225 #225\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHKdHL06n-XQ"
      },
      "outputs": [],
      "source": [
        "#create optimal model\n",
        "rfc_op_obj.create_model(param = optimal_param)\n",
        "rfc_op_obj.get_model_score(get_test_score = True)\n",
        "rfc_op_obj.plot_confusion_mat(set = \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky_TrV-dn-XQ"
      },
      "outputs": [],
      "source": [
        "[item for item in df.columns.tolist() if item not in df_2.columns.tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1t0BBRRn-XQ"
      },
      "outputs": [],
      "source": [
        "df_2.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BD8Fwdtn-XR"
      },
      "outputs": [],
      "source": [
        "rfc_op_obj = RFOP(\n",
        "    df              = df_2,\n",
        "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "    n_jobs          = 4,\n",
        "\n",
        "    data_folder     = data_folder,\n",
        "    results_file    = \"optim_reults_rfc_op.csv\",\n",
        "\n",
        "    model_metric    =  \"c\" # r = regression, c = classification\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ighm04jcn-XR"
      },
      "outputs": [],
      "source": [
        "#create optimal model\n",
        "rfc_op_obj.create_model(param = optimal_param)\n",
        "rfc_op_obj.get_model_score(get_test_score = True)\n",
        "rfc_op_obj.plot_confusion_mat(set = \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFhUL6S7n-XR"
      },
      "source": [
        "# 4. Modeling - Nu support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgTCpuTqn-XR"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\"\n",
        "\n",
        "#machine learning\n",
        "from sklearn.svm import NuSVC as svc\n",
        "from sklearn.svm import NuSVR as svr\n",
        "from sklearn.multioutput import MultiOutputClassifier #nusvc does not support mutli calss natively\n",
        "from sklearn.multioutput import MultiOutputRegressor #nusvc does not support mutli calss natively\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAsElMeBn-XR"
      },
      "outputs": [],
      "source": [
        "run_optim = False #runtime: n min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyvWjAfyn-XR"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    fig.update_layout(\n",
        "        width=1500,\n",
        "        height=750,\n",
        "    )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2mA34DVn-XR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df_main.csv\"))\n",
        "df.head().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTRqw_cOn-XR"
      },
      "outputs": [],
      "source": [
        "class Base(): #parent\n",
        "\n",
        "    def __init__(self, df : object, y_col : list, data_folder : str, results_file : str, model_metric : str, n_jobs : int = 1, window : int = 30):\n",
        "\n",
        "        #save raw df\n",
        "        self.df_raw         = df.copy()\n",
        "\n",
        "        #drop forbidden cols\n",
        "        df = df.copy() #pass by value\n",
        "        df = Base.__drop_forbidden_cols(df, y_col)\n",
        "\n",
        "        #set dataframe for refferencing\n",
        "        self.df             = df.copy() #windowed df, copy because obj is passed by refference\n",
        "\n",
        "        #get and get x_col and y_col\n",
        "        self.y_col          = y_col\n",
        "        self.x_col          = list(df.drop(labels = y_col, axis = 1, inplace = False).columns.to_list())\n",
        "\n",
        "        #misc params\n",
        "        self.random_state   = 42\n",
        "        self.n_jobs         = n_jobs\n",
        "        self.data_folder    = data_folder\n",
        "        self.results_file   = os.path.join(data_folder,results_file)\n",
        "        self.model_metric   = model_metric\n",
        "\n",
        "        #windowing parameters\n",
        "        self.x_window       = window #number of shifting window input features\n",
        "\n",
        "        self.__setup()\n",
        "\n",
        "        return\n",
        "\n",
        "    def __setup(self):\n",
        "\n",
        "        #order was chosen to minimize data loss, at the cost of more needed processing power\n",
        "\n",
        "        #data preparation\n",
        "        self.__windowing()\n",
        "        self.__split_data()\n",
        "        self.__standardize_data()\n",
        "\n",
        "        #setup of metrics and results\n",
        "        self.__set_assesment()\n",
        "\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def __drop_forbidden_cols(df, y_col):\n",
        "\n",
        "        forbidden_cols = ['date','t2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', 't2m_t1_cat', 't2m_t2_cat']\n",
        "\n",
        "        #prevent y_cols from being dropped from the data frame\n",
        "        for y in y_col:\n",
        "            if y in forbidden_cols:\n",
        "                forbidden_cols.remove(y)\n",
        "\n",
        "        #drop forbidden cols, to prevent adding future information to the time series\n",
        "        print(f\"Removed forbidden cols:\\n{forbidden_cols}\")\n",
        "        df.drop(labels = forbidden_cols, axis = 1, inplace = True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def __windowing(self):\n",
        "        \"\"\"creates the windowed data frame\"\"\"\n",
        "\n",
        "        self.x_col_windowed = self.x_col.copy() #copy, becaus lists are past by refference\n",
        "\n",
        "        #input fetures: x\n",
        "        for i in range(1, self.x_window + 1):\n",
        "            for x_col in self.x_col: #inefficient but works just fine\n",
        "\n",
        "                x_col_i             = f\"{x_col}_-{i}\"\n",
        "                self.df[x_col_i]    = df[x_col].shift(i)\n",
        "\n",
        "                self.x_col_windowed.append(x_col_i)\n",
        "\n",
        "        #clean na columns, which were caused by the shifts\n",
        "        self.df.dropna(inplace = True)\n",
        "        print(f\"\\nApplying shifitng window:\\nx_window: -{self.x_window}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __split_data(self):\n",
        "\n",
        "        #reset index for splitting data\n",
        "        self.df.reset_index(inplace = True, drop = True)\n",
        "        length = self.df.shape[0]\n",
        "\n",
        "        #setting split value fractions\n",
        "        valid_frac_0      = 0.1\n",
        "        test_frac_1       = 0.05\n",
        "        train_frac_2      = 0.7\n",
        "        valid_frac_3      = 0.1\n",
        "        test_frac_4       = 0.05\n",
        "\n",
        "        #get end indexes\n",
        "        index_end_list = []\n",
        "        cum_frac = 0\n",
        "\n",
        "        for frac in [valid_frac_0, test_frac_1, train_frac_2, valid_frac_3, test_frac_4]:\n",
        "            cum_frac += frac\n",
        "            index_end_list.append(round(length * cum_frac))\n",
        "\n",
        "        #get indexes (ugly code)\n",
        "        df_indexes = self.df.index.tolist()\n",
        "        train_i     = df_indexes[index_end_list[1] : index_end_list[2]]\n",
        "        valid_i     = df_indexes[ : index_end_list[0]]                      + df_indexes[index_end_list[2] : index_end_list[3]]\n",
        "        test_i      = df_indexes[index_end_list[0] : index_end_list[1]]     + df_indexes[index_end_list[3] : index_end_list[4]]\n",
        "\n",
        "        #get df from indexes\n",
        "        self.df_train_x = self.df[self.x_col_windowed].loc[self.df.index.isin(train_i)]\n",
        "        self.df_train_y = self.df[self.y_col].loc[self.df.index.isin(train_i)]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_valid_x = self.df[self.x_col_windowed].loc[self.df.index.isin(valid_i)]\n",
        "        self.df_valid_y = self.df[self.y_col].loc[self.df.index.isin(valid_i)]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_test_x = self.df[self.x_col_windowed].loc[self.df.index.isin(test_i)]\n",
        "        self.df_test_y = self.df[self.y_col].loc[self.df.index.isin(test_i)]\n",
        "\n",
        "        #check\n",
        "        print(\"\\nSplitting data:\")\n",
        "        for df, df_type in zip (\n",
        "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
        "            \"train,valid,test\".split(\",\")\n",
        "            ):\n",
        "\n",
        "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
        "\n",
        "        #set data for plotting in raw df\n",
        "        self.df_raw[\"set\"] = None\n",
        "        for index_items, set_type in zip([train_i, valid_i, test_i],[\"train\", \"valid\", \"test\"]):\n",
        "            self.df_raw.loc[self.df_raw.index.isin(index_items), \"set\"] = set_type\n",
        "\n",
        "        return\n",
        "\n",
        "    def plot_set_distribution(self, plotter, style, plt_style):\n",
        "\n",
        "        if (style == \"histogram\") and (self.model_metric == \"c\"): #only plotlable with classificaiton model\n",
        "\n",
        "            fig = px.histogram(\n",
        "                data_frame = self.df_raw,\n",
        "                x = \"set\",\n",
        "                color = \"t2m_t2_cat\",\n",
        "                histfunc = \"count\",\n",
        "\n",
        "                barmode = \"group\",\n",
        "                title = \"Categorical distribution of sets\",\n",
        "                color_discrete_sequence = plt_style,\n",
        "            )\n",
        "\n",
        "        elif (style == \"scatter\"):\n",
        "\n",
        "            fig = px.scatter(\n",
        "                data_frame = self.df_raw,\n",
        "                x = \"date\",\n",
        "                y = \"t2m\",\n",
        "                color = \"set\",\n",
        "\n",
        "                title = \"Trend distribution of sets\",\n",
        "                color_discrete_sequence = plt_style,\n",
        "            )\n",
        "\n",
        "        plotter(fig)\n",
        "\n",
        "    def __split_data_deprecated(self):\n",
        "\n",
        "        #df length\n",
        "        length = self.df.shape[0]\n",
        "\n",
        "        #setting split values\n",
        "        valid_frac     = 0.2\n",
        "        test_frac      = 0.1\n",
        "\n",
        "        #get indexes\n",
        "        train_end       = round(length * (1 - (valid_frac + test_frac)))\n",
        "        valid_end       = round(length * (1 - (test_frac)))\n",
        "        test_end        = round(length * (1))\n",
        "\n",
        "        #create train df\n",
        "        self.df_train_x = self.df[self.x_col_windowed].iloc[:train_end]\n",
        "        self.df_train_y = self.df[self.y_col].iloc[:train_end]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_valid_x = self.df[self.x_col_windowed].iloc[train_end:valid_end]\n",
        "        self.df_valid_y = self.df[self.y_col].iloc[train_end:valid_end]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_test_x = self.df[self.x_col_windowed].iloc[valid_end:test_end]\n",
        "        self.df_test_y = self.df[self.y_col].iloc[valid_end:test_end]\n",
        "\n",
        "        #check\n",
        "        print(\"\\nSplitting data:\")\n",
        "        for df, df_type in zip (\n",
        "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
        "            \"train,valid,test\".split(\",\")\n",
        "            ):\n",
        "\n",
        "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __standardize_data(self):\n",
        "\n",
        "        label_cat = [0,1]; label_cat.sort()\n",
        "        self.standardizing_values = {\n",
        "            \"x\" : {},\n",
        "            \"y\" : {},\n",
        "            #    col1 : {\"mean\" : value, \"std\"  : value},\n",
        "            #    col2 : {\"mean\" : value, \"std\"  : value},\n",
        "            #}\n",
        "            #\"y\" : ...\n",
        "        }\n",
        "\n",
        "        print(\"\\nStandardizing values:\")\n",
        "        for col in self.df.columns:\n",
        "\n",
        "            distinct_values = list(self.df[col].unique())\n",
        "            distinct_values.sort()\n",
        "\n",
        "            if label_cat == distinct_values: #skip categorical values\n",
        "                continue\n",
        "\n",
        "            #get mean and std for all columns across both data both data frames\n",
        "            if col in self.x_col_windowed:\n",
        "\n",
        "                self.standardizing_values[\"x\"][col]             = {}\n",
        "                self.standardizing_values[\"x\"][col][\"mean\"]     = self.df_train_x[col].mean()\n",
        "                self.standardizing_values[\"x\"][col][\"std\"]      = self.df_train_x[col].std()\n",
        "\n",
        "            elif col in self.y_col:\n",
        "\n",
        "                self.standardizing_values[\"y\"][col]             = {}\n",
        "                self.standardizing_values[\"y\"][col][\"mean\"]     = self.df_train_y[col].mean()\n",
        "                self.standardizing_values[\"y\"][col][\"std\"]      = self.df_train_y[col].std()\n",
        "\n",
        "        #apply values\n",
        "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
        "            for col in self.standardizing_values[col_type].keys():\n",
        "\n",
        "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
        "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
        "                df[col] = (df[col] - mean) / std #standardization\n",
        "\n",
        "        #check sum\n",
        "        print(f\"Checksum train x: {self.df_train_x[list(self.standardizing_values['x'].keys())].mean().round(2).sum()}\")\n",
        "        print(f\"Checksum train y: {self.df_train_y[list(self.standardizing_values['y'].keys())].mean().round(2).sum()}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def unstandardize_data(self):\n",
        "\n",
        "        #apply values\n",
        "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
        "            for col in self.standardizing_values[col_type].keys():\n",
        "\n",
        "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
        "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
        "                df[col] = df[col] * std + mean #reversed standardization\n",
        "\n",
        "        return\n",
        "\n",
        "    def __unstanardize_y(self, y_t1, y_t2):\n",
        "\n",
        "        mean_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"mean\"]\n",
        "        mean_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"mean\"]\n",
        "\n",
        "        std_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"std\"]\n",
        "        std_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"std\"]\n",
        "\n",
        "        y_t1_unst  = y_t1 * std_t1 + mean_t1\n",
        "        y_t2_unst  = y_t2 * std_t1 + mean_t2\n",
        "\n",
        "        return y_t1_unst, y_t2_unst\n",
        "\n",
        "    def __set_assesment(self):\n",
        "\n",
        "        if self.model_metric == \"c\":\n",
        "            self.get_model_score = self.__get_model_score_c\n",
        "\n",
        "        elif self.model_metric == \"r\":\n",
        "            self.get_model_score = self.__get_model_score_r\n",
        "\n",
        "    def __set_assesment(self):\n",
        "\n",
        "        if self.model_metric == \"c\":\n",
        "            self.get_model_score = self.__get_model_score_c\n",
        "\n",
        "        elif self.model_metric == \"r\":\n",
        "            self.get_model_score = self.__get_model_score_r\n",
        "\n",
        "    def __get_model_score_c(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #default\n",
        "        get_conf_mat = False\n",
        "        mat_labels = [0,1]\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        if model is None:\n",
        "            model = self.model\n",
        "            get_conf_mat = True #only get confuciton matrix when a single model is created\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #get acc\n",
        "            score[f\"{raw_key}_accuracy_t1\"]     = round(accuracy_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_accuracy_t2\"]     = round(accuracy_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_accuracy\"]        = round(accuracy_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get conf mat\n",
        "            if get_conf_mat is True:\n",
        "                score[f\"{raw_key}_mat_t1\"]     = confusion_matrix(y_true = y_t1, y_pred = y_pred_t1, labels = mat_labels)\n",
        "                score[f\"{raw_key}_mat_t2\"]     = confusion_matrix(y_true = y_t2, y_pred = y_pred_t2, labels = mat_labels)\n",
        "                #score[f\"{raw_key}_mat\"]        = confusion_matrix(y_true = y, y_pred = y_pred) #multi labels are not supported\n",
        "\n",
        "        #return metrics\n",
        "        if get_conf_mat is True:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def __get_model_score_r(self, model = None, get_test_score = False, unstandardize_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        set_score = False\n",
        "\n",
        "        if model is None: #model is not none when automation is run\n",
        "            model = self.model\n",
        "            set_score = True\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #unstandardize data (ugly code go brrrr)\n",
        "            if unstandardize_score:\n",
        "                y_t1, y_t2              = self.__unstanardize_y(y_t1 = y_t1, y_t2 = y_t2)\n",
        "                y_pred_t1, y_pred_t2    = self.__unstanardize_y(y_t1 = y_pred_t1, y_t2 = y_pred_t2)\n",
        "\n",
        "                y_pred[:,0], y_pred[:,1]                = y_pred_t1, y_pred_t2\n",
        "                y[self.y_col[0]], y[self.y_col[1]]      = y_t1, y_t2\n",
        "\n",
        "            #get r^2\n",
        "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get rmse\n",
        "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
        "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
        "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
        "\n",
        "        #return metrics\n",
        "        if set_score:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def __get_model_score_r_deprecated(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        if model is None:\n",
        "            model = self.model\n",
        "            set_score = True\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #get r^2\n",
        "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get rmse\n",
        "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
        "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
        "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
        "\n",
        "        #return metrics\n",
        "        if set_score:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def plot_confusion_mat(self, set = \"valid\"):\n",
        "        \"\"\"set = 'train', 'valid', 'test'\"\"\"\n",
        "\n",
        "        mat_keys = [key for key in self.score.keys() if (\"mat\" in key) and (set in key)]\n",
        "\n",
        "        for mat_key in mat_keys:\n",
        "\n",
        "            mat = self.score[mat_key]\n",
        "            title = str(mat_key).replace(\"_mat_\", \" \")\n",
        "\n",
        "            fig  = px.imshow(\n",
        "                mat,\n",
        "                color_continuous_scale = px.colors.sequential.haline_r,\n",
        "                text_auto = True,\n",
        "            )\n",
        "\n",
        "            #labels and layout\n",
        "            fig.update_layout(\n",
        "\n",
        "                title = f\"Confusion matrix: {title}\",\n",
        "\n",
        "                width=500,\n",
        "                height=500,\n",
        "\n",
        "                xaxis_title=\"Predicted label\",\n",
        "                yaxis_title=\"True label\",\n",
        "\n",
        "                xaxis = dict(\n",
        "                    tickmode = 'array',\n",
        "                    tickvals = [0,1],\n",
        "                    ticktext = [\"above\", \"below\"]\n",
        "                ),\n",
        "\n",
        "                yaxis = dict(\n",
        "                    tickmode = 'array',\n",
        "                    tickvals = [0,1],\n",
        "                    ticktext = [\"above\", \"below\"],\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            #set font\n",
        "            fig.update_layout(\n",
        "                font = dict(size=16),\n",
        "                title_font = dict(size=20),\n",
        "                xaxis_title_font = dict(size=18),\n",
        "                yaxis_title_font = dict(size=18),\n",
        "            )\n",
        "\n",
        "            fig.show()\n",
        "\n",
        "    def save_result(self, param, score):\n",
        "\n",
        "        #merge and create a dataframe\n",
        "        param.update(score); data = param\n",
        "        df_result = pd.DataFrame([data])\n",
        "\n",
        "        #create results file and set header length as param to negate reading file\n",
        "        if os.path.isfile(self.results_file) is True:\n",
        "            df_saved_result = pd.read_csv(self.results_file)\n",
        "            df_result = df_saved_result.append(df_result)\n",
        "\n",
        "        df_result.to_csv(self.results_file, index = False)\n",
        "\n",
        "        return\n",
        "\n",
        "    def get_results(self):\n",
        "\n",
        "        df = pd.read_csv(self.results_file)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX9PT0esn-XT"
      },
      "outputs": [],
      "source": [
        "class SV(Base): #child\n",
        "\n",
        "    def run_optim(self, nu_max = 11, degrees = 9):\n",
        "\n",
        "        self.model = None #clear any models if there should be one\n",
        "\n",
        "        param_list = self.create_param_list(\n",
        "            nu_max = nu_max,\n",
        "            degrees = degrees,\n",
        "        )\n",
        "\n",
        "        for param in param_list:\n",
        "\n",
        "            print( f\"Progress of optim:\\t{round((param_list.index(param) / len(param_list)) * 100,1)}\",end = \"\\r\")\n",
        "\n",
        "            score = self.create_model(param = param, single_model = False)\n",
        "            self.save_result(param = param, score = score)\n",
        "            break\n",
        "\n",
        "        print(\"Optim successfull. Read results with self.get_results()\")\n",
        "        return\n",
        "\n",
        "    def create_param_list(self, nu_max, degrees):\n",
        "        \"\"\"return list: [{param_1 : value_1},{},]\"\"\"\n",
        "\n",
        "        param_list = []\n",
        "\n",
        "        #set values to optimze for with the model\n",
        "        kernels     = [\"linear\", \"rbf\", \"sigmoid\", \"poly\"]\n",
        "        nus         = [i / 10 for i in range(1,nu_max)]\n",
        "        degrees     = list(range(1,degrees))\n",
        "        gammas       = [\"scale\", \"auto\", 0.1, 0.5, 1, 5, 10]\n",
        "\n",
        "        #create kernel dicts\n",
        "        combinations = itertools.product(kernels, nus, degrees, gammas)\n",
        "\n",
        "        for combination in combinations:\n",
        "\n",
        "            #kick out unvalid combiations to minimize calculation time\n",
        "            if (combination[0] != \"poly\") and (combination[2] != 1):\n",
        "                continue\n",
        "            if (combination[0] == \"linear\") and (combination[3] != \"scale\"): #scale is the default param\n",
        "                continue\n",
        "\n",
        "            params = {\n",
        "                \"kernel\"        : combination[0],\n",
        "                \"nu\"            : combination[1],\n",
        "                \"degree\"        : combination[2],\n",
        "                \"gamma\"         : combination[3],\n",
        "            }\n",
        "\n",
        "            param_list.append(params)\n",
        "\n",
        "        print(f\"\\nGenerated param combinations: {len(param_list)}\")\n",
        "        return param_list\n",
        "\n",
        "    def create_model(self, param, single_model = True):\n",
        "        \"\"\"if single_model == False:\n",
        "            the scores get retuned\n",
        "            self.mode is not set\n",
        "        elif single_model == True:\n",
        "            scores do not get returned\n",
        "            seld.model is set\"\"\"\n",
        "\n",
        "        #create model\n",
        "        if self.model_metric == \"r\":\n",
        "            ml_model = svr\n",
        "            multi_output = MultiOutputRegressor\n",
        "\n",
        "        elif self.model_metric == \"c\":\n",
        "            ml_model = svc\n",
        "            multi_output = MultiOutputClassifier\n",
        "\n",
        "        model = multi_output(\n",
        "            n_jobs = self.n_jobs,\n",
        "            estimator = ml_model(\n",
        "                random_state    = self.random_state,\n",
        "                **param, #unpack the dict and dumps its values\n",
        "            )\n",
        "        )\n",
        "\n",
        "        #fit model\n",
        "        model.fit(X = self.df_train_x, Y = self.df_train_y)\n",
        "\n",
        "        #set according metrics\n",
        "        if single_model is True:\n",
        "            self.model = model\n",
        "            print(self.model)\n",
        "            return\n",
        "\n",
        "        elif single_model is False:\n",
        "            score = self.get_model_score(model)\n",
        "            return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52H28Po-n-XT"
      },
      "outputs": [],
      "source": [
        "svc_obj = SV(\n",
        "    df              = df,\n",
        "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "    n_jobs          = 5,\n",
        "\n",
        "    data_folder     = data_folder,\n",
        "    results_file    = \"optim_reults_svc.csv\",\n",
        "\n",
        "    model_metric    =  \"c\", # r = regression, c = classification\n",
        "    window          = 2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QShJxeQLn-XT"
      },
      "outputs": [],
      "source": [
        "if run_optim is True:\n",
        "    svc_obj.run_optim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dynwuyaPn-XU"
      },
      "source": [
        "## 4.2 Model evaluation:\n",
        "- Too high dimensionality\n",
        "- Too many datapoints\n",
        "- Calculation times for a singler model with the given paramets are way too high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx7G-ORRn-XU"
      },
      "source": [
        "# 5. Modeling - Multi layer preceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmX9-EDan-XU"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\"\n",
        "\n",
        "#machine learning\n",
        "from sklearn.neural_network import MLPClassifier    as mlc\n",
        "from sklearn.neural_network import MLPRegressor     as mlr\n",
        "\n",
        "#model scoring\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "#warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTu0DoHSn-XU"
      },
      "outputs": [],
      "source": [
        "run_optim = False #runtime: 711 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-skDMxKSn-XV"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    fig.update_layout(\n",
        "        width=1500,\n",
        "        height=750,\n",
        "    )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAjQsYU-n-XV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df_main.csv\"))\n",
        "df.head().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY1Y7kuTn-XV"
      },
      "source": [
        "## 5.1 Data preparation, Modeling, Architecutre tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRHsbqXOn-XV"
      },
      "outputs": [],
      "source": [
        "class Base(): #parent\n",
        "\n",
        "    def __init__(self, df : object, y_col : list, data_folder : str, results_file : str, model_metric : str, n_jobs : int = 1, window : int = 30):\n",
        "\n",
        "        #save raw df\n",
        "        self.df_raw         = df.copy()\n",
        "\n",
        "        #drop forbidden cols\n",
        "        df = df.copy() #pass by value\n",
        "        df = Base.__drop_forbidden_cols(df, y_col)\n",
        "\n",
        "        #set dataframe for refferencing\n",
        "        self.df             = df.copy() #windowed df, copy because obj is passed by refference\n",
        "\n",
        "        #get and get x_col and y_col\n",
        "        self.y_col          = y_col\n",
        "        self.x_col          = list(df.drop(labels = y_col, axis = 1, inplace = False).columns.to_list())\n",
        "\n",
        "        #misc params\n",
        "        self.random_state   = 42\n",
        "        self.n_jobs         = n_jobs\n",
        "        self.data_folder    = data_folder\n",
        "        self.results_file   = os.path.join(data_folder,results_file)\n",
        "        self.model_metric   = model_metric\n",
        "\n",
        "        #ann parameters\n",
        "        self.default_param = {\n",
        "            \"activation\"        : \"relu\",\n",
        "            \"solver\"            : \"adam\",       #stochastic gradiant descent\n",
        "            \"alpha\"             : 0.1,          #see: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html\n",
        "            \"learning_rate\"     : \"adaptive\",   #trying to improve performance\n",
        "            \"shuffle\"           : False,        #keep order because of time series\n",
        "            \"early_stopping\"    : True,         #trying to reduce processing time\n",
        "            \"max_iter\"          : 200,          #change this if the training is too slow\n",
        "        }\n",
        "\n",
        "        #windowing parameters\n",
        "        self.x_window       = window #number of shifting window input features\n",
        "\n",
        "        self.__setup()\n",
        "\n",
        "        return\n",
        "\n",
        "    def __setup(self):\n",
        "\n",
        "        #order was chosen to minimize data loss, at the cost of more needed processing power\n",
        "\n",
        "        #data preparation\n",
        "        self.__windowing()\n",
        "        self.__split_data()\n",
        "        self.__standardize_data()\n",
        "\n",
        "        #setup of metrics and results\n",
        "        self.__set_assesment()\n",
        "\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def __drop_forbidden_cols(df, y_col):\n",
        "\n",
        "        forbidden_cols = ['date','t2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', 't2m_t1_cat', 't2m_t2_cat']\n",
        "\n",
        "        #prevent y_cols from being dropped from the data frame\n",
        "        for y in y_col:\n",
        "            if y in forbidden_cols:\n",
        "                forbidden_cols.remove(y)\n",
        "\n",
        "        #drop forbidden cols, to prevent adding future information to the time series\n",
        "        print(f\"Removed forbidden cols:\\n{forbidden_cols}\")\n",
        "        df.drop(labels = forbidden_cols, axis = 1, inplace = True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def __windowing(self):\n",
        "        \"\"\"creates the windowed data frame\"\"\"\n",
        "\n",
        "        self.x_col_windowed = self.x_col.copy() #copy, becaus lists are past by refference\n",
        "\n",
        "        #input fetures: x\n",
        "        for i in range(1, self.x_window + 1):\n",
        "            for x_col in self.x_col: #inefficient but works just fine\n",
        "\n",
        "                x_col_i             = f\"{x_col}_-{i}\"\n",
        "                self.df[x_col_i]    = df[x_col].shift(i)\n",
        "\n",
        "                self.x_col_windowed.append(x_col_i)\n",
        "\n",
        "        #clean na columns, which were caused by the shifts\n",
        "        self.df.dropna(inplace = True)\n",
        "        print(f\"\\nApplying shifitng window:\\nx_window: -{self.x_window}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __split_data(self):\n",
        "\n",
        "        #reset index for splitting data\n",
        "        self.df.reset_index(inplace = True, drop = True)\n",
        "        length = self.df.shape[0]\n",
        "\n",
        "        #setting split value fractions\n",
        "        valid_frac_0      = 0.1\n",
        "        test_frac_1       = 0.05\n",
        "        train_frac_2      = 0.7\n",
        "        valid_frac_3      = 0.1\n",
        "        test_frac_4       = 0.05\n",
        "\n",
        "        #get end indexes\n",
        "        index_end_list = []\n",
        "        cum_frac = 0\n",
        "\n",
        "        for frac in [valid_frac_0, test_frac_1, train_frac_2, valid_frac_3, test_frac_4]:\n",
        "            cum_frac += frac\n",
        "            index_end_list.append(round(length * cum_frac))\n",
        "\n",
        "        #get indexes (ugly code)\n",
        "        df_indexes = self.df.index.tolist()\n",
        "        train_i     = df_indexes[index_end_list[1] : index_end_list[2]]\n",
        "        valid_i     = df_indexes[ : index_end_list[0]]                      + df_indexes[index_end_list[2] : index_end_list[3]]\n",
        "        test_i      = df_indexes[index_end_list[0] : index_end_list[1]]     + df_indexes[index_end_list[3] : index_end_list[4]]\n",
        "\n",
        "        #get df from indexes\n",
        "        self.df_train_x = self.df[self.x_col_windowed].loc[self.df.index.isin(train_i)]\n",
        "        self.df_train_y = self.df[self.y_col].loc[self.df.index.isin(train_i)]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_valid_x = self.df[self.x_col_windowed].loc[self.df.index.isin(valid_i)]\n",
        "        self.df_valid_y = self.df[self.y_col].loc[self.df.index.isin(valid_i)]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_test_x = self.df[self.x_col_windowed].loc[self.df.index.isin(test_i)]\n",
        "        self.df_test_y = self.df[self.y_col].loc[self.df.index.isin(test_i)]\n",
        "\n",
        "        #check\n",
        "        print(\"\\nSplitting data:\")\n",
        "        for df, df_type in zip (\n",
        "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
        "            \"train,valid,test\".split(\",\")\n",
        "            ):\n",
        "\n",
        "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
        "\n",
        "        #set data for plotting in raw df\n",
        "        self.df_raw[\"set\"] = None\n",
        "        for index_items, set_type in zip([train_i, valid_i, test_i],[\"train\", \"valid\", \"test\"]):\n",
        "            self.df_raw.loc[self.df_raw.index.isin(index_items), \"set\"] = set_type\n",
        "\n",
        "        return\n",
        "\n",
        "    def plot_set_distribution(self, plotter, style, plt_style):\n",
        "\n",
        "        if (style == \"histogram\") and (self.model_metric == \"c\"): #only plotlable with classificaiton model\n",
        "\n",
        "            fig = px.histogram(\n",
        "                data_frame = self.df_raw,\n",
        "                x = \"set\",\n",
        "                color = \"t2m_t2_cat\",\n",
        "                histfunc = \"count\",\n",
        "\n",
        "                barmode = \"group\",\n",
        "                title = \"Categorical distribution of sets\",\n",
        "                color_discrete_sequence = plt_style,\n",
        "            )\n",
        "\n",
        "        elif (style == \"scatter\"):\n",
        "\n",
        "            fig = px.scatter(\n",
        "                data_frame = self.df_raw,\n",
        "                x = \"date\",\n",
        "                y = \"t2m\",\n",
        "                color = \"set\",\n",
        "\n",
        "                title = \"Trend distribution of sets\",\n",
        "                color_discrete_sequence = plt_style,\n",
        "            )\n",
        "\n",
        "        plotter(fig)\n",
        "\n",
        "    def __split_data_deprecated(self):\n",
        "\n",
        "        #df length\n",
        "        length = self.df.shape[0]\n",
        "\n",
        "        #setting split values\n",
        "        valid_frac     = 0.2\n",
        "        test_frac      = 0.1\n",
        "\n",
        "        #get indexes\n",
        "        train_end       = round(length * (1 - (valid_frac + test_frac)))\n",
        "        valid_end       = round(length * (1 - (test_frac)))\n",
        "        test_end        = round(length * (1))\n",
        "\n",
        "        #create train df\n",
        "        self.df_train_x = self.df[self.x_col_windowed].iloc[:train_end]\n",
        "        self.df_train_y = self.df[self.y_col].iloc[:train_end]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_valid_x = self.df[self.x_col_windowed].iloc[train_end:valid_end]\n",
        "        self.df_valid_y = self.df[self.y_col].iloc[train_end:valid_end]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_test_x = self.df[self.x_col_windowed].iloc[valid_end:test_end]\n",
        "        self.df_test_y = self.df[self.y_col].iloc[valid_end:test_end]\n",
        "\n",
        "        #check\n",
        "        print(\"\\nSplitting data:\")\n",
        "        for df, df_type in zip (\n",
        "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
        "            \"train,valid,test\".split(\",\")\n",
        "            ):\n",
        "\n",
        "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __standardize_data(self):\n",
        "\n",
        "        label_cat = [0,1]; label_cat.sort()\n",
        "        self.standardizing_values = {\n",
        "            \"x\" : {},\n",
        "            \"y\" : {},\n",
        "            #    col1 : {\"mean\" : value, \"std\"  : value},\n",
        "            #    col2 : {\"mean\" : value, \"std\"  : value},\n",
        "            #}\n",
        "            #\"y\" : ...\n",
        "        }\n",
        "\n",
        "        print(\"\\nStandardizing values:\")\n",
        "        for col in self.df.columns:\n",
        "\n",
        "            distinct_values = list(self.df[col].unique())\n",
        "            distinct_values.sort()\n",
        "\n",
        "            if label_cat == distinct_values: #skip categorical values\n",
        "                continue\n",
        "\n",
        "            #get mean and std for all columns across both data both data frames\n",
        "            if col in self.x_col_windowed:\n",
        "\n",
        "                self.standardizing_values[\"x\"][col]             = {}\n",
        "                self.standardizing_values[\"x\"][col][\"mean\"]     = self.df_train_x[col].mean()\n",
        "                self.standardizing_values[\"x\"][col][\"std\"]      = self.df_train_x[col].std()\n",
        "\n",
        "            elif col in self.y_col:\n",
        "\n",
        "                self.standardizing_values[\"y\"][col]             = {}\n",
        "                self.standardizing_values[\"y\"][col][\"mean\"]     = self.df_train_y[col].mean()\n",
        "                self.standardizing_values[\"y\"][col][\"std\"]      = self.df_train_y[col].std()\n",
        "\n",
        "        #apply values\n",
        "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
        "            for col in self.standardizing_values[col_type].keys():\n",
        "\n",
        "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
        "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
        "                df[col] = (df[col] - mean) / std #standardization\n",
        "\n",
        "        #check sum\n",
        "        print(f\"Checksum train x: {self.df_train_x[list(self.standardizing_values['x'].keys())].mean().round(2).sum()}\")\n",
        "        print(f\"Checksum train y: {self.df_train_y[list(self.standardizing_values['y'].keys())].mean().round(2).sum()}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def unstandardize_data(self):\n",
        "\n",
        "        #apply values\n",
        "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
        "            for col in self.standardizing_values[col_type].keys():\n",
        "\n",
        "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
        "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
        "                df[col] = df[col] * std + mean #reversed standardization\n",
        "\n",
        "        return\n",
        "\n",
        "    def __unstanardize_y(self, y_t1, y_t2):\n",
        "\n",
        "        mean_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"mean\"]\n",
        "        mean_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"mean\"]\n",
        "\n",
        "        std_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"std\"]\n",
        "        std_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"std\"]\n",
        "\n",
        "        y_t1_unst  = y_t1 * std_t1 + mean_t1\n",
        "        y_t2_unst  = y_t2 * std_t1 + mean_t2\n",
        "\n",
        "        return y_t1_unst, y_t2_unst\n",
        "\n",
        "    def __set_assesment(self):\n",
        "\n",
        "        if self.model_metric == \"c\":\n",
        "            self.get_model_score = self.__get_model_score_c\n",
        "\n",
        "        elif self.model_metric == \"r\":\n",
        "            self.get_model_score = self.__get_model_score_r\n",
        "\n",
        "    def __get_model_score_c(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #default\n",
        "        get_conf_mat = False\n",
        "        mat_labels = [0,1]\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        if model is None:\n",
        "            model = self.model\n",
        "            get_conf_mat = True #only get confuciton matrix when a single model is created\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #get acc\n",
        "            score[f\"{raw_key}_accuracy_t1\"]     = round(accuracy_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_accuracy_t2\"]     = round(accuracy_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_accuracy\"]        = round(accuracy_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get conf mat\n",
        "            if get_conf_mat is True:\n",
        "                score[f\"{raw_key}_mat_t1\"]     = confusion_matrix(y_true = y_t1, y_pred = y_pred_t1, labels = mat_labels)\n",
        "                score[f\"{raw_key}_mat_t2\"]     = confusion_matrix(y_true = y_t2, y_pred = y_pred_t2, labels = mat_labels)\n",
        "                #score[f\"{raw_key}_mat\"]        = confusion_matrix(y_true = y, y_pred = y_pred) #multi labels are not supported\n",
        "\n",
        "        #return metrics\n",
        "        if get_conf_mat is True:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def __get_model_score_r(self, model = None, get_test_score = False, unstandardize_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        set_score = False\n",
        "\n",
        "        if model is None: #model is not none when automation is run\n",
        "            model = self.model\n",
        "            set_score = True\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #unstandardize data (ugly code go brrrr)\n",
        "            if unstandardize_score:\n",
        "                y_t1, y_t2              = self.__unstanardize_y(y_t1 = y_t1, y_t2 = y_t2)\n",
        "                y_pred_t1, y_pred_t2    = self.__unstanardize_y(y_t1 = y_pred_t1, y_t2 = y_pred_t2)\n",
        "\n",
        "                y_pred[:,0], y_pred[:,1]                = y_pred_t1, y_pred_t2\n",
        "                y[self.y_col[0]], y[self.y_col[1]]      = y_t1, y_t2\n",
        "\n",
        "            #get r^2\n",
        "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get rmse\n",
        "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
        "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
        "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
        "\n",
        "        #return metrics\n",
        "        if set_score:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def plot_confusion_mat(self, set = \"valid\"):\n",
        "        \"\"\"set = 'train', 'valid', 'test'\"\"\"\n",
        "\n",
        "        mat_keys = [key for key in self.score.keys() if (\"mat\" in key) and (set in key)]\n",
        "\n",
        "        for mat_key in mat_keys:\n",
        "\n",
        "            mat = self.score[mat_key]\n",
        "            title = str(mat_key).replace(\"_mat_\", \" \")\n",
        "\n",
        "            fig  = px.imshow(\n",
        "                mat,\n",
        "                color_continuous_scale = px.colors.sequential.haline_r,\n",
        "                text_auto = True,\n",
        "            )\n",
        "\n",
        "            #labels and layout\n",
        "            fig.update_layout(\n",
        "\n",
        "                title = f\"Confusion matrix: {title}\",\n",
        "\n",
        "                width=500,\n",
        "                height=500,\n",
        "\n",
        "                xaxis_title=\"Predicted label\",\n",
        "                yaxis_title=\"True label\",\n",
        "\n",
        "                xaxis = dict(\n",
        "                    tickmode = 'array',\n",
        "                    tickvals = [0,1],\n",
        "                    ticktext = [\"above\", \"below\"]\n",
        "                ),\n",
        "\n",
        "                yaxis = dict(\n",
        "                    tickmode = 'array',\n",
        "                    tickvals = [0,1],\n",
        "                    ticktext = [\"above\", \"below\"],\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            #set font\n",
        "            fig.update_layout(\n",
        "                font = dict(size=16),\n",
        "                title_font = dict(size=20),\n",
        "                xaxis_title_font = dict(size=18),\n",
        "                yaxis_title_font = dict(size=18),\n",
        "            )\n",
        "\n",
        "            fig.show()\n",
        "\n",
        "    def save_result(self, param, score):\n",
        "\n",
        "        #merge and create a dataframe\n",
        "        param.update(score); data = param\n",
        "        df_result = pd.DataFrame([data])\n",
        "\n",
        "        #create results file and set header length as param to negate reading file\n",
        "        if os.path.isfile(self.results_file) is True:\n",
        "            df_saved_result = pd.read_csv(self.results_file)\n",
        "            df_result = df_saved_result.append(df_result)\n",
        "\n",
        "        df_result.to_csv(self.results_file, index = False)\n",
        "\n",
        "        return\n",
        "\n",
        "    def get_results(self):\n",
        "\n",
        "        df = pd.read_csv(self.results_file)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgJg93xtn-XW"
      },
      "outputs": [],
      "source": [
        "a = list(range(10))\n",
        "print(sum(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBowFSQtn-XW"
      },
      "outputs": [],
      "source": [
        "class ML(Base): #child\n",
        "\n",
        "    def run_optim(self, nu_max = 11, degrees = 9, use_small_arch_list = False):\n",
        "\n",
        "        self.model = None #clear any models if there should be one\n",
        "\n",
        "        if use_small_arch_list is False:\n",
        "            arch_list = self.__generate_arch_list_all(\n",
        "                n_layers            = 4,\n",
        "                n_node_steps_div    = 3,\n",
        "                min_nodes           = 10\n",
        "            )\n",
        "        elif use_small_arch_list is True:\n",
        "            arch_list = self.__generate_arch_list_limited()\n",
        "\n",
        "        for arch in arch_list:\n",
        "            print( f\"Progress of optim:\\t{round((arch_list.index(arch) / len(arch_list)) * 100,1)}\",end = \"\\r\")\n",
        "\n",
        "            #refromat to save all comparison data\n",
        "            param = {\n",
        "                \"hidden_layer_sizes\"        : arch,\n",
        "                \"n_layers\"                  : len(arch),\n",
        "                \"n_neurons\"                 : sum(arch),\n",
        "                \"mean_neurosn_per_layer\"    : sum(arch) / len(arch),\n",
        "            }\n",
        "\n",
        "            #create model scoring to evalute models\n",
        "            score = self.create_model(arch = arch, single_model = False)\n",
        "            self.save_result(param = param, score = score)\n",
        "\n",
        "        print(\"Optim successfull. Read results with self.get_results()\")\n",
        "        return\n",
        "\n",
        "    def __generate_arch_list_limited(self, arch_log = 2, lin_arch_scaling = 2, n_layers = 5):\n",
        "\n",
        "        arch_list : list = []\n",
        "        n_features = len(self.df_train_x.columns.tolist())\n",
        "\n",
        "        #nodes_pow_2 = [3 ** (cone_arch_base_power + p) for p in range(1, n_layers + 1)][::-1]\n",
        "        nodes_log_2 = [int(n_features * (1 / arch_log ** i)) for i in range(1,n_layers + 1)]\n",
        "\n",
        "        for n_layer in range(1, n_layers + 1):\n",
        "\n",
        "            #linear\n",
        "            for size in  [n_features / (i * lin_arch_scaling) for i in range(1,4)]:\n",
        "                arch_lin = [int(size)] * n_layer\n",
        "                arch_list.append(arch_lin)\n",
        "\n",
        "            #cone\n",
        "            arch_cone = nodes_log_2[:n_layer]\n",
        "            arch_list.append(arch_cone)\n",
        "\n",
        "            #cone r\n",
        "            arch_cone_r = arch_cone[::-1]\n",
        "            arch_list.append(arch_cone_r)\n",
        "\n",
        "        return arch_list\n",
        "\n",
        "    def __generate_arch_list_all(self, n_layers = 4, n_node_steps_div = 3, min_nodes = 10):\n",
        "\n",
        "        n_input_nodes : int = len(self.df_train_x.columns)\n",
        "        arch_list = []\n",
        "        n_node_list : list = [n_input_nodes]\n",
        "\n",
        "        #set fixed params\n",
        "        min_node_division = 2 #minumum number of nodes on a layer\n",
        "\n",
        "        #divisonal\n",
        "        counter = 1\n",
        "\n",
        "        while True:\n",
        "\n",
        "            n_nodes = int(n_input_nodes / (n_node_steps_div ** counter))\n",
        "\n",
        "            if n_nodes < min_nodes:\n",
        "                break\n",
        "\n",
        "            n_node_list.append(n_nodes)\n",
        "            counter += 1\n",
        "\n",
        "        #clean up\n",
        "        n_node_list = list(set(n_node_list)); n_node_list.append(0); n_node_list.sort()\n",
        "\n",
        "        #create archs\n",
        "        for i in range(1, (len(n_node_list)**n_layers) + 1):\n",
        "\n",
        "            arch = []\n",
        "\n",
        "            for j in list(range(n_layers))[::-1]:\n",
        "\n",
        "                v =  int((i % (len(n_node_list) ** (j + 1)) / (len(n_node_list) ** j)))\n",
        "                arch.append(n_node_list[v])\n",
        "\n",
        "            arch = [k for k in arch if k != 0] #remove zero value\n",
        "\n",
        "            if arch in arch_list:\n",
        "                continue\n",
        "            arch_list.append(arch)\n",
        "\n",
        "        return arch_list\n",
        "\n",
        "    def create_model(self, arch, single_model = True, param = None):\n",
        "        \"\"\"if single_model == False:\n",
        "            the scores get retuned\n",
        "            self.mode is not set\n",
        "        elif single_model == True:\n",
        "            scores do not get returned\n",
        "            seld.model is set\"\"\"\n",
        "\n",
        "        #if no parameters are given, the following default params are used\n",
        "        if param is None:\n",
        "            param = self.default_param\n",
        "\n",
        "        #create model\n",
        "        if self.model_metric == \"r\":\n",
        "            ml_model = mlr\n",
        "            #multi_output = MultiOutputRegressor\n",
        "\n",
        "        elif self.model_metric == \"c\":\n",
        "            ml_model = mlc\n",
        "            #multi_output = MultiOutputClassifier\n",
        "\n",
        "        model = ml_model(\n",
        "            random_state            = self.random_state,\n",
        "            hidden_layer_sizes      = arch,\n",
        "            **param,\n",
        "        )\n",
        "\n",
        "        #model = multi_output(\n",
        "        #    n_jobs = self.n_jobs,\n",
        "        #    estimator = ml_model(\n",
        "        #        random_state    = self.random_state,\n",
        "        #        **param, #unpack the dict and dumps its values\n",
        "        #    )\n",
        "        #)\n",
        "\n",
        "        #fit model\n",
        "        model.fit(X = self.df_train_x, y = self.df_train_y)\n",
        "\n",
        "        #set according metrics\n",
        "        if single_model is True:\n",
        "            self.model = model\n",
        "            print(self.model)\n",
        "            return\n",
        "\n",
        "        elif single_model is False:\n",
        "            score = self.get_model_score(model)\n",
        "            return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKJxwrW4n-XW"
      },
      "outputs": [],
      "source": [
        "mlc_obj = ML(\n",
        "    df              = df,\n",
        "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "    n_jobs          = 5,\n",
        "\n",
        "    data_folder     = data_folder,\n",
        "    results_file    = \"optim_reults_mlc.csv\",\n",
        "\n",
        "    model_metric    =  \"c\", # r = regression, c = classification\n",
        "    window          = 30,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWUruphbn-XW"
      },
      "outputs": [],
      "source": [
        "if run_optim is True:\n",
        "    mlc_obj.run_optim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCehJdx9n-XW"
      },
      "source": [
        "## 5.2 Model evalution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk548TSsn-XW"
      },
      "outputs": [],
      "source": [
        "df_results = mlc_obj.get_results()\n",
        "df_results.sort_values(by = \"valid_accuracy_t1\", ascending = False, inplace = True)\n",
        "\n",
        "df_results.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW9yjZTVn-XW"
      },
      "outputs": [],
      "source": [
        "df_results.sort_values(by = \"valid_accuracy_t2\", ascending = False, inplace = True)\n",
        "df_results.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8mpaTjqn-XX"
      },
      "outputs": [],
      "source": [
        "df_results.sort_values(by = \"valid_accuracy\", ascending = False, inplace = True)\n",
        "df_results.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYhWTO0fn-XY"
      },
      "outputs": [],
      "source": [
        "for col in [\"valid_accuracy\", \"valid_accuracy_t1\", \"valid_accuracy_t2\"]:\n",
        "\n",
        "    fig = px.scatter(\n",
        "        data_frame = df_results,\n",
        "        x = \"n_neurons\",\n",
        "        y = col,\n",
        "        color = \"mean_neurosn_per_layer\",\n",
        "        #size = \"n_layers\",\n",
        "        color_continuous_scale = plt_style_c,\n",
        "\n",
        "        title = \"Fitting graph: Multi-layer perceptron classifier\",\n",
        "        opacity = 1,\n",
        "        trendline = \"lowess\",\n",
        "\n",
        "        labels = {\"mean_neurosn_per_layer\": \"mean neurons\\nper layer\"}\n",
        "    )\n",
        "\n",
        "    #fig.update_traces(marker=dict(line=dict(color='rgba(0, 0, 0, 0)')))\n",
        "\n",
        "    scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvXs02aAn-XY"
      },
      "outputs": [],
      "source": [
        "class Networkplotter():\n",
        "\n",
        "    def plot_ann(arch = [10,3,2,1], title = \"ANN architecture\"):\n",
        "\n",
        "        structure = Networkplotter.create_neurons(arch) #df\n",
        "        connections = Networkplotter.create_connections(structure, arch) #dict\n",
        "        Networkplotter.draw_network(structure, connections, arch, title)\n",
        "\n",
        "    def create_neurons(arch):\n",
        "\n",
        "        structure = {\n",
        "            \"layer_pos\"  : [],\n",
        "            \"neuron_pos\" : [],\n",
        "        }\n",
        "\n",
        "        max_neurons = max(arch)\n",
        "        mid_pos = max_neurons / 2\n",
        "\n",
        "        for i in range(len(arch)):\n",
        "\n",
        "            neuron_pos = mid_pos - (arch[i] / 2)\n",
        "\n",
        "            for neuron in range(arch[i]):\n",
        "\n",
        "                structure[\"layer_pos\"].append(i),\n",
        "                structure[\"neuron_pos\"].append(neuron_pos)\n",
        "                neuron_pos += 1\n",
        "\n",
        "        return pd.DataFrame(structure)\n",
        "\n",
        "    def create_connections(structure, arch):\n",
        "\n",
        "        connections = {\n",
        "            \"x\" :   [], #(x1,x2), (x1,x2), layer_pos\n",
        "            \"y\" :   [], #(y1,y2), (y1,y2), neuron_pos\n",
        "        }\n",
        "\n",
        "        relevant_layers = list(range(len(arch)))[:-1]\n",
        "        relevant_neurons = structure.loc[structure[\"layer_pos\"].isin(relevant_layers)]\n",
        "\n",
        "        for i in range(relevant_neurons.shape[0]):\n",
        "\n",
        "            x1 = structure.iloc[i][\"layer_pos\"]\n",
        "            y1 = structure.iloc[i][\"neuron_pos\"]\n",
        "            x2 = x1 + 1\n",
        "\n",
        "            for j in structure.loc[structure[\"layer_pos\"] == x2].index.tolist():\n",
        "                y2 = float(structure.iloc[j][\"neuron_pos\"])\n",
        "\n",
        "                connections[\"x\"].append((x1,x2))\n",
        "                connections[\"y\"].append((y1,y2))\n",
        "\n",
        "        return connections\n",
        "\n",
        "    def draw_network(structure, connections, arch, title):\n",
        "\n",
        "        width   = len(arch) * 150\n",
        "        height  = 700\n",
        "        structure[\"size\"] = 1\n",
        "\n",
        "        fig_base = px.scatter(\n",
        "            data_frame = structure,\n",
        "            x = \"layer_pos\",\n",
        "            y = \"neuron_pos\",\n",
        "            size_max = 10,\n",
        "            size = \"size\",\n",
        "\n",
        "            title = title,\n",
        "\n",
        "            width = width,\n",
        "            height = height,\n",
        "            #color = \"neuron_pos\",\n",
        "            labels = {\"layer_pos\" : \"layer\", \"neuron_pos\" : \"\",}\n",
        "        )\n",
        "\n",
        "        data = fig_base.data\n",
        "        for i in range(len(list(connections[\"x\"]))):\n",
        "\n",
        "            fig_base.add_shape(\n",
        "                type='line',\n",
        "                x0 = connections[\"x\"][i][0], y0 = connections[\"y\"][i][0],\n",
        "                x1 = connections[\"x\"][i][1], y1 = connections[\"y\"][i][1],\n",
        "                line=dict(color=\"lightgrey\", width=2),\n",
        "                layer = \"below\",\n",
        "            )\n",
        "\n",
        "        tick_text = list(range(len(arch)))\n",
        "        tick_text[0] = \"Input layer\"\n",
        "        tick_text[-1] = \"Output layer\"\n",
        "\n",
        "        fig_base.update_layout(\n",
        "            xaxis = dict(\n",
        "                tickmode = 'array',\n",
        "                tickvals = list(range(len(arch))),\n",
        "                ticktext = tick_text,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        #unlcean code\n",
        "        fig_base.update_yaxes(showticklabels=False)\n",
        "        fig_base.update_layout(\n",
        "            xaxis=dict(showgrid=False),\n",
        "            yaxis=dict(showgrid=False)\n",
        "        )\n",
        "        fig_base.update_layout({\n",
        "            \"plot_bgcolor\": \"rgba(255, 255, 255, 255)\",\n",
        "            \"paper_bgcolor\": \"rgba(255, 255, 255, 255)\",\n",
        "            })\n",
        "\n",
        "        fig_base.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze-t1gDDn-XY"
      },
      "outputs": [],
      "source": [
        "ann_arch = [len(mlc_obj.df_train_x.columns.tolist()), 12, 12, 12, 330, 4]\n",
        "ann_arch = [int(item / 4) for item in ann_arch]\n",
        "\n",
        "print(ann_arch)\n",
        "\n",
        "\n",
        "#Networkplotter.plot_ann(\n",
        "#    arch = ann_arch,\n",
        "#    title = \"ANN architecture: MLP classifier\",\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVoSc7GNn-XY"
      },
      "outputs": [],
      "source": [
        "mlc_obj.create_model(arch = [12, 12, 12, 330])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0rMD2w9n-XY"
      },
      "outputs": [],
      "source": [
        "optimal_param = pd.DataFrame(mlc_obj.default_param, index = [\"optimal paraeters\"])\n",
        "optimal_param[\"hidden layer size\"] = str([12, 12, 12, 330])\n",
        "optimal_param.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc7Osa45n-XY"
      },
      "outputs": [],
      "source": [
        "scores = mlc_obj.get_model_score()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxElxiSon-XY"
      },
      "outputs": [],
      "source": [
        "mlc_obj.plot_confusion_mat(set = \"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x97afjc8n-XY"
      },
      "outputs": [],
      "source": [
        "mlc_obj.get_model_score(get_test_score = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQjBP92_n-XY"
      },
      "outputs": [],
      "source": [
        "mlc_obj.plot_confusion_mat(set = \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSDh28iXn-XZ"
      },
      "outputs": [],
      "source": [
        "#further improvements\n",
        "\n",
        "#overwrite defualt params\n",
        "\n",
        "for alpha in [1,0.5,0.25, 0.1, 0.01, 0.001]:\n",
        "    print(f\"\\nAlpha: {alpha}\")\n",
        "    mlc_obj.default_param = {\n",
        "        \"activation\"        : \"relu\",\n",
        "        \"solver\"            : \"adam\",\n",
        "        \"alpha\"             : alpha,         #defualt:   0.1\n",
        "        \"learning_rate\"     : \"adaptive\",\n",
        "        \"shuffle\"           : False,\n",
        "        \"early_stopping\"    : True,\n",
        "        \"max_iter\"          : 200,\n",
        "    }\n",
        "\n",
        "    mlc_obj.create_model(arch = [12, 12, 12, 330])\n",
        "    mlc_obj.get_model_score()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-SPto1sn-XZ"
      },
      "source": [
        "## 5.3 Overfitting prevention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6u0OiN1n-XZ"
      },
      "source": [
        "## 5.3 Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPFEcx1-n-XZ"
      },
      "outputs": [],
      "source": [
        "mlr_obj = ML(\n",
        "    df              = df,\n",
        "    y_col           = [\"t2m_t1\", \"t2m_t2\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "    n_jobs          = 5,\n",
        "\n",
        "    data_folder     = data_folder,\n",
        "    results_file    = \"optim_reults_mlr.csv\",\n",
        "\n",
        "    model_metric    =  \"r\", # r = regression, c = classification\n",
        "    window          = 30,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pvJmXj9n-XZ"
      },
      "outputs": [],
      "source": [
        "if run_optim is True:\n",
        "    mlr_obj.run_optim(use_small_arch_list = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6qkGVzZn-Xa"
      },
      "outputs": [],
      "source": [
        "df_results = mlr_obj.get_results()\n",
        "df_results.sort_values(by = \"valid_rmse\", ascending = True).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEtfevpbn-Xb"
      },
      "outputs": [],
      "source": [
        "df_results.sort_values(by = \"valid_r^2\", ascending = False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHgO0Wxjn-Xb"
      },
      "outputs": [],
      "source": [
        "for arch in df_results[\"hidden_layer_sizes\"].tolist():\n",
        "    print(arch, end = \", \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUh9hsYsn-Xb"
      },
      "outputs": [],
      "source": [
        "mlr_obj.create_model(arch = [496, 248, 124, 62])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2SHRQISn-Xb"
      },
      "outputs": [],
      "source": [
        "mlr_obj.get_model_score(get_test_score = True, unstandardize_score = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gprFu9dn-Xb"
      },
      "outputs": [],
      "source": [
        "mlr_obj.get_model_score(get_test_score = True, unstandardize_score = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0doWxWl8n-Xb"
      },
      "outputs": [],
      "source": [
        "draw_nn = False\n",
        "\n",
        "if draw_nn is True:\n",
        "    ann_arch = [496 * 2, 496, 248, 124, 62, 2]\n",
        "    ann_arch = [int(item / 25) if int(item / 25) > 0 else 1 for item in ann_arch]\n",
        "\n",
        "    print(ann_arch)\n",
        "\n",
        "\n",
        "    Networkplotter.plot_ann(\n",
        "        arch = ann_arch,\n",
        "        title = \"ANN architecture: MLP regressor\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzmFWsrHn-Xc"
      },
      "source": [
        "# 6. Recurrent neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQtOL1dWn-Xc"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\"\n",
        "\n",
        "#machine learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K\n",
        "\n",
        "#model scoring\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9gruNnOn-Xc"
      },
      "outputs": [],
      "source": [
        "run_optim = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBHmdewan-Xc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df_main.csv\"))\n",
        "df.head().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECPxsnmin-Xc"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    fig.update_layout(\n",
        "        width=1500,\n",
        "        height=750,\n",
        "    )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnzSXzvsn-Xd"
      },
      "outputs": [],
      "source": [
        "class Base(): #parent\n",
        "\n",
        "    def __init__(self, df : object, y_col : list, data_folder : str, results_file : str, model_metric : str, n_jobs : int = 1, window : int = 30):\n",
        "\n",
        "        #save raw df\n",
        "        self.df_raw         = df.copy()\n",
        "\n",
        "        #drop forbidden cols\n",
        "        df = df.copy() #pass by value\n",
        "        df = Base.__drop_forbidden_cols(df, y_col)\n",
        "\n",
        "        #set dataframe for refferencing\n",
        "        self.df             = df.copy() #windowed df, copy because obj is passed by refference\n",
        "\n",
        "        #get and get x_col and y_col\n",
        "        self.y_col          = y_col\n",
        "        self.x_col          = list(df.drop(labels = y_col, axis = 1, inplace = False).columns.to_list())\n",
        "\n",
        "        #misc params\n",
        "        self.random_state   = 42\n",
        "        self.n_jobs         = n_jobs\n",
        "        self.data_folder    = data_folder\n",
        "        self.results_file   = os.path.join(data_folder,results_file)\n",
        "        self.model_metric   = model_metric\n",
        "\n",
        "        #ann parameters\n",
        "        self.acitvation_func        = \"selu\"\n",
        "        self.solver                 = tf.keras.optimizers.legacy.SGD(learning_rate=0.0001) #tf.keras.optimizers.SGD(learning_rate=0.0001) #tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.initializer            = tf.keras.initializers.HeNormal(seed = self.random_state)\n",
        "        self.n_epochs               = 5\n",
        "\n",
        "        #windowing parameters\n",
        "        self.x_window       = window #number of shifting window input features\n",
        "\n",
        "        self.__setup()\n",
        "\n",
        "        return\n",
        "\n",
        "    def __setup(self):\n",
        "\n",
        "        #order was chosen to minimize data loss, at the cost of more needed processing power\n",
        "\n",
        "        #data preparation\n",
        "        self.__windowing()\n",
        "        self.__split_data()\n",
        "        self.__standardize_data()\n",
        "\n",
        "        #setup of metrics and results\n",
        "        self.__set_assesment()\n",
        "\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def __drop_forbidden_cols(df, y_col):\n",
        "\n",
        "        forbidden_cols = ['date','t2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', 't2m_t1_cat', 't2m_t2_cat']\n",
        "\n",
        "        #prevent y_cols from being dropped from the data frame\n",
        "        for y in y_col:\n",
        "            if y in forbidden_cols:\n",
        "                forbidden_cols.remove(y)\n",
        "\n",
        "        #drop forbidden cols, to prevent adding future information to the time series\n",
        "        print(f\"Removed forbidden cols:\\n{forbidden_cols}\")\n",
        "        df.drop(labels = forbidden_cols, axis = 1, inplace = True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def __windowing(self):\n",
        "        \"\"\"creates the windowed data frame\"\"\"\n",
        "\n",
        "        self.x_col_windowed = self.x_col.copy() #copy, becaus lists are past by refference\n",
        "\n",
        "        #input fetures: x\n",
        "        for i in range(1, self.x_window + 1):\n",
        "            for x_col in self.x_col: #inefficient but works just fine\n",
        "\n",
        "                x_col_i             = f\"{x_col}_-{i}\"\n",
        "                self.df[x_col_i]    = df[x_col].shift(i)\n",
        "\n",
        "                self.x_col_windowed.append(x_col_i)\n",
        "\n",
        "        #clean na columns, which were caused by the shifts\n",
        "        self.df.dropna(inplace = True)\n",
        "        print(f\"\\nApplying shifitng window:\\nx_window: -{self.x_window}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __split_data(self):\n",
        "\n",
        "        #reset index for splitting data\n",
        "        self.df.reset_index(inplace = True, drop = True)\n",
        "        length = self.df.shape[0]\n",
        "\n",
        "        #setting split value fractions\n",
        "        valid_frac_0      = 0.1\n",
        "        test_frac_1       = 0.05\n",
        "        train_frac_2      = 0.7\n",
        "        valid_frac_3      = 0.1\n",
        "        test_frac_4       = 0.05\n",
        "\n",
        "        #get end indexes\n",
        "        index_end_list = []\n",
        "        cum_frac = 0\n",
        "\n",
        "        for frac in [valid_frac_0, test_frac_1, train_frac_2, valid_frac_3, test_frac_4]:\n",
        "            cum_frac += frac\n",
        "            index_end_list.append(round(length * cum_frac))\n",
        "\n",
        "        #get indexes (ugly code)\n",
        "        df_indexes = self.df.index.tolist()\n",
        "        train_i     = df_indexes[index_end_list[1] : index_end_list[2]]\n",
        "        valid_i     = df_indexes[ : index_end_list[0]]                      + df_indexes[index_end_list[2] : index_end_list[3]]\n",
        "        test_i      = df_indexes[index_end_list[0] : index_end_list[1]]     + df_indexes[index_end_list[3] : index_end_list[4]]\n",
        "\n",
        "        #get df from indexes\n",
        "        self.df_train_x = self.df[self.x_col_windowed].loc[self.df.index.isin(train_i)]\n",
        "        self.df_train_y = self.df[self.y_col].loc[self.df.index.isin(train_i)]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_valid_x = self.df[self.x_col_windowed].loc[self.df.index.isin(valid_i)]\n",
        "        self.df_valid_y = self.df[self.y_col].loc[self.df.index.isin(valid_i)]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_test_x = self.df[self.x_col_windowed].loc[self.df.index.isin(test_i)]\n",
        "        self.df_test_y = self.df[self.y_col].loc[self.df.index.isin(test_i)]\n",
        "\n",
        "        #check\n",
        "        print(\"\\nSplitting data:\")\n",
        "        for df, df_type in zip (\n",
        "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
        "            \"train,valid,test\".split(\",\")\n",
        "            ):\n",
        "\n",
        "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
        "\n",
        "        #set data for plotting in raw df\n",
        "        self.df_raw[\"set\"] = None\n",
        "        for index_items, set_type in zip([train_i, valid_i, test_i],[\"train\", \"valid\", \"test\"]):\n",
        "            self.df_raw.loc[self.df_raw.index.isin(index_items), \"set\"] = set_type\n",
        "\n",
        "        return\n",
        "\n",
        "    def plot_set_distribution(self, plotter, style, plt_style):\n",
        "\n",
        "        if (style == \"histogram\") and (self.model_metric == \"c\"): #only plotlable with classificaiton model\n",
        "\n",
        "            fig = px.histogram(\n",
        "                data_frame = self.df_raw,\n",
        "                x = \"set\",\n",
        "                color = \"t2m_t2_cat\",\n",
        "                histfunc = \"count\",\n",
        "\n",
        "                barmode = \"group\",\n",
        "                title = \"Categorical distribution of sets\",\n",
        "                color_discrete_sequence = plt_style,\n",
        "            )\n",
        "\n",
        "        elif (style == \"scatter\"):\n",
        "\n",
        "            fig = px.scatter(\n",
        "                data_frame = self.df_raw,\n",
        "                x = \"date\",\n",
        "                y = \"t2m\",\n",
        "                color = \"set\",\n",
        "\n",
        "                title = \"Trend distribution of sets\",\n",
        "                color_discrete_sequence = plt_style,\n",
        "            )\n",
        "\n",
        "        plotter(fig)\n",
        "\n",
        "    def __split_data_deprecated(self):\n",
        "\n",
        "        #df length\n",
        "        length = self.df.shape[0]\n",
        "\n",
        "        #setting split values\n",
        "        valid_frac     = 0.2\n",
        "        test_frac      = 0.1\n",
        "\n",
        "        #get indexes\n",
        "        train_end       = round(length * (1 - (valid_frac + test_frac)))\n",
        "        valid_end       = round(length * (1 - (test_frac)))\n",
        "        test_end        = round(length * (1))\n",
        "\n",
        "        #create train df\n",
        "        self.df_train_x = self.df[self.x_col_windowed].iloc[:train_end]\n",
        "        self.df_train_y = self.df[self.y_col].iloc[:train_end]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_valid_x = self.df[self.x_col_windowed].iloc[train_end:valid_end]\n",
        "        self.df_valid_y = self.df[self.y_col].iloc[train_end:valid_end]\n",
        "\n",
        "        #create valid df\n",
        "        self.df_test_x = self.df[self.x_col_windowed].iloc[valid_end:test_end]\n",
        "        self.df_test_y = self.df[self.y_col].iloc[valid_end:test_end]\n",
        "\n",
        "        #check\n",
        "        print(\"\\nSplitting data:\")\n",
        "        for df, df_type in zip (\n",
        "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
        "            \"train,valid,test\".split(\",\")\n",
        "            ):\n",
        "\n",
        "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __standardize_data(self):\n",
        "\n",
        "        label_cat = [0,1]; label_cat.sort()\n",
        "        self.standardizing_values = {\n",
        "            \"x\" : {},\n",
        "            \"y\" : {},\n",
        "            #    col1 : {\"mean\" : value, \"std\"  : value},\n",
        "            #    col2 : {\"mean\" : value, \"std\"  : value},\n",
        "            #}\n",
        "            #\"y\" : ...\n",
        "        }\n",
        "\n",
        "        print(\"\\nStandardizing values:\")\n",
        "        for col in self.df.columns:\n",
        "\n",
        "            distinct_values = list(self.df[col].unique())\n",
        "            distinct_values.sort()\n",
        "\n",
        "            if label_cat == distinct_values: #skip categorical values\n",
        "                continue\n",
        "\n",
        "            #get mean and std for all columns across both data both data frames\n",
        "            if col in self.x_col_windowed:\n",
        "\n",
        "                self.standardizing_values[\"x\"][col]             = {}\n",
        "                self.standardizing_values[\"x\"][col][\"mean\"]     = self.df_train_x[col].mean()\n",
        "                self.standardizing_values[\"x\"][col][\"std\"]      = self.df_train_x[col].std()\n",
        "\n",
        "            elif col in self.y_col:\n",
        "\n",
        "                self.standardizing_values[\"y\"][col]             = {}\n",
        "                self.standardizing_values[\"y\"][col][\"mean\"]     = self.df_train_y[col].mean()\n",
        "                self.standardizing_values[\"y\"][col][\"std\"]      = self.df_train_y[col].std()\n",
        "\n",
        "        #apply values\n",
        "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
        "            for col in self.standardizing_values[col_type].keys():\n",
        "\n",
        "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
        "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
        "                df[col] = (df[col] - mean) / std #standardization\n",
        "\n",
        "        #check sum\n",
        "        print(f\"Checksum train x: {self.df_train_x[list(self.standardizing_values['x'].keys())].mean().round(2).sum()}\")\n",
        "        print(f\"Checksum train y: {self.df_train_y[list(self.standardizing_values['y'].keys())].mean().round(2).sum()}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def unstandardize_data(self):\n",
        "\n",
        "        #apply values\n",
        "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
        "            for col in self.standardizing_values[col_type].keys():\n",
        "\n",
        "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
        "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
        "                df[col] = df[col] * std + mean #reversed standardization\n",
        "\n",
        "        return\n",
        "\n",
        "    def __set_assesment(self):\n",
        "\n",
        "        #used as a custom scoring metric for regression models\n",
        "        def r_squared(y_true, y_pred):\n",
        "            ss_res = K.sum(K.square(y_true - y_pred))\n",
        "            ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "            return 1 - ss_res / (ss_tot + K.epsilon())\n",
        "\n",
        "        #set loss functions and metrics based on model type\n",
        "\n",
        "        if self.model_metric == \"c\":\n",
        "            self.get_model_score = self.__get_model_score_c\n",
        "            self.kears_metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
        "            self.loss_func = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "        elif self.model_metric == \"r\":\n",
        "            self.get_model_score = self.__get_model_score_r\n",
        "            self.kears_metrics = [tf.keras.metrics.MeanSquaredError(), r_squared]\n",
        "            self.loss_func = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"only allowed model metrics are: r, c\")\n",
        "\n",
        "    def __get_model_score_c(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
        "\n",
        "        #default\n",
        "        get_conf_mat = False\n",
        "        mat_labels = [0,1]\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        if model is None:\n",
        "            model = self.model\n",
        "            get_conf_mat = True #only get confuciton matrix when a single model is created\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        #y_train_pred    = model.evaluate(self.df_train_x, self.df_train_y)\n",
        "        #y_valid_pred    = model.evaluate(self.df_valid_x, self.df_valid_y)\n",
        "        #y_test_pred     = model.evaluate(self.df_test_x, self.df_test_y)\n",
        "\n",
        "        y_train_pred    = (model.predict(self.df_train_x) > 0.5).astype(int)\n",
        "        y_valid_pred    = (model.predict(self.df_valid_x) > 0.5).astype(int)\n",
        "        y_test_pred     = (model.predict(self.df_test_x) > 0.5).astype(int)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #get acc\n",
        "            score[f\"{raw_key}_accuracy_t1\"]     = round(accuracy_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_accuracy_t2\"]     = round(accuracy_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_accuracy\"]        = round(accuracy_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get conf mat\n",
        "            if get_conf_mat is True:\n",
        "                score[f\"{raw_key}_mat_t1\"]     = confusion_matrix(y_true = y_t1, y_pred = y_pred_t1, labels = mat_labels)\n",
        "                score[f\"{raw_key}_mat_t2\"]     = confusion_matrix(y_true = y_t2, y_pred = y_pred_t2, labels = mat_labels)\n",
        "                #score[f\"{raw_key}_mat\"]        = confusion_matrix(y_true = y, y_pred = y_pred) #multi labels are not supported\n",
        "\n",
        "        #return metrics\n",
        "        if get_conf_mat is True:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def __get_model_score_r(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
        "        \"\"\"not yet fitted to keras\"\"\"\n",
        "\n",
        "        set_score = False\n",
        "\n",
        "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
        "        if model is None:\n",
        "            model = self.model\n",
        "            set_score = True\n",
        "        if model is None:\n",
        "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
        "            return\n",
        "\n",
        "        score = {}\n",
        "\n",
        "        #create predictions\n",
        "        y_train_pred    = model.predict(self.df_train_x)\n",
        "        y_valid_pred    = model.predict(self.df_valid_x)\n",
        "        y_test_pred     = model.predict(self.df_test_x)\n",
        "\n",
        "        #seperate t1 and t2 for individual scoring\n",
        "        for raw_key, y_pred, y in zip(\n",
        "            [\"train\",           \"valid\",            \"test\"],\n",
        "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
        "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
        "        ):\n",
        "\n",
        "            #not fetting test accurarcy if not set\n",
        "            if (get_test_score == False) and raw_key == \"test\":\n",
        "                continue\n",
        "\n",
        "            #split\n",
        "            y_pred_t1 = y_pred[:,0]\n",
        "            y_pred_t2 = y_pred[:,1]\n",
        "\n",
        "            y_t1 = y[self.y_col[0]]\n",
        "            y_t2 = y[self.y_col[1]]\n",
        "\n",
        "            #get r^2\n",
        "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
        "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
        "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
        "\n",
        "            #get rmse\n",
        "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
        "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
        "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
        "\n",
        "        #return metrics\n",
        "        if set_score:\n",
        "            self.score = score\n",
        "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
        "            return\n",
        "\n",
        "        return score\n",
        "\n",
        "    def plot_confusion_mat(self, set = \"valid\"):\n",
        "        \"\"\"set = 'train', 'valid', 'test'\"\"\"\n",
        "\n",
        "        mat_keys = [key for key in self.score.keys() if (\"mat\" in key) and (set in key)]\n",
        "\n",
        "        for mat_key in mat_keys:\n",
        "\n",
        "            mat = self.score[mat_key]\n",
        "            title = str(mat_key).replace(\"_mat_\", \" \")\n",
        "\n",
        "            fig  = px.imshow(\n",
        "                mat,\n",
        "                color_continuous_scale = px.colors.sequential.haline_r,\n",
        "                text_auto = True,\n",
        "            )\n",
        "\n",
        "            #labels and layout\n",
        "            fig.update_layout(\n",
        "\n",
        "                title = f\"Confusion matrix: {title}\",\n",
        "\n",
        "                width=500,\n",
        "                height=500,\n",
        "\n",
        "                xaxis_title=\"Predicted label\",\n",
        "                yaxis_title=\"True label\",\n",
        "\n",
        "                xaxis = dict(\n",
        "                    tickmode = 'array',\n",
        "                    tickvals = [0,1],\n",
        "                    ticktext = [\"above\", \"below\"]\n",
        "                ),\n",
        "\n",
        "                yaxis = dict(\n",
        "                    tickmode = 'array',\n",
        "                    tickvals = [0,1],\n",
        "                    ticktext = [\"above\", \"below\"],\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            #set font\n",
        "            fig.update_layout(\n",
        "                font = dict(size=16),\n",
        "                title_font = dict(size=20),\n",
        "                xaxis_title_font = dict(size=18),\n",
        "                yaxis_title_font = dict(size=18),\n",
        "            )\n",
        "\n",
        "            fig.show()\n",
        "\n",
        "    def save_result(self, param, score):\n",
        "\n",
        "        #merge and create a dataframe\n",
        "        param.update(score); data = param\n",
        "        df_result = pd.DataFrame([data])\n",
        "\n",
        "        #create results file and set header length as param to negate reading file\n",
        "        if os.path.isfile(self.results_file) is True:\n",
        "            df_saved_result = pd.read_csv(self.results_file)\n",
        "            df_result = df_saved_result.append(df_result)\n",
        "\n",
        "        df_result.to_csv(self.results_file, index = False)\n",
        "\n",
        "        return\n",
        "\n",
        "    def get_results(self):\n",
        "\n",
        "        df = pd.read_csv(self.results_file)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k6SxIgDn-Xd"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "      if(logs.get(\"accuracy\") < 0.001):\n",
        "          print(\"\\nMAEthreshold reached. Training stopped.\")\n",
        "          self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS-dc2czn-Xd"
      },
      "outputs": [],
      "source": [
        "class RN(Base):\n",
        "\n",
        "    def run_optim(self):\n",
        "\n",
        "        self.model = None #clear any models if there should be one\n",
        "\n",
        "        arch_list = self.__generate_arch_list(\n",
        "            arch_log               = 2,\n",
        "            lin_arch_scaling       = 2,\n",
        "            n_layers               = 5,\n",
        "        )\n",
        "\n",
        "        for arch in arch_list:\n",
        "            print( f\"Progress of optim:\\t{round((arch_list.index(arch) / len(arch_list)) * 100,1)}\",end = \"\\r\")\n",
        "\n",
        "            #refromat to save all comparison data\n",
        "            if arch[0] == arch[-1]:\n",
        "                shape = \"linear\"\n",
        "            elif arch[0] > arch[-1]:\n",
        "                shape = \"cone\"\n",
        "            elif arch[0] < arch[-1]:\n",
        "                shape = \"cone_r\"\n",
        "\n",
        "            param = {\n",
        "                \"hidden_layer_sizes\"        : str(arch),\n",
        "                \"n_layers\"                  : len(arch),\n",
        "                \"n_neurons\"                 : sum(arch),\n",
        "                \"shape\"                     : shape,\n",
        "            }\n",
        "\n",
        "            #create model scoring to evalute models\n",
        "            score = self.create_model(arch = arch, single_model = False)\n",
        "            self.save_result(param = param, score = score)\n",
        "\n",
        "        print(\"Optim successfull. Read results with self.get_results()\")\n",
        "        return\n",
        "\n",
        "    def __generate_arch_list(self, arch_log = 2, lin_arch_scaling = 2, n_layers = 5):\n",
        "\n",
        "        arch_list : list = []\n",
        "        n_features = len(self.df_train_x.columns.tolist())\n",
        "\n",
        "        #nodes_pow_2 = [3 ** (cone_arch_base_power + p) for p in range(1, n_layers + 1)][::-1]\n",
        "        nodes_log_2 = [int(n_features * (1 / arch_log ** i)) for i in range(1,n_layers + 1)]\n",
        "\n",
        "        for n_layer in range(1, n_layers + 1):\n",
        "\n",
        "            #linear\n",
        "            for size in  [n_features / (i * lin_arch_scaling) for i in range(1,4)]:\n",
        "                arch_lin = [int(size)] * n_layer\n",
        "                arch_list.append(arch_lin)\n",
        "\n",
        "            #cone\n",
        "            arch_cone = nodes_log_2[:n_layer]\n",
        "            arch_list.append(arch_cone)\n",
        "\n",
        "            #cone r\n",
        "            arch_cone_r = arch_cone[::-1]\n",
        "            arch_list.append(arch_cone_r)\n",
        "\n",
        "        return arch_list\n",
        "\n",
        "    def create_model(self, arch, single_model = True):\n",
        "\n",
        "        shape = [len(self.df_train_x.columns.tolist())]\n",
        "\n",
        "        #init model\n",
        "        model = tf.keras.models.Sequential()\n",
        "\n",
        "        #dynamicaly scaling input layer\n",
        "        model.add(tf.keras.layers.Lambda(\n",
        "            lambda x: tf.expand_dims(x, axis=-1),\n",
        "            input_shape=shape, #working: [None]\n",
        "        ))\n",
        "        \"\"\"\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(\n",
        "            units = len(self.df_train_x.columns.tolist()),\n",
        "            input_dim = 1,\n",
        "            activation = self.acitvation_func,\n",
        "        ))\n",
        "        \"\"\"\n",
        "\n",
        "        #add LSTM layers as hidden layers\n",
        "        for i in range(len(arch)):\n",
        "\n",
        "            if (i + 1) == len(arch):\n",
        "                return_sequences = False\n",
        "            else:\n",
        "                return_sequences = True\n",
        "\n",
        "            #hidden layers\n",
        "            model.add(tf.keras.layers.Bidirectional(\n",
        "                tf.keras.layers.LSTM(\n",
        "                    arch[i],\n",
        "                    #kernel_initializer = self.initializer,\n",
        "                    activation = self.acitvation_func,\n",
        "                    return_sequences = return_sequences,\n",
        "                )\n",
        "            ))\n",
        "\n",
        "        #add a Dense output layer\n",
        "        model.add(tf.keras.layers.Dense(units = 2, activation = self.acitvation_func)) #untis = 2, two binary classes\n",
        "\n",
        "        model = self.__compile_model(model)\n",
        "\n",
        "        #set according metrics\n",
        "        if single_model is True:\n",
        "            self.model = model\n",
        "            print(self.model)\n",
        "            return None\n",
        "\n",
        "        elif single_model is False:\n",
        "            score = self.get_model_score(model)\n",
        "            return score\n",
        "\n",
        "    def __compile_model(self, model):\n",
        "\n",
        "        model.compile(\n",
        "            loss = self.loss_func,\n",
        "            optimizer = self.solver,\n",
        "            metrics =  self.kears_metrics #accuracy\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            x = self.df_train_x,\n",
        "            y = self.df_train_y,\n",
        "            #validation_data = (self.df_valid_x, self.df_valid_y),\n",
        "            shuffle = False, #keep in order because it is time series data\n",
        "            epochs = self.n_epochs,\n",
        "            callbacks =[EarlyStopping()]\n",
        "        )\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pNz_0u0n-Xe"
      },
      "outputs": [],
      "source": [
        "rnc_obj = RN(\n",
        "    df              = df,\n",
        "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
        "\n",
        "    n_jobs          = 5, #not applicable for keras models\n",
        "\n",
        "    data_folder     = data_folder,\n",
        "    results_file    = \"optim_reults_rnc.csv\",\n",
        "\n",
        "    model_metric    =  \"c\", # r = regression, c = classification\n",
        "    window          = 30,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9ZbCvCyn-Xe"
      },
      "outputs": [],
      "source": [
        "if run_optim:\n",
        "    rnc_obj.run_optim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0R3Re_vn-Xe"
      },
      "outputs": [],
      "source": [
        "rnc_obj.get_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc6HfYNrn-Xe"
      },
      "outputs": [],
      "source": [
        "class Networkplotter():\n",
        "\n",
        "    def plot_ann(arch = [10,3,2,1], title = \"ANN architecture\"):\n",
        "\n",
        "        structure = Networkplotter.create_neurons(arch) #df\n",
        "        connections = Networkplotter.create_connections(structure, arch) #dict\n",
        "        Networkplotter.draw_network(structure, connections, arch, title)\n",
        "\n",
        "    def create_neurons(arch):\n",
        "\n",
        "        structure = {\n",
        "            \"layer_pos\"  : [],\n",
        "            \"neuron_pos\" : [],\n",
        "        }\n",
        "\n",
        "        max_neurons = max(arch)\n",
        "        mid_pos = max_neurons / 2\n",
        "\n",
        "        for i in range(len(arch)):\n",
        "\n",
        "            neuron_pos = mid_pos - (arch[i] / 2)\n",
        "\n",
        "            for neuron in range(arch[i]):\n",
        "\n",
        "                structure[\"layer_pos\"].append(i),\n",
        "                structure[\"neuron_pos\"].append(neuron_pos)\n",
        "                neuron_pos += 1\n",
        "\n",
        "        return pd.DataFrame(structure)\n",
        "\n",
        "    def create_connections(structure, arch):\n",
        "\n",
        "        connections = {\n",
        "            \"x\" :   [], #(x1,x2), (x1,x2), layer_pos\n",
        "            \"y\" :   [], #(y1,y2), (y1,y2), neuron_pos\n",
        "        }\n",
        "\n",
        "        relevant_layers = list(range(len(arch)))[:-1]\n",
        "        relevant_neurons = structure.loc[structure[\"layer_pos\"].isin(relevant_layers)]\n",
        "\n",
        "        for i in range(relevant_neurons.shape[0]):\n",
        "\n",
        "            x1 = structure.iloc[i][\"layer_pos\"]\n",
        "            y1 = structure.iloc[i][\"neuron_pos\"]\n",
        "            x2 = x1 + 1\n",
        "\n",
        "            for j in structure.loc[structure[\"layer_pos\"] == x2].index.tolist():\n",
        "                y2 = float(structure.iloc[j][\"neuron_pos\"])\n",
        "\n",
        "                connections[\"x\"].append((x1,x2))\n",
        "                connections[\"y\"].append((y1,y2))\n",
        "\n",
        "        return connections\n",
        "\n",
        "    def draw_network(structure, connections, arch, title):\n",
        "\n",
        "        width   = len(arch) * 150\n",
        "        height  = 700\n",
        "        structure[\"size\"] = 1\n",
        "\n",
        "        fig_base = px.scatter(\n",
        "            data_frame = structure,\n",
        "            x = \"layer_pos\",\n",
        "            y = \"neuron_pos\",\n",
        "            size_max = 10,\n",
        "            size = \"size\",\n",
        "\n",
        "            title = title,\n",
        "\n",
        "            width = width,\n",
        "            height = height,\n",
        "            #color = \"neuron_pos\",\n",
        "            labels = {\"layer_pos\" : \"layer\", \"neuron_pos\" : \"\",}\n",
        "        )\n",
        "\n",
        "        data = fig_base.data\n",
        "        for i in range(len(list(connections[\"x\"]))):\n",
        "\n",
        "            fig_base.add_shape(\n",
        "                type='line',\n",
        "                x0 = connections[\"x\"][i][0], y0 = connections[\"y\"][i][0],\n",
        "                x1 = connections[\"x\"][i][1], y1 = connections[\"y\"][i][1],\n",
        "                line=dict(color=\"lightgrey\", width=2),\n",
        "                layer = \"below\",\n",
        "            )\n",
        "\n",
        "        tick_text = list(range(len(arch)))\n",
        "        tick_text[0] = \"Input layer\"\n",
        "        tick_text[-1] = \"Output layer\"\n",
        "\n",
        "        fig_base.update_layout(\n",
        "            xaxis = dict(\n",
        "                tickmode = 'array',\n",
        "                tickvals = list(range(len(arch))),\n",
        "                ticktext = tick_text,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        #unlcean code\n",
        "        fig_base.update_yaxes(showticklabels=False)\n",
        "        fig_base.update_layout(\n",
        "            xaxis=dict(showgrid=False),\n",
        "            yaxis=dict(showgrid=False)\n",
        "        )\n",
        "        fig_base.update_layout({\n",
        "            \"plot_bgcolor\": \"rgba(255, 255, 255, 255)\",\n",
        "            \"paper_bgcolor\": \"rgba(255, 255, 255, 255)\",\n",
        "            })\n",
        "\n",
        "        fig_base.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWRWwi1Un-Xe"
      },
      "outputs": [],
      "source": [
        "ann_arch = [len(rnc_obj.df_train_x.columns.tolist()), 496, 496, 496, 4]\n",
        "ann_arch = [int(item / 30) if int(item / 30) > 0 else 1 for item in ann_arch]\n",
        "\n",
        "print(ann_arch)\n",
        "\n",
        "Networkplotter.plot_ann(\n",
        "    arch = ann_arch,\n",
        "    title = \"ANN architecture: RNN classifier\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPmis9oCn-Xf"
      },
      "source": [
        "# 7. Gas price and temperature correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z75zbYRHn-Xf"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\"\n",
        "\n",
        "#warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r7UdqIBn-Xf"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    fig.update_layout(\n",
        "        width=1500,\n",
        "        height=750,\n",
        "    )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p4-bd8Dn-Xf"
      },
      "source": [
        "## 7.1 Data understanding (gathering, cleaning)\n",
        "- source: https://www.eia.gov/dnav/ng/hist/rngwhhdD.htm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34upoM6Pn-Xg"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df_main.csv\"))\n",
        "df = df[[\"date\", \"t2m\", \"t2m_t1_cat\"]]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USuRbXann-Xg"
      },
      "outputs": [],
      "source": [
        "df_gas_raw = pd.read_csv(os.path.join(data_folder, \"raw_gas\", \"raw_download.csv\"))\n",
        "df_gas_raw.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdyrF4iBn-Xg"
      },
      "outputs": [],
      "source": [
        "df_gas = df_gas_raw\n",
        "df_gas.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWqNZijNn-Xh"
      },
      "outputs": [],
      "source": [
        "df_gas[\"gas_usd_spot\"].interpolate(inplace = True, method = \"linear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwz4Ix_Wn-Xi"
      },
      "outputs": [],
      "source": [
        "df_gas = df_gas_raw\n",
        "df_gas.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LEZgRZTn-Xi"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame= df_gas,\n",
        "    x = \"date\",\n",
        "    y = \"gas_usd_spot\",\n",
        "    title = \"Natural gas price, spot (USD per Million Btu)\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtP0JUM5n-Xj"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(right=df, left = df_gas, on = \"date\", how = \"inner\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p2yWy-Hn-Xj"
      },
      "source": [
        "## 7.2 Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVn0Dkwn-Xj"
      },
      "outputs": [],
      "source": [
        "#calculate percentual change\n",
        "df[\"t2m_change\"] = (((df[\"t2m\"] - df[\"t2m\"].shift(1)) / df[\"t2m\"].shift(1)) * 100).round(2)\n",
        "df[\"gas_usd_spot_change\"] = (((df[\"gas_usd_spot\"] - df[\"gas_usd_spot\"].shift(1)) / df[\"gas_usd_spot\"].shift(1)) * 100).round(2)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%Y-%m-%d\")\n",
        "\n",
        "df.dropna(inplace = True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBi8xZpJn-Xj"
      },
      "outputs": [],
      "source": [
        "#standardize values to make them compareable\n",
        "for col in [\"gas_usd_spot\", \"t2m\", \"t2m_change\", \"gas_usd_spot_change\"]:\n",
        "    df[col] = (df[col] - df[col].mean()) / df[col].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JOwPGaYn-Xk"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df,\n",
        "    x = \"t2m_change\",\n",
        "    y = \"gas_usd_spot_change\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    title = \"Gas price change and temperature change comparison\",\n",
        "    trendline = \"ols\",\n",
        "    trendline_color_override = \"red\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUWKIbqzn-Xk"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df,\n",
        "    x = \"t2m\",\n",
        "    y = \"gas_usd_spot\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    title = \"Gas price change and temperature change comparison\",\n",
        "    trendline = \"ols\",\n",
        "    trendline_color_override = \"red\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9Yrw4_Fn-Xk"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df.loc[df[\"date\"].dt.month.isin([10,11,12,1,2,3])],\n",
        "    x = \"t2m\",\n",
        "    y = \"gas_usd_spot\",\n",
        "    color_discrete_sequence= plt_style_s,\n",
        "    title = \"Gas price change and temperature change comparison\",\n",
        "    trendline = \"ols\",\n",
        "    trendline_color_override = \"red\",\n",
        "    facet_col = \"t2m_t1_cat\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-SM5NtOn-Xl"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df.loc[df[\"date\"].dt.month.isin([10,11,12,1,2,3])],\n",
        "    x = \"t2m_change\",\n",
        "    y = \"gas_usd_spot_change\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "    title = \"Gas price change and temperature change comparison\",\n",
        "    trendline = \"ols\",\n",
        "    trendline_color_override = \"red\",\n",
        "    facet_col = \"t2m_t1_cat\",\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6Isflg4n-Xl"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df.iloc[-200:],\n",
        "    x = \"date\",\n",
        "    y = [\"t2m_change\", \"gas_usd_spot_change\"],\n",
        "    title = \"Change over time\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgLtN17dn-Xl"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df.iloc[-40000:],\n",
        "    x = \"date\",\n",
        "    y = [\"t2m\", \"gas_usd_spot\"],\n",
        "    title = \"Stanardized values over time\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjvKoF-un-Xl"
      },
      "outputs": [],
      "source": [
        "#apply z score peaks\n",
        "#source: https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data/43512887#43512887\n",
        "\n",
        "def thresholding_algo(y, lag, threshold, influence):\n",
        "    \"\"\"Robust peak detection algorithm (using z-scores)\n",
        "\n",
        "    Args:\n",
        "        y (_type_): y_vector / time series\n",
        "        lag (_type_): the lag of the moving window\n",
        "        threshold (_type_): the z-score at which the algorithm signals\n",
        "        influence (_type_): the influence (between 0 and 1) of new signals on the mean and\n",
        "\n",
        "    Returns:\n",
        "        _type_: dict {\n",
        "            singals\n",
        "            avgFilter\n",
        "            stdFilter\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    signals = np.zeros(len(y))\n",
        "    filteredY = np.array(y)\n",
        "    avgFilter = [0]*len(y)\n",
        "    stdFilter = [0]*len(y)\n",
        "\n",
        "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
        "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
        "\n",
        "    for i in range(lag, len(y)):\n",
        "        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:\n",
        "            if y[i] > avgFilter[i-1]:\n",
        "                signals[i] = 1\n",
        "            else:\n",
        "                signals[i] = -1\n",
        "\n",
        "            filteredY[i] = influence * y[i] + (1 - influence) * filteredY[i-1]\n",
        "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
        "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
        "        else:\n",
        "            signals[i] = 0\n",
        "            filteredY[i] = y[i]\n",
        "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
        "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
        "\n",
        "    return dict(signals = np.asarray(signals),\n",
        "                avgFilter = np.asarray(avgFilter),\n",
        "                stdFilter = np.asarray(stdFilter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC1bFL6Xn-Xl"
      },
      "outputs": [],
      "source": [
        "#get score\n",
        "for col in [\"t2m\", \"gas_usd_spot\"]:\n",
        "\n",
        "\n",
        "    df[f\"{col}_zscore\"] = thresholding_algo(y = df[col], lag = 360, threshold = 2.5, influence = 0.1)[\"signals\"]\n",
        "\n",
        "    #get peak spots for plotting\n",
        "    df[f\"{col}_zscore\"] = (df[col] * df[f\"{col}_zscore\"]) * df[f\"{col}_zscore\"]\n",
        "\n",
        "    #set others to NaN to for plotting as scatter\n",
        "    df.loc[df[f\"{col}_zscore\"] == 0, f\"{col}_zscore\"] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4G1tcFXn-Xm"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df.iloc[-10000:],\n",
        "    x = \"date\",\n",
        "    y = \"t2m\",\n",
        "    title = \"Stanardized values over time\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "fig.add_scatter(\n",
        "    x = df[\"date\"],\n",
        "    y = df[\"t2m_zscore\"],\n",
        "    mode = \"markers\",\n",
        "    name = \"local z score peaks t2m\",\n",
        "    marker = {\"size\" : 10},\n",
        "\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd3B-IIhn-Xm"
      },
      "outputs": [],
      "source": [
        "def fixed_lower_th(y, th = -2.4):\n",
        "\n",
        "    signal = (y <= th).astype(int)\n",
        "    signal = signal * -1\n",
        "\n",
        "    return signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1pp1ctsn-Xm"
      },
      "outputs": [],
      "source": [
        "#get score\n",
        "for col in [\"t2m\", \"gas_usd_spot\"]:\n",
        "\n",
        "    df[f\"{col}_th\"] = fixed_lower_th(y = df[col], th = -2.4)\n",
        "\n",
        "    #get peak spots for plotting\n",
        "    df[f\"{col}_th\"] = (df[col] * df[f\"{col}_th\"]) * df[f\"{col}_th\"]\n",
        "\n",
        "    #set others to NaN to for plotting as scatter\n",
        "    df.loc[df[f\"{col}_th\"] == 0, f\"{col}_th\"] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgf7_LVXn-Xn"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df.iloc[-10000:],\n",
        "    x = \"date\",\n",
        "    y = \"t2m\",\n",
        "    title = \"Stanardized values over time\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "fig.add_scatter(\n",
        "    x = df[\"date\"],\n",
        "    y = df[\"t2m_th\"],\n",
        "    mode = \"markers\",\n",
        "    name = \"local z score peaks t2m\",\n",
        "    marker = {\"size\" : 10},\n",
        "\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpjfDgDWn-Xn"
      },
      "outputs": [],
      "source": [
        "df[\"t2m_zscore\"] = thresholding_algo(y = df[\"t2m\"], lag = 60, threshold = 3.5, influence = 0.5)[\"signals\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yfi3Uyen-Xn"
      },
      "outputs": [],
      "source": [
        "df.loc[df[\"t2m_th\"].isna() == False, \"t2m_th\"] = 10\n",
        "df.loc[df[\"t2m_th\"].isna() == True, \"t2m_th\"] = -10\n",
        "\n",
        "df[\"t2m_zscore\"] = df[\"t2m_zscore\"].abs() * 20 - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GE3J1-fn-Xn"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df,\n",
        "    x = \"date\",\n",
        "    y = [\"gas_usd_spot\", \"t2m_th\", \"t2m_zscore\"],\n",
        "    title = \"Gas price\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        ")\n",
        "\n",
        "fig.update_layout(yaxis_range=[-3.5, 10])\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-JF5wuXn-Xn"
      },
      "outputs": [],
      "source": [
        "fig = px.line(\n",
        "    data_frame = df,\n",
        "    x = \"date\",\n",
        "    y = [\"gas_usd_spot\", \"t2m_zscore\",],\n",
        "    title = \"Gas price\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        "\n",
        ")\n",
        "\n",
        "fig.update_layout(yaxis_range=[-3.5, 10])\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59XR6nwen-Xo"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    data_frame = df,\n",
        "    x = \"date\",\n",
        "    y = \"gas_usd_spot\",\n",
        "    color = \"t2m_t1_cat\",\n",
        "    title = \"Gas price\",\n",
        "    color_continuous_scale = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYG7ybS1n-Xo"
      },
      "source": [
        "# 8 PV image data clustering\n",
        "- process: https://towardsdatascience.com/a-step-by-step-guide-for-clustering-images-4b45f9906128\n",
        "- clustering methods: https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a\n",
        "- hog method for feature method: https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R-Fgczgn-Xo"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.io as pio\n",
        "\n",
        "#image clustering library\n",
        "from clustimage import Clustimage\n",
        "\n",
        "#machine learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#folders\n",
        "data_folder = \"data\"\n",
        "\n",
        "#warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15pLAMCxn-Xo"
      },
      "outputs": [],
      "source": [
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYrtAbzxn-Xo"
      },
      "outputs": [],
      "source": [
        "#set if data should be safed\n",
        "save_data = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DjtYiHvn-Xo"
      },
      "outputs": [],
      "source": [
        "#plot styles\n",
        "plt_style_c = px.colors.sequential.haline #complex\n",
        "plt_style_s = px.colors.diverging.Portland #simple\n",
        "\n",
        "#defualt plot size\n",
        "size = {\n",
        "    \"width\" : 1500 ,\n",
        "    \"height\" : 750 ,\n",
        "}\n",
        "\n",
        "#function for plotting\n",
        "def scale_show(fig, size_override = False):\n",
        "\n",
        "    #set font\n",
        "    fig.update_layout(\n",
        "        font = dict(size=16),\n",
        "        title_font = dict(size=20),\n",
        "        xaxis_title_font = dict(size=18),\n",
        "        yaxis_title_font = dict(size=18),\n",
        "    )\n",
        "\n",
        "    #set size\n",
        "    if size_override == False:\n",
        "        fig.update_layout(\n",
        "            width=1500,\n",
        "            height=750,\n",
        "        )\n",
        "\n",
        "    #show\n",
        "    fig.show()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0fivU40n-Xo"
      },
      "source": [
        "## 8.1 data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhAP4gMwn-Xo"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, \"df_pv_clustering.csv\"))\n",
        "#df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df.drop(labels = \"longitude\", axis = 1, inplace = True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZKIVrPxn-Xo"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by = [\"date\", \"level\", \"latitude\"], ascending = [True, True, True], inplace = True)\n",
        "df.reset_index(inplace = True, drop = True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZsmZWHjn-Xq"
      },
      "outputs": [],
      "source": [
        "n_dates     = df[\"date\"].unique().shape[0]\n",
        "n_lats      = df[\"latitude\"].unique().shape[0]\n",
        "n_levels    = df[\"level\"].unique().shape[0]\n",
        "\n",
        "print(n_dates)\n",
        "print(n_lats)\n",
        "print(n_levels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cIf-uyen-Xq"
      },
      "outputs": [],
      "source": [
        "#cost: high\n",
        "\n",
        "\"\"\"\n",
        "metric = \"speed\" #[\"speed\", \"t\"]\n",
        "images_na = {}\n",
        "\n",
        "for date in df[\"date\"].unique().tolist()[:3]:\n",
        "    im = []\n",
        "\n",
        "    for level in df[\"level\"].unique():\n",
        "        metrics = df.loc[(df[\"date\"] == date) & (df[\"level\"] == level)][\"speed\"].tolist()\n",
        "        im.append(metrics)\n",
        "\n",
        "    print(f\"Compiling image: {date}\", end = \"\\r\")\n",
        "    images_na[date] = im\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgvSwOz9n-Xq"
      },
      "outputs": [],
      "source": [
        "def image_compiler(df, metric):\n",
        "\n",
        "    n_lats      = df[\"latitude\"].unique().shape[0]\n",
        "    n_levels    = df[\"level\"].unique().shape[0]\n",
        "\n",
        "    pixel_row = []\n",
        "    image = []\n",
        "    images = []\n",
        "\n",
        "    #standardize pixels\n",
        "    pixels = (df[metric] - df[metric].mean()) / df[metric].std()\n",
        "    pixels = pixels.tolist()\n",
        "\n",
        "    for pixel in pixels:\n",
        "\n",
        "        pixel_row.append(pixel)\n",
        "\n",
        "        if len(pixel_row) == n_lats:\n",
        "            image.append(np.array(pixel_row.copy()))\n",
        "            pixel_row.clear()\n",
        "\n",
        "            if len(image) == n_levels:\n",
        "                images.append(np.array(image.copy()))\n",
        "                image.clear()\n",
        "\n",
        "    images = np.stack(images)\n",
        "\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE5qxQeXn-Xq"
      },
      "outputs": [],
      "source": [
        "images = image_compiler(df = df, metric = \"speed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaPW2Ks3n-Xq"
      },
      "outputs": [],
      "source": [
        "#checksum\n",
        "print(df.shape[0] / (n_lats * n_levels))\n",
        "print(len(images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSqWilK5n-Xq"
      },
      "outputs": [],
      "source": [
        "px.imshow(images[0], title = \"Wind speed standardized\", color_continuous_scale = plt_style_s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn4x4TcIn-Xr"
      },
      "outputs": [],
      "source": [
        "px.imshow(images[-1], title = \"Wind speed standardized\", color_continuous_scale = plt_style_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvogFmifn-Xr"
      },
      "outputs": [],
      "source": [
        "images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJyBT0bEn-Xr"
      },
      "outputs": [],
      "source": [
        "#split the data, no validiotn and test sset\n",
        "\n",
        "def splitter (images):\n",
        "\n",
        "    images = images.copy() #prevent altering the original list\n",
        "\n",
        "    train_split     = 0.8\n",
        "    test_split      = 0.2  #only used for idication\n",
        "\n",
        "    index_list = list(range(len(images)))\n",
        "    train_size = int(len(images) * train_split)\n",
        "\n",
        "    train_set_i     = random.sample(population = index_list, k = train_size)\n",
        "    test_set_i      = [index for index in index_list if index not in train_set_i]\n",
        "\n",
        "    train_set       = [images[i] for i in train_set_i]\n",
        "    test_set        = [images[i] for i in test_set_i]\n",
        "\n",
        "    train_set = np.stack(train_set)\n",
        "    test_set = np.stack(test_set)\n",
        "\n",
        "    print(f\"train set:\\t{round(len(train_set) / len(images), 2)}\\ntest set:\\t{round(len(test_set) / len(images),2)}\")\n",
        "\n",
        "    return train_set, test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgKJvPWvn-Xr"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = splitter(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu5karepn-Xr"
      },
      "outputs": [],
      "source": [
        "train_set[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr3gVyfRn-Xr"
      },
      "source": [
        "## 8.2 modelling (clustimage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIT7NjPtn-Xr"
      },
      "outputs": [],
      "source": [
        "cl = Clustimage(\n",
        "    method='hog',\n",
        "    embedding='tsne',\n",
        "    grayscale=False,\n",
        "    dim=(10,45),\n",
        "\n",
        "    params_hog = {\n",
        "        \"orientations\"      : 8,\n",
        "        \"pixels_per_cell\"   : (4,4),\n",
        "    },\n",
        "\n",
        "    verbose = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmzBUsDjn-Xr"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    results = cl.fit_transform(\n",
        "        train_set,\n",
        "        cluster='agglomerative',\n",
        "        evaluate='silhouette',\n",
        "        metric='euclidean',\n",
        "        linkage='ward',\n",
        "        min_clust=3,\n",
        "        max_clust=15,\n",
        "        cluster_space='high',\n",
        "    )\n",
        "except :\n",
        "    print(\"Not able to conver the datatypes of numpy arrays to image data. Twat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4xXB2Hwn-Xs"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    #create model\n",
        "    cl = Clustimage(method='hog')\n",
        "\n",
        "    #extract features\n",
        "    train_set_feat = cl.extract_feat(train_set)\n",
        "\n",
        "    # Embedding using tSNE\n",
        "    xycoord = cl.embedding(train_set_feat)\n",
        "\n",
        "    # Cluster with all default settings\n",
        "    labels = cl.cluster(\n",
        "        cluster='agglomerative',\n",
        "        evaluate='silhouette',\n",
        "        metric='euclidean',\n",
        "        linkage='ward',\n",
        "        min_clust=3,\n",
        "        max_clust=15,\n",
        "        cluster_space='high',\n",
        "    )\n",
        "\n",
        "    # Return\n",
        "    results = cl.results\n",
        "\n",
        "except:\n",
        "    print(\"Not able to conver the datatypes of numpy arrays to image data. Twat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc-pNyZCn-Xs"
      },
      "source": [
        "## 8.3 modelling (skealrn)\n",
        "- https://medium.com/@chengweizhang2012/how-to-do-unsupervised-clustering-with-keras-9e1284448437"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qczDmcHyn-Xs"
      },
      "outputs": [],
      "source": [
        "class KMC():\n",
        "\n",
        "    def __init__ (self, n_cluster, train_set, test_set):\n",
        "\n",
        "        #set unflattended valus\n",
        "        self.train_set      = train_set.copy()\n",
        "        self.test_set       = test_set.copy()\n",
        "\n",
        "        #model params\n",
        "        self.random_state   = 42\n",
        "        self.n_cluster      = n_cluster\n",
        "        self.algorithm      = \"full\"\n",
        "\n",
        "        #flatten values\n",
        "        self.train_set_flat     = self.reshape(self.train_set)\n",
        "        self.test_set_flat      = self.reshape(self.test_set)\n",
        "\n",
        "    def reshape(self, set):\n",
        "\n",
        "        n_samples, height, width = set.shape\n",
        "        images_flat = set.reshape((n_samples, height * width))\n",
        "\n",
        "        return images_flat\n",
        "\n",
        "    def create_model(self):\n",
        "\n",
        "        self.model = KMeans(\n",
        "            n_clusters      = self.n_cluster,\n",
        "            random_state    = self.random_state,\n",
        "            algorithm       = self.algorithm,\n",
        "        )\n",
        "\n",
        "        self.model.fit(self.train_set_flat)\n",
        "        self.__predict_match()\n",
        "\n",
        "        return\n",
        "\n",
        "    def __predict_match(self):\n",
        "\n",
        "        #match image and labels for analysis (train)\n",
        "        labels_train = self.model.labels_\n",
        "        self.labels = []\n",
        "\n",
        "        for i in range(len(labels_train)):\n",
        "\n",
        "            data = {\n",
        "                \"set\"   : \"train\",\n",
        "                \"label\" : labels_train[i],\n",
        "                \"im\"    : self.train_set[i],\n",
        "            }\n",
        "            self.labels.append(data)\n",
        "\n",
        "        #match image and labels for analysis (test)\n",
        "        labels_test = self.model.predict(self.test_set_flat)\n",
        "\n",
        "        for i in range(len(labels_test)):\n",
        "\n",
        "            data = {\n",
        "                \"set\"   : \"test\",\n",
        "                \"label\" : labels_test[i],\n",
        "                \"im\"    : self.test_set[i],\n",
        "            }\n",
        "            self.labels.append(data)\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGl4r05bn-Xs"
      },
      "outputs": [],
      "source": [
        "kmc = KMC(n_cluster = 9, train_set = train_set, test_set = test_set)\n",
        "kmc.create_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6MY_UQZn-Xt"
      },
      "outputs": [],
      "source": [
        "df_cluster = pd.DataFrame(kmc.labels)\n",
        "df_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4KZ0RMzn-Xt"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(\n",
        "    data_frame = df_cluster,\n",
        "    x = \"label\",\n",
        "    histfunc = \"count\",\n",
        "    histnorm = \"probability density\",\n",
        "    color = \"set\",\n",
        "    barmode = \"group\",\n",
        "\n",
        "    title = \"KMeans Clusters\",\n",
        "    color_discrete_sequence = plt_style_s,\n",
        ")\n",
        "\n",
        "scale_show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUq-uAOUn-Xt"
      },
      "outputs": [],
      "source": [
        "kmc.labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZMax4YJn-Xt"
      },
      "outputs": [],
      "source": [
        "#get a sample from each cluster to get an idea of the distribution\n",
        "label_indexes = {}\n",
        "sample_plots = 3\n",
        "\n",
        "for label in df_cluster[\"label\"].unique().tolist():\n",
        "    indexes = df_cluster.loc[df_cluster[\"label\"] == label].index.tolist()\n",
        "    label_indexes[label] = indexes\n",
        "\n",
        "\n",
        "#get first elemt of each cluster and plot image\n",
        "keys = list(label_indexes.keys())\n",
        "keys.sort()\n",
        "\n",
        "for label in keys:\n",
        "\n",
        "    for i in range(sample_plots):\n",
        "\n",
        "        im_ind_all = label_indexes[label]\n",
        "        im_ind = random.sample(population = im_ind_all, k = 1, )[0]\n",
        "\n",
        "        #retrvie data for plotting\n",
        "        im_data     = kmc.labels[im_ind][\"im\"]\n",
        "        im_label    = kmc.labels[im_ind][\"label\"]\n",
        "        im_set      = kmc.labels[im_ind][\"set\"]\n",
        "\n",
        "        #generate image plot\n",
        "        fig = px.imshow(\n",
        "            im_data,\n",
        "            title = f\"Wind speed (norm) - Label: {im_label} ({im_set})\",\n",
        "            color_continuous_scale = plt_style_s,\n",
        "            range_color = [-2,6],\n",
        "            width = 1500,\n",
        "            height = 500,\n",
        "            )\n",
        "\n",
        "        fig.update_xaxes(title_text=\"Latitude (offset by -44)\")\n",
        "        fig.update_yaxes(title_text=\"Pressure level [hPa]\")\n",
        "\n",
        "        #image_file = \"test.png\"\n",
        "        #pio.write_image(fig, image_file, engine=\"plotly.io\")\n",
        "\n",
        "        #fig.write_image(\"test.png\") #f\"Wind_speed_(norm)-Label:{im_label}({im_set}).png\"\n",
        "\n",
        "        scale_show(fig, size_override = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW4EUKoyn-Xt"
      },
      "outputs": [],
      "source": [
        "#get predction son the test set\n",
        "# Flatten the images\n",
        "n_samples, height, width = images.shape\n",
        "images_flat = images.reshape((n_samples, height * width))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f6bbfc4e4f578cb9f7c85fe350d2fab0be0faacc19ccc874c1f1be2572a1188f"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}