{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modeling - Multi layer preceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "\n",
    "#folders\n",
    "data_folder = \"data\"\n",
    "\n",
    "#machine learning\n",
    "from sklearn.neural_network import MLPClassifier    as mlc\n",
    "from sklearn.neural_network import MLPRegressor     as mlr\n",
    "\n",
    "#model scoring\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optim = False #runtime: 711 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot styles\n",
    "plt_style_c = px.colors.sequential.haline #complex\n",
    "plt_style_s = px.colors.diverging.Portland #simple\n",
    "\n",
    "#defualt plot size \n",
    "size = {\n",
    "    \"width\" : 1500 ,\n",
    "    \"height\" : 750 ,\n",
    "}\n",
    "\n",
    "#function for plotting\n",
    "def scale_show(fig):\n",
    "\n",
    "    #set font\n",
    "    fig.update_layout(\n",
    "        font = dict(size=16),\n",
    "        title_font = dict(size=20),\n",
    "        xaxis_title_font = dict(size=18),\n",
    "        yaxis_title_font = dict(size=18),\n",
    "    )\n",
    "\n",
    "    #set size\n",
    "    fig.update_layout(\n",
    "        width=1500,\n",
    "        height=750,\n",
    "    )\n",
    "\n",
    "    #show\n",
    "    fig.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_folder, \"df_main.csv\"))\n",
    "df.head().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Data preparation, Modeling, Architecutre tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(): #parent\n",
    "\n",
    "    def __init__(self, df : object, y_col : list, data_folder : str, results_file : str, model_metric : str, n_jobs : int = 1, window : int = 30):\n",
    "\n",
    "        #save raw df\n",
    "        self.df_raw         = df.copy()\n",
    "\n",
    "        #drop forbidden cols\n",
    "        df = df.copy() #pass by value\n",
    "        df = Base.__drop_forbidden_cols(df, y_col)\n",
    "\n",
    "        #set dataframe for refferencing\n",
    "        self.df             = df.copy() #windowed df, copy because obj is passed by refference\n",
    "\n",
    "        #get and get x_col and y_col\n",
    "        self.y_col          = y_col\n",
    "        self.x_col          = list(df.drop(labels = y_col, axis = 1, inplace = False).columns.to_list())\n",
    "\n",
    "        #misc params\n",
    "        self.random_state   = 42\n",
    "        self.n_jobs         = n_jobs\n",
    "        self.data_folder    = data_folder\n",
    "        self.results_file   = os.path.join(data_folder,results_file)\n",
    "        self.model_metric   = model_metric\n",
    "\n",
    "        #ann parameters\n",
    "        self.default_param = {\n",
    "            \"activation\"        : \"relu\",\n",
    "            \"solver\"            : \"adam\",       #stochastic gradiant descent\n",
    "            \"alpha\"             : 0.1,          #see: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html\n",
    "            \"learning_rate\"     : \"adaptive\",   #trying to improve performance\n",
    "            \"shuffle\"           : False,        #keep order because of time series\n",
    "            \"early_stopping\"    : True,         #trying to reduce processing time\n",
    "            \"max_iter\"          : 200,          #change this if the training is too slow\n",
    "        }\n",
    "\n",
    "        #windowing parameters\n",
    "        self.x_window       = window #number of shifting window input features\n",
    "\n",
    "        self.__setup()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __setup(self):\n",
    "\n",
    "        #order was chosen to minimize data loss, at the cost of more needed processing power\n",
    "\n",
    "        #data preparation\n",
    "        self.__windowing()\n",
    "        self.__split_data()\n",
    "        self.__standardize_data()\n",
    "\n",
    "        #setup of metrics and results\n",
    "        self.__set_assesment()\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def __drop_forbidden_cols(df, y_col):\n",
    "\n",
    "        forbidden_cols = ['date','t2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', 't2m_t1_cat', 't2m_t2_cat']\n",
    "\n",
    "        #prevent y_cols from being dropped from the data frame\n",
    "        for y in y_col:\n",
    "            if y in forbidden_cols:\n",
    "                forbidden_cols.remove(y)\n",
    "\n",
    "        #drop forbidden cols, to prevent adding future information to the time series\n",
    "        print(f\"Removed forbidden cols:\\n{forbidden_cols}\")\n",
    "        df.drop(labels = forbidden_cols, axis = 1, inplace = True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __windowing(self):\n",
    "        \"\"\"creates the windowed data frame\"\"\"\n",
    "\n",
    "        self.x_col_windowed = self.x_col.copy() #copy, becaus lists are past by refference\n",
    "\n",
    "        #input fetures: x\n",
    "        for i in range(1, self.x_window + 1):\n",
    "            for x_col in self.x_col: #inefficient but works just fine\n",
    "\n",
    "                x_col_i             = f\"{x_col}_-{i}\"\n",
    "                self.df[x_col_i]    = df[x_col].shift(i)\n",
    "\n",
    "                self.x_col_windowed.append(x_col_i)\n",
    "\n",
    "        #clean na columns, which were caused by the shifts\n",
    "        self.df.dropna(inplace = True)\n",
    "        print(f\"\\nApplying shifitng window:\\nx_window: -{self.x_window}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def __split_data(self):\n",
    "\n",
    "        #reset index for splitting data\n",
    "        self.df.reset_index(inplace = True, drop = True)\n",
    "        length = self.df.shape[0]\n",
    "\n",
    "        #setting split value fractions\n",
    "        valid_frac_0      = 0.1\n",
    "        test_frac_1       = 0.05\n",
    "        train_frac_2      = 0.7\n",
    "        valid_frac_3      = 0.1\n",
    "        test_frac_4       = 0.05\n",
    "\n",
    "        #get end indexes\n",
    "        index_end_list = []\n",
    "        cum_frac = 0\n",
    "\n",
    "        for frac in [valid_frac_0, test_frac_1, train_frac_2, valid_frac_3, test_frac_4]:\n",
    "            cum_frac += frac\n",
    "            index_end_list.append(round(length * cum_frac))\n",
    "\n",
    "        #get indexes (ugly code)\n",
    "        df_indexes = self.df.index.tolist()\n",
    "        train_i     = df_indexes[index_end_list[1] : index_end_list[2]]\n",
    "        valid_i     = df_indexes[ : index_end_list[0]]                      + df_indexes[index_end_list[2] : index_end_list[3]]\n",
    "        test_i      = df_indexes[index_end_list[0] : index_end_list[1]]     + df_indexes[index_end_list[3] : index_end_list[4]]\n",
    "\n",
    "        #get df from indexes\n",
    "        self.df_train_x = self.df[self.x_col_windowed].loc[self.df.index.isin(train_i)]\n",
    "        self.df_train_y = self.df[self.y_col].loc[self.df.index.isin(train_i)]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_valid_x = self.df[self.x_col_windowed].loc[self.df.index.isin(valid_i)]\n",
    "        self.df_valid_y = self.df[self.y_col].loc[self.df.index.isin(valid_i)]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_test_x = self.df[self.x_col_windowed].loc[self.df.index.isin(test_i)]\n",
    "        self.df_test_y = self.df[self.y_col].loc[self.df.index.isin(test_i)]\n",
    "\n",
    "        #check\n",
    "        print(\"\\nSplitting data:\")\n",
    "        for df, df_type in zip (\n",
    "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
    "            \"train,valid,test\".split(\",\")\n",
    "            ):\n",
    "\n",
    "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
    "\n",
    "        #set data for plotting in raw df\n",
    "        self.df_raw[\"set\"] = None\n",
    "        for index_items, set_type in zip([train_i, valid_i, test_i],[\"train\", \"valid\", \"test\"]):\n",
    "            self.df_raw.loc[self.df_raw.index.isin(index_items), \"set\"] = set_type\n",
    "\n",
    "        return\n",
    "\n",
    "    def plot_set_distribution(self, plotter, style, plt_style):\n",
    "\n",
    "        if (style == \"histogram\") and (self.model_metric == \"c\"): #only plotlable with classificaiton model\n",
    "\n",
    "            fig = px.histogram(\n",
    "                data_frame = self.df_raw,\n",
    "                x = \"set\",\n",
    "                color = \"t2m_t2_cat\",\n",
    "                histfunc = \"count\",\n",
    "\n",
    "                barmode = \"group\",\n",
    "                title = \"Categorical distribution of sets\",\n",
    "                color_discrete_sequence = plt_style,\n",
    "            )\n",
    "\n",
    "        elif (style == \"scatter\"):\n",
    "\n",
    "            fig = px.scatter(\n",
    "                data_frame = self.df_raw,\n",
    "                x = \"date\",\n",
    "                y = \"t2m\",\n",
    "                color = \"set\",\n",
    "\n",
    "                title = \"Trend distribution of sets\",\n",
    "                color_discrete_sequence = plt_style,\n",
    "            )\n",
    "\n",
    "        plotter(fig)\n",
    "\n",
    "    def __split_data_deprecated(self):\n",
    "\n",
    "        #df length\n",
    "        length = self.df.shape[0]\n",
    "\n",
    "        #setting split values\n",
    "        valid_frac     = 0.2\n",
    "        test_frac      = 0.1\n",
    "\n",
    "        #get indexes\n",
    "        train_end       = round(length * (1 - (valid_frac + test_frac)))\n",
    "        valid_end       = round(length * (1 - (test_frac)))\n",
    "        test_end        = round(length * (1))\n",
    "\n",
    "        #create train df\n",
    "        self.df_train_x = self.df[self.x_col_windowed].iloc[:train_end]\n",
    "        self.df_train_y = self.df[self.y_col].iloc[:train_end]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_valid_x = self.df[self.x_col_windowed].iloc[train_end:valid_end]\n",
    "        self.df_valid_y = self.df[self.y_col].iloc[train_end:valid_end]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_test_x = self.df[self.x_col_windowed].iloc[valid_end:test_end]\n",
    "        self.df_test_y = self.df[self.y_col].iloc[valid_end:test_end]\n",
    "\n",
    "        #check\n",
    "        print(\"\\nSplitting data:\")\n",
    "        for df, df_type in zip (\n",
    "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
    "            \"train,valid,test\".split(\",\")\n",
    "            ):\n",
    "\n",
    "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def __standardize_data(self):\n",
    "\n",
    "        label_cat = [0,1]; label_cat.sort()\n",
    "        self.standardizing_values = {\n",
    "            \"x\" : {},\n",
    "            \"y\" : {},\n",
    "            #    col1 : {\"mean\" : value, \"std\"  : value},\n",
    "            #    col2 : {\"mean\" : value, \"std\"  : value},\n",
    "            #}\n",
    "            #\"y\" : ...\n",
    "        }\n",
    "\n",
    "        print(\"\\nStandardizing values:\")\n",
    "        for col in self.df.columns:\n",
    "\n",
    "            distinct_values = list(self.df[col].unique())\n",
    "            distinct_values.sort()\n",
    "\n",
    "            if label_cat == distinct_values: #skip categorical values\n",
    "                continue\n",
    "\n",
    "            #get mean and std for all columns across both data both data frames\n",
    "            if col in self.x_col_windowed:\n",
    "\n",
    "                self.standardizing_values[\"x\"][col]             = {}\n",
    "                self.standardizing_values[\"x\"][col][\"mean\"]     = self.df_train_x[col].mean()\n",
    "                self.standardizing_values[\"x\"][col][\"std\"]      = self.df_train_x[col].std()\n",
    "\n",
    "            elif col in self.y_col:\n",
    "\n",
    "                self.standardizing_values[\"y\"][col]             = {}\n",
    "                self.standardizing_values[\"y\"][col][\"mean\"]     = self.df_train_y[col].mean()\n",
    "                self.standardizing_values[\"y\"][col][\"std\"]      = self.df_train_y[col].std()\n",
    "\n",
    "        #apply values\n",
    "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
    "            for col in self.standardizing_values[col_type].keys():\n",
    "\n",
    "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
    "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
    "                df[col] = (df[col] - mean) / std #standardization\n",
    "\n",
    "        #check sum\n",
    "        print(f\"Checksum train x: {self.df_train_x[list(self.standardizing_values['x'].keys())].mean().round(2).sum()}\")\n",
    "        print(f\"Checksum train y: {self.df_train_y[list(self.standardizing_values['y'].keys())].mean().round(2).sum()}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def unstandardize_data(self):\n",
    "\n",
    "        #apply values\n",
    "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
    "            for col in self.standardizing_values[col_type].keys():\n",
    "\n",
    "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
    "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
    "                df[col] = df[col] * std + mean #reversed standardization\n",
    "\n",
    "        return\n",
    "\n",
    "    def __unstanardize_y(self, y_t1, y_t2):\n",
    "\n",
    "        mean_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"mean\"]\n",
    "        mean_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"mean\"]\n",
    "    \n",
    "        std_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"std\"]\n",
    "        std_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"std\"]\n",
    "\n",
    "        y_t1_unst  = y_t1 * std_t1 + mean_t1\n",
    "        y_t2_unst  = y_t2 * std_t1 + mean_t2\n",
    "\n",
    "        return y_t1_unst, y_t2_unst\n",
    "\n",
    "    def __set_assesment(self):\n",
    "\n",
    "        if self.model_metric == \"c\":\n",
    "            self.get_model_score = self.__get_model_score_c\n",
    "\n",
    "        elif self.model_metric == \"r\":\n",
    "            self.get_model_score = self.__get_model_score_r\n",
    "\n",
    "    def __get_model_score_c(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
    "\n",
    "        #default\n",
    "        get_conf_mat = False\n",
    "        mat_labels = [0,1]\n",
    "\n",
    "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "            get_conf_mat = True #only get confuciton matrix when a single model is created\n",
    "        if model is None:\n",
    "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
    "            return\n",
    "\n",
    "        score = {}\n",
    "\n",
    "        #create predictions\n",
    "        y_train_pred    = model.predict(self.df_train_x)\n",
    "        y_valid_pred    = model.predict(self.df_valid_x)\n",
    "        y_test_pred     = model.predict(self.df_test_x)\n",
    "\n",
    "        #seperate t1 and t2 for individual scoring\n",
    "        for raw_key, y_pred, y in zip(\n",
    "            [\"train\",           \"valid\",            \"test\"],\n",
    "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
    "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
    "        ):\n",
    "\n",
    "            #not fetting test accurarcy if not set\n",
    "            if (get_test_score == False) and raw_key == \"test\":\n",
    "                continue\n",
    "\n",
    "            #split\n",
    "            y_pred_t1 = y_pred[:,0]\n",
    "            y_pred_t2 = y_pred[:,1]\n",
    "\n",
    "            y_t1 = y[self.y_col[0]]\n",
    "            y_t2 = y[self.y_col[1]]\n",
    "\n",
    "            #get acc\n",
    "            score[f\"{raw_key}_accuracy_t1\"]     = round(accuracy_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
    "            score[f\"{raw_key}_accuracy_t2\"]     = round(accuracy_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
    "            score[f\"{raw_key}_accuracy\"]        = round(accuracy_score(y_true = y, y_pred = y_pred),3)\n",
    "\n",
    "            #get conf mat\n",
    "            if get_conf_mat is True:\n",
    "                score[f\"{raw_key}_mat_t1\"]     = confusion_matrix(y_true = y_t1, y_pred = y_pred_t1, labels = mat_labels)\n",
    "                score[f\"{raw_key}_mat_t2\"]     = confusion_matrix(y_true = y_t2, y_pred = y_pred_t2, labels = mat_labels)\n",
    "                #score[f\"{raw_key}_mat\"]        = confusion_matrix(y_true = y, y_pred = y_pred) #multi labels are not supported\n",
    "\n",
    "        #return metrics\n",
    "        if get_conf_mat is True:\n",
    "            self.score = score\n",
    "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
    "            return\n",
    "\n",
    "        return score\n",
    "\n",
    "    def __get_model_score_r(self, model = None, get_test_score = False, unstandardize_score = False): #used, when model_metric == \"c\"\n",
    "\n",
    "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
    "        set_score = False\n",
    "\n",
    "        if model is None: #model is not none when automation is run\n",
    "            model = self.model\n",
    "            set_score = True\n",
    "        if model is None:\n",
    "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
    "            return\n",
    "\n",
    "        score = {}\n",
    "\n",
    "        #create predictions\n",
    "        y_train_pred    = model.predict(self.df_train_x)\n",
    "        y_valid_pred    = model.predict(self.df_valid_x)\n",
    "        y_test_pred     = model.predict(self.df_test_x)\n",
    "\n",
    "        #seperate t1 and t2 for individual scoring\n",
    "        for raw_key, y_pred, y in zip(\n",
    "            [\"train\",           \"valid\",            \"test\"],\n",
    "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
    "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
    "        ):\n",
    "\n",
    "            #not fetting test accurarcy if not set\n",
    "            if (get_test_score == False) and raw_key == \"test\":\n",
    "                continue\n",
    "\n",
    "            #split\n",
    "            y_pred_t1 = y_pred[:,0]\n",
    "            y_pred_t2 = y_pred[:,1]\n",
    "\n",
    "            y_t1 = y[self.y_col[0]]\n",
    "            y_t2 = y[self.y_col[1]]\n",
    "\n",
    "            #unstandardize data (ugly code go brrrr)\n",
    "            if unstandardize_score:\n",
    "                y_t1, y_t2              = self.__unstanardize_y(y_t1 = y_t1, y_t2 = y_t2)\n",
    "                y_pred_t1, y_pred_t2    = self.__unstanardize_y(y_t1 = y_pred_t1, y_t2 = y_pred_t2)\n",
    "\n",
    "                y_pred[:,0], y_pred[:,1]                = y_pred_t1, y_pred_t2\n",
    "                y[self.y_col[0]], y[self.y_col[1]]      = y_t1, y_t2\n",
    "\n",
    "            #get r^2\n",
    "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
    "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
    "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
    "\n",
    "            #get rmse\n",
    "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
    "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
    "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
    "\n",
    "        #return metrics\n",
    "        if set_score:\n",
    "            self.score = score\n",
    "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
    "            return\n",
    "\n",
    "        return score\n",
    "\n",
    "    def plot_confusion_mat(self, set = \"valid\"):\n",
    "        \"\"\"set = 'train', 'valid', 'test'\"\"\"\n",
    "\n",
    "        mat_keys = [key for key in self.score.keys() if (\"mat\" in key) and (set in key)]\n",
    "\n",
    "        for mat_key in mat_keys:\n",
    "\n",
    "            mat = self.score[mat_key]\n",
    "            title = str(mat_key).replace(\"_mat_\", \" \")\n",
    "\n",
    "            fig  = px.imshow(\n",
    "                mat,\n",
    "                color_continuous_scale = px.colors.sequential.haline_r,\n",
    "                text_auto = True,\n",
    "            )\n",
    "\n",
    "            #labels and layout\n",
    "            fig.update_layout(\n",
    "\n",
    "                title = f\"Confusion matrix: {title}\",\n",
    "\n",
    "                width=500,\n",
    "                height=500,\n",
    "                \n",
    "                xaxis_title=\"Predicted label\",\n",
    "                yaxis_title=\"True label\",\n",
    "\n",
    "                xaxis = dict(\n",
    "                    tickmode = 'array',\n",
    "                    tickvals = [0,1],\n",
    "                    ticktext = [\"above\", \"below\"]\n",
    "                ),\n",
    "\n",
    "                yaxis = dict(\n",
    "                    tickmode = 'array',\n",
    "                    tickvals = [0,1],\n",
    "                    ticktext = [\"above\", \"below\"],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            #set font\n",
    "            fig.update_layout(\n",
    "                font = dict(size=16),\n",
    "                title_font = dict(size=20),\n",
    "                xaxis_title_font = dict(size=18),\n",
    "                yaxis_title_font = dict(size=18),\n",
    "            )\n",
    "\n",
    "            fig.show()\n",
    "\n",
    "    def save_result(self, param, score):\n",
    "\n",
    "        #merge and create a dataframe\n",
    "        param.update(score); data = param\n",
    "        df_result = pd.DataFrame([data])\n",
    "\n",
    "        #create results file and set header length as param to negate reading file\n",
    "        if os.path.isfile(self.results_file) is True:\n",
    "            df_saved_result = pd.read_csv(self.results_file)\n",
    "            df_result = df_saved_result.append(df_result)\n",
    "\n",
    "        df_result.to_csv(self.results_file, index = False)\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_results(self):\n",
    "\n",
    "        df = pd.read_csv(self.results_file)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(10))\n",
    "print(sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML(Base): #child\n",
    "\n",
    "    def run_optim(self, nu_max = 11, degrees = 9, use_small_arch_list = False):\n",
    "\n",
    "        self.model = None #clear any models if there should be one\n",
    "\n",
    "        if use_small_arch_list is False:\n",
    "            arch_list = self.__generate_arch_list_all(\n",
    "                n_layers            = 4,\n",
    "                n_node_steps_div    = 3,\n",
    "                min_nodes           = 10\n",
    "            )\n",
    "        elif use_small_arch_list is True:\n",
    "            arch_list = self.__generate_arch_list_limited()\n",
    "\n",
    "        for arch in arch_list:\n",
    "            print( f\"Progress of optim:\\t{round((arch_list.index(arch) / len(arch_list)) * 100,1)}\",end = \"\\r\")\n",
    "\n",
    "            #refromat to save all comparison data\n",
    "            param = {\n",
    "                \"hidden_layer_sizes\"        : arch,\n",
    "                \"n_layers\"                  : len(arch),\n",
    "                \"n_neurons\"                 : sum(arch),\n",
    "                \"mean_neurosn_per_layer\"    : sum(arch) / len(arch),\n",
    "            }\n",
    "\n",
    "            #create model scoring to evalute models\n",
    "            score = self.create_model(arch = arch, single_model = False)\n",
    "            self.save_result(param = param, score = score)\n",
    "\n",
    "        print(\"Optim successfull. Read results with self.get_results()\")\n",
    "        return\n",
    "\n",
    "    def __generate_arch_list_limited(self, arch_log = 2, lin_arch_scaling = 2, n_layers = 5):\n",
    "\n",
    "        arch_list : list = []\n",
    "        n_features = len(self.df_train_x.columns.tolist())\n",
    "\n",
    "        #nodes_pow_2 = [3 ** (cone_arch_base_power + p) for p in range(1, n_layers + 1)][::-1]\n",
    "        nodes_log_2 = [int(n_features * (1 / arch_log ** i)) for i in range(1,n_layers + 1)]\n",
    "\n",
    "        for n_layer in range(1, n_layers + 1):\n",
    "\n",
    "            #linear\n",
    "            for size in  [n_features / (i * lin_arch_scaling) for i in range(1,4)]:\n",
    "                arch_lin = [int(size)] * n_layer\n",
    "                arch_list.append(arch_lin)\n",
    "\n",
    "            #cone\n",
    "            arch_cone = nodes_log_2[:n_layer]\n",
    "            arch_list.append(arch_cone)\n",
    "\n",
    "            #cone r\n",
    "            arch_cone_r = arch_cone[::-1]\n",
    "            arch_list.append(arch_cone_r)\n",
    "\n",
    "        return arch_list\n",
    "\n",
    "    def __generate_arch_list_all(self, n_layers = 4, n_node_steps_div = 3, min_nodes = 10):\n",
    "\n",
    "        n_input_nodes : int = len(self.df_train_x.columns)\n",
    "        arch_list = []\n",
    "        n_node_list : list = [n_input_nodes]\n",
    "\n",
    "        #set fixed params\n",
    "        min_node_division = 2 #minumum number of nodes on a layer\n",
    "\n",
    "        #divisonal\n",
    "        counter = 1\n",
    "\n",
    "        while True:\n",
    "\n",
    "            n_nodes = int(n_input_nodes / (n_node_steps_div ** counter))\n",
    "\n",
    "            if n_nodes < min_nodes:\n",
    "                break\n",
    "\n",
    "            n_node_list.append(n_nodes)\n",
    "            counter += 1\n",
    "\n",
    "        #clean up\n",
    "        n_node_list = list(set(n_node_list)); n_node_list.append(0); n_node_list.sort()\n",
    "\n",
    "        #create archs\n",
    "        for i in range(1, (len(n_node_list)**n_layers) + 1):\n",
    "\n",
    "            arch = []\n",
    "\n",
    "            for j in list(range(n_layers))[::-1]:\n",
    "\n",
    "                v =  int((i % (len(n_node_list) ** (j + 1)) / (len(n_node_list) ** j)))\n",
    "                arch.append(n_node_list[v])\n",
    "\n",
    "            arch = [k for k in arch if k != 0] #remove zero value\n",
    "\n",
    "            if arch in arch_list:\n",
    "                continue\n",
    "            arch_list.append(arch)\n",
    "\n",
    "        return arch_list\n",
    "\n",
    "    def create_model(self, arch, single_model = True, param = None):\n",
    "        \"\"\"if single_model == False:\n",
    "            the scores get retuned\n",
    "            self.mode is not set\n",
    "        elif single_model == True:\n",
    "            scores do not get returned\n",
    "            seld.model is set\"\"\"\n",
    "\n",
    "        #if no parameters are given, the following default params are used\n",
    "        if param is None:\n",
    "            param = self.default_param\n",
    "\n",
    "        #create model\n",
    "        if self.model_metric == \"r\":\n",
    "            ml_model = mlr\n",
    "            #multi_output = MultiOutputRegressor\n",
    "\n",
    "        elif self.model_metric == \"c\":\n",
    "            ml_model = mlc\n",
    "            #multi_output = MultiOutputClassifier\n",
    "\n",
    "        model = ml_model(\n",
    "            random_state            = self.random_state,\n",
    "            hidden_layer_sizes      = arch,\n",
    "            **param,\n",
    "        )\n",
    "\n",
    "        #model = multi_output(\n",
    "        #    n_jobs = self.n_jobs,\n",
    "        #    estimator = ml_model(\n",
    "        #        random_state    = self.random_state,\n",
    "        #        **param, #unpack the dict and dumps its values\n",
    "        #    )\n",
    "        #)\n",
    "\n",
    "        #fit model\n",
    "        model.fit(X = self.df_train_x, y = self.df_train_y)\n",
    "\n",
    "        #set according metrics\n",
    "        if single_model is True:\n",
    "            self.model = model\n",
    "            print(self.model)\n",
    "            return\n",
    "\n",
    "        elif single_model is False:\n",
    "            score = self.get_model_score(model)\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlc_obj = ML(\n",
    "    df              = df,\n",
    "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "    n_jobs          = 5,\n",
    "\n",
    "    data_folder     = data_folder,\n",
    "    results_file    = \"optim_reults_mlc.csv\",\n",
    "\n",
    "    model_metric    =  \"c\", # r = regression, c = classification\n",
    "    window          = 30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_optim is True:\n",
    "    mlc_obj.run_optim()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = mlc_obj.get_results()\n",
    "df_results.sort_values(by = \"valid_accuracy_t1\", ascending = False, inplace = True)\n",
    "\n",
    "df_results.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by = \"valid_accuracy_t2\", ascending = False, inplace = True)\n",
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by = \"valid_accuracy\", ascending = False, inplace = True)\n",
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"valid_accuracy\", \"valid_accuracy_t1\", \"valid_accuracy_t2\"]:\n",
    "\n",
    "    fig = px.scatter(\n",
    "        data_frame = df_results,\n",
    "        x = \"n_neurons\",\n",
    "        y = col,\n",
    "        color = \"mean_neurosn_per_layer\",\n",
    "        #size = \"n_layers\",\n",
    "        color_continuous_scale = plt_style_c,\n",
    "\n",
    "        title = \"Fitting graph: Multi-layer perceptron classifier\",\n",
    "        opacity = 1,\n",
    "        trendline = \"lowess\",\n",
    "\n",
    "        labels = {\"mean_neurosn_per_layer\": \"mean neurons\\nper layer\"}\n",
    "    )\n",
    "\n",
    "    #fig.update_traces(marker=dict(line=dict(color='rgba(0, 0, 0, 0)')))\n",
    "\n",
    "    scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Networkplotter():\n",
    "\n",
    "    def plot_ann(arch = [10,3,2,1], title = \"ANN architecture\"):\n",
    "\n",
    "        structure = Networkplotter.create_neurons(arch) #df\n",
    "        connections = Networkplotter.create_connections(structure, arch) #dict\n",
    "        Networkplotter.draw_network(structure, connections, arch, title)\n",
    "\n",
    "    def create_neurons(arch):\n",
    "\n",
    "        structure = {\n",
    "            \"layer_pos\"  : [],\n",
    "            \"neuron_pos\" : [],\n",
    "        }\n",
    "\n",
    "        max_neurons = max(arch)\n",
    "        mid_pos = max_neurons / 2\n",
    "\n",
    "        for i in range(len(arch)):\n",
    "\n",
    "            neuron_pos = mid_pos - (arch[i] / 2)\n",
    "\n",
    "            for neuron in range(arch[i]):\n",
    "\n",
    "                structure[\"layer_pos\"].append(i),\n",
    "                structure[\"neuron_pos\"].append(neuron_pos)\n",
    "                neuron_pos += 1\n",
    "\n",
    "        return pd.DataFrame(structure)\n",
    "\n",
    "    def create_connections(structure, arch):\n",
    "\n",
    "        connections = {\n",
    "            \"x\" :   [], #(x1,x2), (x1,x2), layer_pos\n",
    "            \"y\" :   [], #(y1,y2), (y1,y2), neuron_pos\n",
    "        }\n",
    "\n",
    "        relevant_layers = list(range(len(arch)))[:-1]\n",
    "        relevant_neurons = structure.loc[structure[\"layer_pos\"].isin(relevant_layers)]\n",
    "\n",
    "        for i in range(relevant_neurons.shape[0]):\n",
    "\n",
    "            x1 = structure.iloc[i][\"layer_pos\"]\n",
    "            y1 = structure.iloc[i][\"neuron_pos\"]\n",
    "            x2 = x1 + 1\n",
    "\n",
    "            for j in structure.loc[structure[\"layer_pos\"] == x2].index.tolist():\n",
    "                y2 = float(structure.iloc[j][\"neuron_pos\"])\n",
    "\n",
    "                connections[\"x\"].append((x1,x2))\n",
    "                connections[\"y\"].append((y1,y2))\n",
    "\n",
    "        return connections\n",
    "\n",
    "    def draw_network(structure, connections, arch, title):\n",
    "\n",
    "        width   = len(arch) * 150\n",
    "        height  = 700\n",
    "        structure[\"size\"] = 1\n",
    "\n",
    "        fig_base = px.scatter(\n",
    "            data_frame = structure,\n",
    "            x = \"layer_pos\",\n",
    "            y = \"neuron_pos\",\n",
    "            size_max = 10,\n",
    "            size = \"size\",\n",
    "\n",
    "            title = title,\n",
    "\n",
    "            width = width,\n",
    "            height = height,\n",
    "            #color = \"neuron_pos\",\n",
    "            labels = {\"layer_pos\" : \"layer\", \"neuron_pos\" : \"\",}\n",
    "        )\n",
    "\n",
    "        data = fig_base.data\n",
    "        for i in range(len(list(connections[\"x\"]))):\n",
    "\n",
    "            fig_base.add_shape(\n",
    "                type='line',\n",
    "                x0 = connections[\"x\"][i][0], y0 = connections[\"y\"][i][0],\n",
    "                x1 = connections[\"x\"][i][1], y1 = connections[\"y\"][i][1],\n",
    "                line=dict(color=\"lightgrey\", width=2),\n",
    "                layer = \"below\",\n",
    "            )\n",
    "\n",
    "        tick_text = list(range(len(arch)))\n",
    "        tick_text[0] = \"Input layer\"\n",
    "        tick_text[-1] = \"Output layer\"\n",
    "\n",
    "        fig_base.update_layout(\n",
    "            xaxis = dict(\n",
    "                tickmode = 'array',\n",
    "                tickvals = list(range(len(arch))),\n",
    "                ticktext = tick_text,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #unlcean code\n",
    "        fig_base.update_yaxes(showticklabels=False)\n",
    "        fig_base.update_layout(\n",
    "            xaxis=dict(showgrid=False),\n",
    "            yaxis=dict(showgrid=False)\n",
    "        )\n",
    "        fig_base.update_layout({\n",
    "            \"plot_bgcolor\": \"rgba(255, 255, 255, 255)\",\n",
    "            \"paper_bgcolor\": \"rgba(255, 255, 255, 255)\",\n",
    "            })\n",
    "\n",
    "        fig_base.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_arch = [len(mlc_obj.df_train_x.columns.tolist()), 12, 12, 12, 330, 4]\n",
    "ann_arch = [int(item / 4) for item in ann_arch]\n",
    "\n",
    "print(ann_arch)\n",
    "\n",
    "\n",
    "#Networkplotter.plot_ann(\n",
    "#    arch = ann_arch,\n",
    "#    title = \"ANN architecture: MLP classifier\",\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlc_obj.create_model(arch = [12, 12, 12, 330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_param = pd.DataFrame(mlc_obj.default_param, index = [\"optimal paraeters\"])\n",
    "optimal_param[\"hidden layer size\"] = str([12, 12, 12, 330])\n",
    "optimal_param.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = mlc_obj.get_model_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlc_obj.plot_confusion_mat(set = \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlc_obj.get_model_score(get_test_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlc_obj.plot_confusion_mat(set = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further improvements\n",
    "\n",
    "#overwrite defualt params\n",
    "\n",
    "for alpha in [1,0.5,0.25, 0.1, 0.01, 0.001]:\n",
    "    print(f\"\\nAlpha: {alpha}\")\n",
    "    mlc_obj.default_param = {\n",
    "        \"activation\"        : \"relu\",\n",
    "        \"solver\"            : \"adam\",\n",
    "        \"alpha\"             : alpha,         #defualt:   0.1\n",
    "        \"learning_rate\"     : \"adaptive\",\n",
    "        \"shuffle\"           : False,\n",
    "        \"early_stopping\"    : True,\n",
    "        \"max_iter\"          : 200,\n",
    "    }\n",
    "\n",
    "    mlc_obj.create_model(arch = [12, 12, 12, 330])\n",
    "    mlc_obj.get_model_score()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Overfitting prevention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_obj = ML(\n",
    "    df              = df,\n",
    "    y_col           = [\"t2m_t1\", \"t2m_t2\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "    n_jobs          = 5,\n",
    "\n",
    "    data_folder     = data_folder,\n",
    "    results_file    = \"optim_reults_mlr.csv\",\n",
    "\n",
    "    model_metric    =  \"r\", # r = regression, c = classification\n",
    "    window          = 30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_optim is True:\n",
    "    mlr_obj.run_optim(use_small_arch_list = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = mlr_obj.get_results()\n",
    "df_results.sort_values(by = \"valid_rmse\", ascending = True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by = \"valid_r^2\", ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in df_results[\"hidden_layer_sizes\"].tolist():\n",
    "    print(arch, end = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_obj.create_model(arch = [496, 248, 124, 62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_obj.get_model_score(get_test_score = True, unstandardize_score = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_obj.get_model_score(get_test_score = True, unstandardize_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_nn = False\n",
    "\n",
    "if draw_nn is True:\n",
    "    ann_arch = [496 * 2, 496, 248, 124, 62, 2]\n",
    "    ann_arch = [int(item / 25) if int(item / 25) > 0 else 1 for item in ann_arch]\n",
    "\n",
    "    print(ann_arch)\n",
    "\n",
    "\n",
    "    Networkplotter.plot_ann(\n",
    "        arch = ann_arch,\n",
    "        title = \"ANN architecture: MLP regressor\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
