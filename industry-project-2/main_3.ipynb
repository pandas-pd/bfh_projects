{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "#folders\n",
    "data_folder = \"data\"\n",
    "\n",
    "#machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit as tsp\n",
    "from sklearn.model_selection import GridSearchCV as gscv\n",
    "\n",
    "#warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optim = False #runtime: n min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot styles\n",
    "plt_style_c = px.colors.sequential.haline #complex\n",
    "plt_style_s = px.colors.diverging.Portland #simple\n",
    "\n",
    "#defualt plot size \n",
    "size = {\n",
    "    \"width\" : 1500 ,\n",
    "    \"height\" : 750 ,\n",
    "}\n",
    "\n",
    "#function for plotting\n",
    "def scale_show(fig, width = 1500, height = 750):\n",
    "\n",
    "    #set font\n",
    "    fig.update_layout(\n",
    "        font = dict(size=16),\n",
    "        title_font = dict(size=20),\n",
    "        xaxis_title_font = dict(size=18),\n",
    "        yaxis_title_font = dict(size=18),\n",
    "    )\n",
    "\n",
    "    #set size\n",
    "    fig.update_layout(\n",
    "        width=width,\n",
    "        height=height,\n",
    "    )\n",
    "\n",
    "    #show\n",
    "    fig.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_folder, \"df_main.csv\"))\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources for reasoning\n",
    "\n",
    "#data splitting: https://towardsdatascience.com/time-series-from-scratch-train-test-splits-and-evaluation-metrics-4fd654de1b37\n",
    "\n",
    "#data standardizing (train, test, valid): \n",
    "#   https://stats.stackexchange.com/questions/202287/why-standardization-of-the-testing-set-has-to-be-performed-with-the-mean-and-sd\n",
    "#   https://medium.com/analytics-vidhya/why-it-makes-a-difference-how-to-standardize-training-and-test-set-e95bf350bed3\n",
    "#   https://stats.stackexchange.com/questions/248543/standardize-training-and-validation-data\n",
    "#   https://www.kaggle.com/questions-and-answers/159183\n",
    "\n",
    "#formula for standardizing: https://www.statisticshowto.com/standardized-values-examples/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(): #parent\n",
    "\n",
    "    def __init__(self, df : object, y_col : list, data_folder : str, results_file : str, model_metric : str, n_jobs : int = 1, window : int = 30):\n",
    "\n",
    "        #save raw df\n",
    "        self.df_raw         = df.copy()\n",
    "\n",
    "        #drop forbidden cols\n",
    "        df = df.copy() #pass by value\n",
    "        df = Base.__drop_forbidden_cols(df, y_col)\n",
    "\n",
    "        #set dataframe for refferencing\n",
    "        self.df             = df.copy() #windowed df, copy because obj is passed by refference\n",
    "\n",
    "        #get and get x_col and y_col\n",
    "        self.y_col          = y_col\n",
    "        self.x_col          = list(df.drop(labels = y_col, axis = 1, inplace = False).columns.to_list())\n",
    "\n",
    "        #misc params\n",
    "        self.random_state   = 42\n",
    "        self.n_jobs         = n_jobs\n",
    "        self.data_folder    = data_folder\n",
    "        self.results_file   = os.path.join(data_folder,results_file)\n",
    "        self.model_metric   = model_metric\n",
    "\n",
    "        #windowing parameters\n",
    "        self.x_window       = window #number of shifting window input features\n",
    "\n",
    "        self.__setup()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __setup(self):\n",
    "\n",
    "        #order was chosen to minimize data loss, at the cost of more needed processing power\n",
    "\n",
    "        #data preparation\n",
    "        self.__windowing()\n",
    "        self.__split_data()\n",
    "        self.__standardize_data()\n",
    "\n",
    "        #setup of metrics and results\n",
    "        self.__set_assesment()\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def __drop_forbidden_cols(df, y_col):\n",
    "\n",
    "        forbidden_cols = ['date','t2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', 't2m_t1_cat', 't2m_t2_cat']\n",
    "\n",
    "        #prevent y_cols from being dropped from the data frame\n",
    "        for y in y_col:\n",
    "            if y in forbidden_cols:\n",
    "                forbidden_cols.remove(y)\n",
    "\n",
    "        #drop forbidden cols, to prevent adding future information to the time series\n",
    "        print(f\"Removed forbidden cols:\\n{forbidden_cols}\")\n",
    "        df.drop(labels = forbidden_cols, axis = 1, inplace = True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __windowing(self):\n",
    "        \"\"\"creates the windowed data frame\"\"\"\n",
    "\n",
    "        self.x_col_windowed = self.x_col.copy() #copy, becaus lists are past by refference\n",
    "\n",
    "        #input fetures: x\n",
    "        for i in range(1, self.x_window + 1):\n",
    "            for x_col in self.x_col: #inefficient but works just fine\n",
    "\n",
    "                x_col_i             = f\"{x_col}_-{i}\"\n",
    "                self.df[x_col_i]    = df[x_col].shift(i)\n",
    "\n",
    "                self.x_col_windowed.append(x_col_i)\n",
    "\n",
    "        #clean na columns, which were caused by the shifts\n",
    "        self.df.dropna(inplace = True)\n",
    "        print(f\"\\nApplying shifitng window:\\nx_window: -{self.x_window}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def __split_data(self):\n",
    "\n",
    "        #reset index for splitting data\n",
    "        self.df.reset_index(inplace = True, drop = True)\n",
    "        length = self.df.shape[0]\n",
    "\n",
    "        #setting split value fractions\n",
    "        valid_frac_0      = 0.1\n",
    "        test_frac_1       = 0.05\n",
    "        train_frac_2      = 0.7\n",
    "        valid_frac_3      = 0.1\n",
    "        test_frac_4       = 0.05\n",
    "\n",
    "        #get end indexes\n",
    "        index_end_list = []\n",
    "        cum_frac = 0\n",
    "\n",
    "        for frac in [valid_frac_0, test_frac_1, train_frac_2, valid_frac_3, test_frac_4]:\n",
    "            cum_frac += frac\n",
    "            index_end_list.append(round(length * cum_frac))\n",
    "\n",
    "        #get indexes (ugly code)\n",
    "        df_indexes = self.df.index.tolist()\n",
    "        train_i     = df_indexes[index_end_list[1] : index_end_list[2]]\n",
    "        valid_i     = df_indexes[ : index_end_list[0]]                      + df_indexes[index_end_list[2] : index_end_list[3]]\n",
    "        test_i      = df_indexes[index_end_list[0] : index_end_list[1]]     + df_indexes[index_end_list[3] : index_end_list[4]]\n",
    "\n",
    "        #get df from indexes\n",
    "        self.df_train_x = self.df[self.x_col_windowed].loc[self.df.index.isin(train_i)]\n",
    "        self.df_train_y = self.df[self.y_col].loc[self.df.index.isin(train_i)]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_valid_x = self.df[self.x_col_windowed].loc[self.df.index.isin(valid_i)]\n",
    "        self.df_valid_y = self.df[self.y_col].loc[self.df.index.isin(valid_i)]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_test_x = self.df[self.x_col_windowed].loc[self.df.index.isin(test_i)]\n",
    "        self.df_test_y = self.df[self.y_col].loc[self.df.index.isin(test_i)]\n",
    "\n",
    "        #check\n",
    "        print(\"\\nSplitting data:\")\n",
    "        for df, df_type in zip (\n",
    "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
    "            \"train,valid,test\".split(\",\")\n",
    "            ):\n",
    "\n",
    "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
    "\n",
    "        #set data for plotting in raw df\n",
    "        self.df_raw[\"set\"] = None\n",
    "        for index_items, set_type in zip([train_i, valid_i, test_i],[\"train\", \"valid\", \"test\"]):\n",
    "            self.df_raw.loc[self.df_raw.index.isin(index_items), \"set\"] = set_type\n",
    "\n",
    "        return\n",
    "\n",
    "    def plot_set_distribution(self, plotter, style, plt_style):\n",
    "\n",
    "        if (style == \"histogram\") and (self.model_metric == \"c\"): #only plotlable with classificaiton model\n",
    "\n",
    "            fig = px.histogram(\n",
    "                data_frame = self.df_raw,\n",
    "                x = \"set\",\n",
    "                color = \"t2m_t2_cat\",\n",
    "                histfunc = \"count\",\n",
    "\n",
    "                barmode = \"group\",\n",
    "                title = \"Categorical distribution of sets\",\n",
    "                color_discrete_sequence = plt_style,\n",
    "            )\n",
    "\n",
    "        elif (style == \"scatter\"):\n",
    "\n",
    "            fig = px.scatter(\n",
    "                data_frame = self.df_raw,\n",
    "                x = \"date\",\n",
    "                y = \"t2m\",\n",
    "                color = \"set\",\n",
    "\n",
    "                title = \"Trend distribution of sets\",\n",
    "                color_discrete_sequence = plt_style,\n",
    "            )\n",
    "\n",
    "        plotter(fig)\n",
    "\n",
    "    def __split_data_deprecated(self):\n",
    "\n",
    "        #df length\n",
    "        length = self.df.shape[0]\n",
    "\n",
    "        #setting split values\n",
    "        valid_frac     = 0.2\n",
    "        test_frac      = 0.1\n",
    "\n",
    "        #get indexes\n",
    "        train_end       = round(length * (1 - (valid_frac + test_frac)))\n",
    "        valid_end       = round(length * (1 - (test_frac)))\n",
    "        test_end        = round(length * (1))\n",
    "\n",
    "        #create train df\n",
    "        self.df_train_x = self.df[self.x_col_windowed].iloc[:train_end]\n",
    "        self.df_train_y = self.df[self.y_col].iloc[:train_end]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_valid_x = self.df[self.x_col_windowed].iloc[train_end:valid_end]\n",
    "        self.df_valid_y = self.df[self.y_col].iloc[train_end:valid_end]\n",
    "\n",
    "        #create valid df\n",
    "        self.df_test_x = self.df[self.x_col_windowed].iloc[valid_end:test_end]\n",
    "        self.df_test_y = self.df[self.y_col].iloc[valid_end:test_end]\n",
    "\n",
    "        #check\n",
    "        print(\"\\nSplitting data:\")\n",
    "        for df, df_type in zip (\n",
    "            [self.df_train_y,self.df_valid_y, self.df_test_y],\n",
    "            \"train,valid,test\".split(\",\")\n",
    "            ):\n",
    "\n",
    "            print(f\"{df_type} size:\\t{round(df.shape[0] / length,2)}\\t{df.shape[0]}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def __standardize_data(self):\n",
    "\n",
    "        label_cat = [0,1]; label_cat.sort()\n",
    "        self.standardizing_values = {\n",
    "            \"x\" : {},\n",
    "            \"y\" : {},\n",
    "            #    col1 : {\"mean\" : value, \"std\"  : value},\n",
    "            #    col2 : {\"mean\" : value, \"std\"  : value},\n",
    "            #}\n",
    "            #\"y\" : ...\n",
    "        }\n",
    "\n",
    "        print(\"\\nStandardizing values:\")\n",
    "        for col in self.df.columns:\n",
    "\n",
    "            distinct_values = list(self.df[col].unique())\n",
    "            distinct_values.sort()\n",
    "\n",
    "            if label_cat == distinct_values: #skip categorical values\n",
    "                continue\n",
    "\n",
    "            #get mean and std for all columns across both data both data frames\n",
    "            if col in self.x_col_windowed:\n",
    "\n",
    "                self.standardizing_values[\"x\"][col]             = {}\n",
    "                self.standardizing_values[\"x\"][col][\"mean\"]     = self.df_train_x[col].mean()\n",
    "                self.standardizing_values[\"x\"][col][\"std\"]      = self.df_train_x[col].std()\n",
    "\n",
    "            elif col in self.y_col:\n",
    "\n",
    "                self.standardizing_values[\"y\"][col]             = {}\n",
    "                self.standardizing_values[\"y\"][col][\"mean\"]     = self.df_train_y[col].mean()\n",
    "                self.standardizing_values[\"y\"][col][\"std\"]      = self.df_train_y[col].std()\n",
    "\n",
    "        #apply values\n",
    "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
    "            for col in self.standardizing_values[col_type].keys():\n",
    "\n",
    "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
    "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
    "                df[col] = (df[col] - mean) / std #standardization\n",
    "\n",
    "        #check sum\n",
    "        print(f\"Checksum train x: {self.df_train_x[list(self.standardizing_values['x'].keys())].mean().round(2).sum()}\")\n",
    "        print(f\"Checksum train y: {self.df_train_y[list(self.standardizing_values['y'].keys())].mean().round(2).sum()}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def unstandardize_data(self):\n",
    "\n",
    "        #apply values\n",
    "        for df, col_type in zip([self.df_train_x, self.df_valid_x, self.df_test_x, self.df_train_y, self.df_valid_y, self.df_test_y], [\"x\",\"x\",\"x\",\"y\",\"y\",\"y\"]):\n",
    "            for col in self.standardizing_values[col_type].keys():\n",
    "\n",
    "                mean    = self.standardizing_values[col_type][col][\"mean\"]\n",
    "                std     = self.standardizing_values[col_type][col][\"std\"]\n",
    "                df[col] = df[col] * std + mean #reversed standardization\n",
    "\n",
    "        return\n",
    "\n",
    "    def __unstanardize_y(self, y_t1, y_t2):\n",
    "\n",
    "        mean_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"mean\"]\n",
    "        mean_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"mean\"]\n",
    "    \n",
    "        std_t1        = self.standardizing_values[\"y\"][\"t2m_t1\"][\"std\"]\n",
    "        std_t2        = self.standardizing_values[\"y\"][\"t2m_t2\"][\"std\"]\n",
    "\n",
    "        y_t1_unst  = y_t1 * std_t1 + mean_t1\n",
    "        y_t2_unst  = y_t2 * std_t1 + mean_t2\n",
    "\n",
    "        return y_t1_unst, y_t2_unst\n",
    "\n",
    "    def __set_assesment(self):\n",
    "\n",
    "        if self.model_metric == \"c\":\n",
    "            self.get_model_score = self.__get_model_score_c\n",
    "\n",
    "        elif self.model_metric == \"r\":\n",
    "            self.get_model_score = self.__get_model_score_r\n",
    "\n",
    "    def __get_model_score_c(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
    "\n",
    "        #default\n",
    "        get_conf_mat = False\n",
    "        mat_labels = [0,1]\n",
    "\n",
    "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "            get_conf_mat = True #only get confuciton matrix when a single model is created\n",
    "        if model is None:\n",
    "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
    "            return\n",
    "\n",
    "        score = {}\n",
    "\n",
    "        #create predictions\n",
    "        y_train_pred    = model.predict(self.df_train_x)\n",
    "        y_valid_pred    = model.predict(self.df_valid_x)\n",
    "        y_test_pred     = model.predict(self.df_test_x)\n",
    "\n",
    "        #seperate t1 and t2 for individual scoring\n",
    "        for raw_key, y_pred, y in zip(\n",
    "            [\"train\",           \"valid\",            \"test\"],\n",
    "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
    "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
    "        ):\n",
    "\n",
    "            #not fetting test accurarcy if not set\n",
    "            if (get_test_score == False) and raw_key == \"test\":\n",
    "                continue\n",
    "\n",
    "            #split\n",
    "            y_pred_t1 = y_pred[:,0]\n",
    "            y_pred_t2 = y_pred[:,1]\n",
    "\n",
    "            y_t1 = y[self.y_col[0]]\n",
    "            y_t2 = y[self.y_col[1]]\n",
    "\n",
    "            #get acc\n",
    "            score[f\"{raw_key}_accuracy_t1\"]     = round(accuracy_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
    "            score[f\"{raw_key}_accuracy_t2\"]     = round(accuracy_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
    "            score[f\"{raw_key}_accuracy\"]        = round(accuracy_score(y_true = y, y_pred = y_pred),3)\n",
    "\n",
    "            #get conf mat\n",
    "            if get_conf_mat is True:\n",
    "                score[f\"{raw_key}_mat_t1\"]     = confusion_matrix(y_true = y_t1, y_pred = y_pred_t1, labels = mat_labels)\n",
    "                score[f\"{raw_key}_mat_t2\"]     = confusion_matrix(y_true = y_t2, y_pred = y_pred_t2, labels = mat_labels)\n",
    "                #score[f\"{raw_key}_mat\"]        = confusion_matrix(y_true = y, y_pred = y_pred) #multi labels are not supported\n",
    "\n",
    "        #return metrics\n",
    "        if get_conf_mat is True:\n",
    "            self.score = score\n",
    "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
    "            return score\n",
    "\n",
    "        return score\n",
    "\n",
    "    def __get_model_score_r(self, model = None, get_test_score = False, unstandardize_score = False): #used, when model_metric == \"c\"\n",
    "\n",
    "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
    "        set_score = False\n",
    "\n",
    "        if model is None: #model is not none when automation is run\n",
    "            model = self.model\n",
    "            set_score = True\n",
    "        if model is None:\n",
    "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
    "            return\n",
    "\n",
    "        score = {}\n",
    "\n",
    "        #create predictions\n",
    "        y_train_pred    = model.predict(self.df_train_x)\n",
    "        y_valid_pred    = model.predict(self.df_valid_x)\n",
    "        y_test_pred     = model.predict(self.df_test_x)\n",
    "\n",
    "        #seperate t1 and t2 for individual scoring\n",
    "        for raw_key, y_pred, y in zip(\n",
    "            [\"train\",           \"valid\",            \"test\"],\n",
    "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
    "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
    "        ):\n",
    "\n",
    "            #not fetting test accurarcy if not set\n",
    "            if (get_test_score == False) and raw_key == \"test\":\n",
    "                continue\n",
    "\n",
    "            #split\n",
    "            y_pred_t1 = y_pred[:,0]\n",
    "            y_pred_t2 = y_pred[:,1]\n",
    "\n",
    "            y_t1 = y[self.y_col[0]]\n",
    "            y_t2 = y[self.y_col[1]]\n",
    "\n",
    "            #unstandardize data (ugly code go brrrr)\n",
    "            if unstandardize_score:\n",
    "                y_t1, y_t2              = self.__unstanardize_y(y_t1 = y_t1, y_t2 = y_t2)\n",
    "                y_pred_t1, y_pred_t2    = self.__unstanardize_y(y_t1 = y_pred_t1, y_t2 = y_pred_t2)\n",
    "\n",
    "                y_pred[:,0], y_pred[:,1]                = y_pred_t1, y_pred_t2\n",
    "                y[self.y_col[0]], y[self.y_col[1]]      = y_t1, y_t2\n",
    "\n",
    "            #get r^2\n",
    "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
    "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
    "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
    "\n",
    "            #get rmse\n",
    "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
    "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
    "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
    "\n",
    "        #return metrics\n",
    "        if set_score:\n",
    "            self.score = score\n",
    "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
    "            return\n",
    "\n",
    "        return score\n",
    "\n",
    "    def __get_model_score_r_deprecated(self, model = None, get_test_score = False): #used, when model_metric == \"c\"\n",
    "\n",
    "        #if a model is passed, the function is calles from run_optim, otherwise,\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "            set_score = True\n",
    "        if model is None:\n",
    "            print(\"No model has been set. Create a model first or pass one as a param\")\n",
    "            return\n",
    "\n",
    "        score = {}\n",
    "\n",
    "        #create predictions\n",
    "        y_train_pred    = model.predict(self.df_train_x)\n",
    "        y_valid_pred    = model.predict(self.df_valid_x)\n",
    "        y_test_pred     = model.predict(self.df_test_x)\n",
    "\n",
    "        #seperate t1 and t2 for individual scoring\n",
    "        for raw_key, y_pred, y in zip(\n",
    "            [\"train\",           \"valid\",            \"test\"],\n",
    "            [y_train_pred,      y_valid_pred,       y_test_pred],\n",
    "            [self.df_train_y,   self.df_valid_y,    self.df_test_y],\n",
    "        ):\n",
    "\n",
    "            #not fetting test accurarcy if not set\n",
    "            if (get_test_score == False) and raw_key == \"test\":\n",
    "                continue\n",
    "\n",
    "            #split\n",
    "            y_pred_t1 = y_pred[:,0]\n",
    "            y_pred_t2 = y_pred[:,1]\n",
    "\n",
    "            y_t1 = y[self.y_col[0]]\n",
    "            y_t2 = y[self.y_col[1]]\n",
    "\n",
    "            #get r^2\n",
    "            score[f\"{raw_key}_r^2_t1\"]      = round(r2_score(y_true = y_t1, y_pred = y_pred_t1),3)\n",
    "            score[f\"{raw_key}_r^2_t2\"]      = round(r2_score(y_true = y_t2, y_pred = y_pred_t2),3)\n",
    "            score[f\"{raw_key}_r^2\"]         = round(r2_score(y_true = y, y_pred = y_pred),3)\n",
    "\n",
    "            #get rmse\n",
    "            score[f\"{raw_key}_rmse_t1\"]      = round(np.sqrt(mean_squared_error(y_true = y_t1, y_pred = y_pred_t1)),3)\n",
    "            score[f\"{raw_key}_rmse_t2\"]      = round(np.sqrt(mean_squared_error(y_true = y_t2, y_pred = y_pred_t2)),3)\n",
    "            score[f\"{raw_key}_rmse\"]         = round(np.sqrt(mean_squared_error(y_true = y, y_pred = y_pred)),3)\n",
    "\n",
    "        #return metrics\n",
    "        if set_score:\n",
    "            self.score = score\n",
    "            [print(f\"{key} :\\t\\t{score[key]}\") for key in score.keys() if isinstance(score[key],float)]\n",
    "            return\n",
    "\n",
    "        return score\n",
    "\n",
    "    def plot_confusion_mat(self, set = \"valid\"):\n",
    "        \"\"\"set = 'train', 'valid', 'test'\"\"\"\n",
    "\n",
    "        mat_keys = [key for key in self.score.keys() if (\"mat\" in key) and (set in key)]\n",
    "\n",
    "        for mat_key in mat_keys:\n",
    "\n",
    "            mat = self.score[mat_key]\n",
    "            title = str(mat_key).replace(\"_mat_\", \" \")\n",
    "\n",
    "            fig  = px.imshow(\n",
    "                mat,\n",
    "                color_continuous_scale = px.colors.sequential.haline_r,\n",
    "                text_auto = True,\n",
    "            )\n",
    "\n",
    "            #labels and layout\n",
    "            fig.update_layout(\n",
    "\n",
    "                title = f\"Confusion matrix: {title}\",\n",
    "\n",
    "                width=500,\n",
    "                height=500,\n",
    "                \n",
    "                xaxis_title=\"Predicted label\",\n",
    "                yaxis_title=\"True label\",\n",
    "\n",
    "                xaxis = dict(\n",
    "                    tickmode = 'array',\n",
    "                    tickvals = [0,1],\n",
    "                    ticktext = [\"above\", \"below\"]\n",
    "                ),\n",
    "\n",
    "                yaxis = dict(\n",
    "                    tickmode = 'array',\n",
    "                    tickvals = [0,1],\n",
    "                    ticktext = [\"above\", \"below\"],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            #set font\n",
    "            fig.update_layout(\n",
    "                font = dict(size=16),\n",
    "                title_font = dict(size=20),\n",
    "                xaxis_title_font = dict(size=18),\n",
    "                yaxis_title_font = dict(size=18),\n",
    "            )\n",
    "\n",
    "            fig.show()\n",
    "\n",
    "    def save_result(self, param, score):\n",
    "\n",
    "        #merge and create a dataframe\n",
    "        param.update(score); data = param\n",
    "        df_result = pd.DataFrame([data])\n",
    "\n",
    "        #create results file and set header length as param to negate reading file\n",
    "        if os.path.isfile(self.results_file) is True:\n",
    "            df_saved_result = pd.read_csv(self.results_file)\n",
    "            df_result = df_saved_result.append(df_result)\n",
    "\n",
    "        df_result.to_csv(self.results_file, index = False)\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_results(self):\n",
    "\n",
    "        df = pd.read_csv(self.results_file)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF(Base): #child\n",
    "\n",
    "    def run_optim(self, n_trees = 12, n_depth=3, n_leafs = 5):\n",
    "\n",
    "        self.model = None #clear any models if there should be one\n",
    "\n",
    "        param_list = self.create_param_list(\n",
    "            n_trees = n_trees,\n",
    "            n_depth = n_depth,\n",
    "            n_leafs = n_leafs,\n",
    "        )\n",
    "\n",
    "        for param in param_list:\n",
    "\n",
    "            print( f\"Progress of optim:\\t{round((param_list.index(param) / len(param_list)) * 100,1)}\",end = \"\\r\")\n",
    "\n",
    "            score = self.create_model(param = param, single_model = False)\n",
    "            self.save_result(param = param, score = score)\n",
    "\n",
    "        print(\"Optim successfull. Read results with self.get_results()\")\n",
    "        return\n",
    "\n",
    "    def create_param_list(self, n_trees, n_depth, n_leafs):\n",
    "        \"\"\"return list: [{param_1 : value_1},{},]\"\"\"\n",
    "\n",
    "        param_list = []\n",
    "\n",
    "        #get individual numbers\n",
    "        n_estimators        = [2**n for n in range(1,n_trees+1)]\n",
    "        max_depths          = [10*n for n in range(1,n_depth+1)]\n",
    "        min_sample_leafs    = [2*n  for n in range(1,n_leafs+1)]\n",
    "\n",
    "        combinations = list(itertools.product(n_estimators, max_depths, min_sample_leafs))\n",
    "\n",
    "        for combination in combinations:\n",
    "\n",
    "            params = {\n",
    "                \"n_estimators\"      : combination[0],\n",
    "                \"max_depth\"         : combination[1],\n",
    "                \"min_samples_leaf\"  : combination[2],\n",
    "            }\n",
    "\n",
    "            param_list.append(params)\n",
    "\n",
    "        print(f\"\\nGenerated param combinations: {len(param_list)}\")\n",
    "        return param_list\n",
    "\n",
    "\n",
    "    def create_model(self, param, single_model = True):\n",
    "        \"\"\"if single_model == False:\n",
    "            the scores get retuned\n",
    "            self.mode is not set\n",
    "        elif single_model == True:\n",
    "            scores do not get returned\n",
    "            seld.model is set\"\"\"\n",
    "\n",
    "        #create model\n",
    "        if self.model_metric == \"r\":\n",
    "            ml_model = rfr\n",
    "\n",
    "        elif self.model_metric == \"c\":\n",
    "            ml_model = rfc\n",
    "\n",
    "        model = ml_model(\n",
    "            n_jobs          = self.n_jobs,\n",
    "            random_state    = self.random_state,\n",
    "            **param, #unpack the dict and dumps its values\n",
    "        )\n",
    "\n",
    "        #fit model\n",
    "        model.fit(X = self.df_train_x, y = self.df_train_y)\n",
    "\n",
    "        #set according metrics\n",
    "        if single_model is True:\n",
    "            self.model = model\n",
    "            print(self.model)\n",
    "            return\n",
    "\n",
    "        elif single_model is False:\n",
    "            score = self.get_model_score(model)\n",
    "            return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 RF Calssification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Modeling and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_obj = RF(\n",
    "    df              = df,\n",
    "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "    n_jobs          = 4,\n",
    "\n",
    "    data_folder     = data_folder,\n",
    "    results_file    = \"optim_reults_rfc.csv\",\n",
    "\n",
    "    model_metric    =  \"c\" # r = regression, c = classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist = False\n",
    "\n",
    "if plot_dist:\n",
    "    rfc_obj.plot_set_distribution(plotter = scale_show, style = \"histogram\", plt_style = plt_style_s)\n",
    "    rfc_obj.plot_set_distribution(plotter = scale_show, style = \"scatter\", plt_style = plt_style_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_optim is True:\n",
    "    rfc_obj.run_optim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = rfc_obj.get_results()\n",
    "df_results.sort_values(by = \"valid_accuracy\", ascending = False, inplace = True)\n",
    "df_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by = \"valid_accuracy_t1\", ascending = False, inplace = True)\n",
    "df_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by = \"valid_accuracy_t2\", ascending = False, inplace = True)\n",
    "df_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in [\"\", \"_t1\", \"_t2\"]:\n",
    "\n",
    "    title = \"Fitting graph: estimators\"\n",
    "\n",
    "    if frame != \"\":\n",
    "        title = f\"Fitting graph {frame[1:]}: estimators\"\n",
    "\n",
    "    fig = px.scatter(\n",
    "\n",
    "        data_frame = df_results,\n",
    "        x = \"n_estimators\",\n",
    "        y = [f\"train_accuracy{frame}\", f\"valid_accuracy{frame}\"],\n",
    "\n",
    "        color_discrete_sequence = plt_style_s,\n",
    "        title = title,\n",
    "        log_x  = True,\n",
    "\n",
    "        labels = {\"value\": \"accuracy\"},\n",
    "        range_y = [0,1.1]\n",
    "\n",
    "    )\n",
    "\n",
    "    scale_show(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Top model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sinlge model\n",
    "optimal_param = {\n",
    "    \"n_estimators\"      : 128,\n",
    "    \"max_depth\"         : 10,\n",
    "    \"min_samples_leaf\"   : 8\n",
    "}\n",
    "\n",
    "rfc_obj.create_model(param = optimal_param)\n",
    "rfc_obj.get_model_score()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_obj.plot_confusion_mat(set = \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion mat\n",
    "#add plotting to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance and further improvment\n",
    "weights         = rfc_obj.model.feature_importances_\n",
    "cols_window     = rfc_obj.x_col_windowed\n",
    "n_col           = len(rfc_obj.x_col)\n",
    "window_size     = rfc_obj.x_window\n",
    "\n",
    "mat_head    = rfc_obj.x_col\n",
    "mat         = []\n",
    "\n",
    "last_satrt = 0\n",
    "\n",
    "for i in range(1, 1+window_size):\n",
    "\n",
    "    end = i * n_col\n",
    "    mat.append(list(weights[last_satrt:end]))\n",
    "\n",
    "    last_satrt = end\n",
    "\n",
    "df_feature_importance = pd.DataFrame(mat, columns = rfc_obj.x_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_by_f = df_feature_importance.sum()\n",
    "df_feature_importance_by_f.sort_values(inplace = True, ascending = False)\n",
    "\n",
    "fig = px.histogram(\n",
    "    x = df_feature_importance_by_f,\n",
    "    y = df_feature_importance_by_f.index,\n",
    "    nbins = n_col,\n",
    "    histfunc = \"sum\",\n",
    "    color_discrete_sequence = plt_style_s,\n",
    "    title = \"Feature importance by feature\",\n",
    "\n",
    "    labels = {\"x\" : \"weights\", \"y\" : \"feature\"}\n",
    ")\n",
    "\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "\n",
    "scale_show(fig, width = 750, height = 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_by_t = df_feature_importance.T.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_by_t = df_feature_importance.T.sum()\n",
    "#df_feature_importance_by_t.sort_values(inplace = True, ascending = False)\n",
    "\n",
    "fig = px.histogram(\n",
    "    x = df_feature_importance_by_t,\n",
    "    y = df_feature_importance_by_t.index.astype(str),\n",
    "    nbins = n_col,\n",
    "    histfunc = \"sum\",\n",
    "    color_discrete_sequence = plt_style_s,\n",
    "    title = \"Feature importance by time\",\n",
    "\n",
    "    labels = {\"x\" : \"weights\", \"y\" : \"time offset\"}\n",
    ")\n",
    "\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "\n",
    "scale_show(fig, width = 750, height = 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(df_feature_importance.T.to_numpy(), color_continuous_scale = plt_style_c)\n",
    "\n",
    "fig.update_layout(\n",
    "\n",
    "    title = f\"Feature importance\",\n",
    "\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    \n",
    "    yaxis_title=\"Feature\",\n",
    "    xaxis_title=\"Time\",\n",
    "\n",
    "    yaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = list(range(len(rfc_obj.x_col))),\n",
    "        ticktext =  rfc_obj.x_col,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    font = dict(size=16),\n",
    "    title_font = dict(size=20),\n",
    "    xaxis_title_font = dict(size=18),\n",
    "    yaxis_title_font = dict(size=18),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply to testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_obj.get_model_score( get_test_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_obj.plot_confusion_mat(set = \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Exploration of missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mis = df.drop(labels = \"year\", axis = 1, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_obj_mis = RF(\n",
    "    df              = df_mis,\n",
    "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "    n_jobs          = 4,\n",
    "\n",
    "    data_folder     = data_folder,\n",
    "    results_file    = \"optim_reults_rfc.csv\",\n",
    "\n",
    "    model_metric    =  \"c\" # r = regression, c = classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_obj_mis.create_model(param = optimal_param)\n",
    "rfc_obj_mis.get_model_score(get_test_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_obj_mis.plot_confusion_mat(set = \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping feautres\n",
    "set_0 = df_feature_importance_by_f.index[:5].tolist() + ['date', 't2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', \"t2m_t1_cat\", \"t2m_t2_cat\"]\n",
    "set_1 = df_feature_importance_by_f.index[:10].tolist() + ['date', 't2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', \"t2m_t1_cat\", \"t2m_t2_cat\"]\n",
    "set_2 = df_feature_importance_by_f.index[:15].tolist() + ['date', 't2m_t1', 't2m_t2', 't2m_t1_mean', 't2m_t2_mean', \"t2m_t1_cat\", \"t2m_t2_cat\"]\n",
    "set_3 = df.drop(labels = [\"nao\", \"ao\", \"t2m\", \"mjo_amplitude\", \"sp\", \"day\", \"soi\"], axis = 1, inplace = False).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[set_0]\n",
    "df_1 = df[set_1]\n",
    "df_2 = df[set_2]\n",
    "df_3 = df[set_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic(df, set_name):\n",
    "\n",
    "    rfc_set = RF(\n",
    "        df              = df,\n",
    "        y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "        n_jobs          = 4,\n",
    "\n",
    "        data_folder     = data_folder,\n",
    "        results_file    = \"optim_reults_rfc.csv\",\n",
    "\n",
    "        model_metric    =  \"c\" # r = regression, c = classification\n",
    "    )\n",
    "\n",
    "    #creating sinlge model\n",
    "    optimal_param = {\n",
    "            \"n_estimators\"      : 128,\n",
    "            \"max_depth\"         : 10,\n",
    "            \"min_samples_leaf\"   : 8\n",
    "        }\n",
    "\n",
    "    rfc_set.create_model(param = optimal_param)\n",
    "\n",
    "    print(f\"\\n{set_name}\")\n",
    "    score = rfc_set.get_model_score(get_test_score = False)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for df_set, set_name in zip([df_0, df_1, df_2, df_3], [\"top 5 features\", \"top 10 features\", \"top 15 features\", \"handpicked\"]):\n",
    "\n",
    "    result = magic(df_set, set_name)\n",
    "    results[set_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(results)\n",
    "df_result.drop(labels = [ind for ind in df_result.index if \"mat\" in ind], axis = 0, inplace = True)\n",
    "df_result.T\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Overfitting prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic(df, param):\n",
    "\n",
    "    rfc_param = RF(\n",
    "        df              = df,\n",
    "        y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "        n_jobs          = 4,\n",
    "\n",
    "        data_folder     = data_folder,\n",
    "        results_file    = \"optim_reults_rfc.csv\",\n",
    "\n",
    "        model_metric    =  \"c\" # r = regression, c = classification\n",
    "    )\n",
    "\n",
    "    rfc_param.create_model(param = param)\n",
    "    score = rfc_param.get_model_score(get_test_score = False)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sinlge model\n",
    "param_set0 = {\n",
    "        \"n_estimators\"      : 100,\n",
    "        \"max_depth\"         : 10,\n",
    "        \"min_samples_leaf\"  : 8,\n",
    "    }\n",
    "\n",
    "#creating sinlge model\n",
    "param_set1 = {\n",
    "        \"n_estimators\"      : 128,\n",
    "        \"max_depth\"         : 5,\n",
    "        \"min_samples_leaf\"  : 8,\n",
    "    }\n",
    "\n",
    "#creating sinlge model\n",
    "param_set2 = {\n",
    "        \"n_estimators\"      : 128,\n",
    "        \"max_depth\"         : 10,\n",
    "        \"min_samples_leaf\"  : 200,\n",
    "    }\n",
    "\n",
    "#creating sinlge model\n",
    "param_set3 = {\n",
    "        \"n_estimators\"      : 128,\n",
    "        \"max_depth\"         : 10,\n",
    "        \"min_samples_leaf\"  : 8,\n",
    "        \"min_samples_split\" : 200,\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "for param in [param_set0, param_set1, param_set2, param_set3]:\n",
    "\n",
    "    result = magic(df_2, param)\n",
    "    result.update(param)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwrite RF param generator\n",
    "class RFOP(RF):\n",
    "\n",
    "    def create_param_list(self, n_trees, n_depth, n_leafs):\n",
    "        \"\"\"return list: [{param_1 : value_1},{},]\"\"\"\n",
    "\n",
    "        param_list = []\n",
    "\n",
    "        #get individual numbers\n",
    "        n_estimators        = [128]\n",
    "        max_depths          = [2*n for n in range(1,n_depth+1)]\n",
    "        min_sample_leafs    = [25*n  for n in range(1,n_leafs+1)]\n",
    "\n",
    "        combinations = list(itertools.product(n_estimators, max_depths, min_sample_leafs))\n",
    "\n",
    "        for combination in combinations:\n",
    "\n",
    "            params = {\n",
    "                \"n_estimators\"      : combination[0],\n",
    "                \"max_depth\"         : combination[1],\n",
    "                \"min_samples_leaf\"  : combination[2],\n",
    "            }\n",
    "\n",
    "            param_list.append(params)\n",
    "\n",
    "        print(f\"\\nGenerated param combinations: {len(param_list)}\")\n",
    "        return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_op_obj = RFOP(\n",
    "    df              = df,\n",
    "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "    n_jobs          = 4,\n",
    "\n",
    "    data_folder     = data_folder,\n",
    "    results_file    = \"optim_reults_rfc_op.csv\",\n",
    "\n",
    "    model_metric    =  \"c\" # r = regression, c = classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_optim:\n",
    "     rfc_op_obj.run_optim(n_depth = 7, n_leafs = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_op = rfc_op_obj.get_results()\n",
    "df_results_op.sort_values(by = \"valid_accuracy\", ascending = False, inplace = True)\n",
    "\n",
    "df_results_op.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame = df_results_op,\n",
    "    x = \"train_accuracy\",\n",
    "    y = \"valid_accuracy\",\n",
    "    color = \"min_samples_leaf\",\n",
    "    size = \"max_depth\",\n",
    "\n",
    "    title = \"Fitting graph RF Classifier\",\n",
    "    trendline = \"lowess\",\n",
    "    color_continuous_scale = plt_style_c,\n",
    ")\n",
    "\n",
    "fig.add_hline(\n",
    "    y = 0.55,\n",
    ")\n",
    "fig.add_vline(\n",
    "    x = 0.5\n",
    ")\n",
    "\n",
    "fig.add_scatter()\n",
    "\n",
    "scale_show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_param = {\n",
    "    \"n_estimators\" : 128,\n",
    "    \"max_depth\": 8,\n",
    "    \"min_samples_leaf\": 225 #225\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create optimal model\n",
    "rfc_op_obj.create_model(param = optimal_param)\n",
    "rfc_op_obj.get_model_score(get_test_score = True)\n",
    "rfc_op_obj.plot_confusion_mat(set = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in df.columns.tolist() if item not in df_2.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_op_obj = RFOP(\n",
    "    df              = df_2,\n",
    "    y_col           = [\"t2m_t1_cat\", \"t2m_t2_cat\"], #or [\"t2m_t1_cat\", \"t2m_t1_cat\"]\n",
    "\n",
    "    n_jobs          = 4,\n",
    "\n",
    "    data_folder     = data_folder,\n",
    "    results_file    = \"optim_reults_rfc_op.csv\",\n",
    "\n",
    "    model_metric    =  \"c\" # r = regression, c = classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create optimal model\n",
    "rfc_op_obj.create_model(param = optimal_param)\n",
    "rfc_op_obj.get_model_score(get_test_score = True)\n",
    "rfc_op_obj.plot_confusion_mat(set = \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
